title,abstract,references
ll OPEN ACCESS Review Artiﬁcial intelligence for multimodal data integration in oncology Jana Richard Bowen Ming Matteo Daniel Anurag Chengkuan Luoting Drew Muhammad Tiffany and Faisal of Pathology Brigham and Women s Hospital Harvard Medical School Boston MA USA of Pathology Massachusetts General Hospital Harvard Medical School Boston MA USA Program Broad Institute of Harvard and MIT Cambridge MA USA Science Program Cancer Institute Boston MA USA of Biomedical Informatics Harvard Medical School Boston MA USA Health Sciences and Technology HST Cambridge MA USA of Electrical Engineering and Computer Science Massachusetts Institute of Technology MIT Cambridge MA USA of Computer Science Harvard University Cambridge MA USA Data Science Initiative Harvard University Cambridge MA USA Correspondence faisalmahmood https SUMMARY In oncology the patient state is characterized by whole spectrum of modalities ranging from radiology tology and genomics to electronic health records Current artiﬁcial intelligence AI models operate mainly in the realm of single modality neglecting the broader clinical context which inevitably diminishes their tential Integration of different data modalities provides opportunities to increase robustness and accuracy of diagnostic and prognostic models bringing AI closer to clinical practice AI models are also capable of discovering novel patterns within and across modalities suitable for explaining differences in patient comes or treatment resistance The insights gleaned from such models can guide exploration studies and contribute to the discovery of novel biomarkers and therapeutic targets To support these advances here we present synopsis of AI methods and strategies for multimodal data fusion and association discovery We outline approaches for AI interpretability and directions for exploration through multimodal data interconnections We examine challenges in clinical adoption and discuss emerging solutions INTRODUCTION Cancer is highly complex disease involving cascade of scopic and macroscopic changes with mechanisms and tions that are not yet fully understood Cancer biomarkers provide insights into the state and course of disease in the form of titative or qualitative measurements which consequently guide patient management Based on their primary use biomarkers can be diagnostic prognostic or predictive of response and tance to treatment Diagnostic markers stand at the ﬁrst line of cancer detection and diagnosis including examples such as antigen PSA values indications in radiologic imaging or neoplastic changes in tissue biopsy Examples of predictive markers include microsatellite instability which is commonly used to predict response to hibitor therapy in colorectal cancer Marcus et and KRAS mutations used to indicate resistance to ment Van Cutsem et Prognostic markers forecast risks associated with clinical outcomes such as survival recurrence or disease progression Such prognostic markers range from tumor grade and stage to genomic and transcriptomic assays such as Oncotype DX and Prosigna often used to estimate recurrence and survival likelihood Paik et Despite the vital role of biomarkers patients with similar proﬁles can exhibit diverse outcomes treatment responses Shergalis et recurrence rates Roy et or treatment toxicity Kennedy and Salama while the underlying reasons for such omies largely remain unknown There is crucial need to identify novel and biomarkers Modern cancer centers quire cornucopia of data over the course of patient s nosis and treatment trajectory ranging from radiology histology clinical and laboratory tests to familial and patient histories with each modality providing additional insights on the patient state holistic framework integrating complementary information and clinical context from diverse data sources would enable ery of new biomarkers paving the path to the next generation of personalized medicine as illustrated in Figure An analysis of possible correlation and patterns across diverse data modalities can easily become too complex during subjective analysis making it an attractive application for Boehm et The capacity of AI models to leverage diverse complementary information from multimodal data and identify predictive features within and across modalities allows for automated and objective exploration and discovery of novel biomarkers Additionally AI can identify accessible surrogates for existing but yet expensive markers to itate the spread of advanced targeted therapies and population screenings Cancer Cell October ª The Author s Published by Elsevier This is an open access article under the CC license http ll OPEN ACCESS Review Figure multimodal data integration and AI models can integrate complementary information and clinical context from diverse data sources to provide more accurate outcome predictions The clinical insights identiﬁed by such models can be further elucidated through C interpretability methods and D quantitative analysis to guide and accelerate the discovery of new biomarkers or therapeutic targets and F B AI can reveal novel multimodal interconnections such as relations between certain mutations and changes in cellular morphology or associations between radiology ﬁndings and histology tumor subtypes or molecular features Such associations can serve as or alternatives to existing biomarkers to support patient screening and F Historically the biomarker discovery process typically involved the examination of potentially informative qualitative tures such as tissue morphology or quantitative measurements such as genomic transcriptomic alterations and their tion with clinical endpoints For instance standardized logic assesment pipelines such as the the Nottingham grading system in breast cancer Rakha et and the Gleason grading in prostate cancers Epstein et was determined through dedicated examination of thousands of histopathology slides revealing associations between morphological features and patient outcome Although the identiﬁcation of each new biomarker represents milestone in oncology this process faces several challenges Manual assessment is time and resource intensive often without the possibility of translating observations from one cancer model to another Morphologic cancer interrater variability ment is often qualitative with substantial which hinders reproducibility and contributes to inconsistent comes in clinical trials Given the large complexity of medical data current biomarkers are mostly unimodal However straining the biomarkers to single modality can signiﬁcantly reduce their clinical potential For instance glioma patients with similar genetic or histology proﬁles can have diverse outcomes caused by macroscopic factors such as tumor location venting full resection and irradiation or disruption of the brain barrier altering the efﬁcacy of drug delivery Miller Over the past years artiﬁcial intelligence AI and in particular representation learning methods have demonstrated great Cancer Cell October formance in many clinically relevant tasks inclusing tasks that are often not trivial for human observers Bera et Lu et AI models are able to integrate complementary mation and clinical context from diverse data sources to provide more accurate patient predictions Figure Hosny et The clinical insights identiﬁed by successful models can be further elucidated through interpretability methods and tive analysis to guide and accelerate the discovery of new markers Figures and Similarly AI models can discover associations across multiple modalities such as relations tween certain mutations and speciﬁc changes in cellular morphology Coudray et or associations between ology ﬁndings and tumor subtypes Junior et Hyun et or molecular features Yan et Figure Such associations can identify sible or alternatives for existing biomarkers to port population screenings or selection of patients for clinical trials Figures and In this review we summarize AI methods and strategies for multimodal data fusion outline spective on AI driven exploration through multimodal tions and interpretability methods and conclude with directions for AI adoption in precision oncology AI METHODS IN ONCOLOGY AI methods can be categorized as supervised weakly vised or unsupervised To highlight the concepts speciﬁc to Review ll OPEN ACCESS Figure Overview of AI methods Supervised methods use strong supervision whereby each data point feature or image patch is assigned label B Weakly supervised methods allow one to train the model with weak labels avoiding the need for manual annotations C Unsupervised methods explore patterns subgroups and structures in unlabeled data For comparison all methods are illustrated on binary cancer detection task Cancer Cell October ll OPEN ACCESS each category we present all methods in the framework of puter vision as applied to digital pathology Figure Supervised methods Supervised methods map input data to predeﬁned labels using annotated data points such as tized slides with annotations or radiology images with patient outcome Examples of fully supervised methods include and representation learning methods methods These methods take as input set of predeﬁned features cell shape or size extracted from the data before the training not the data themselves The training is performed with standard ML models such as random forest RF machine SVM or multilayer perceptron MLP simas and Wiberg Figure Since the feature extraction is not part of the learning process the models typically have simpler architecture lower computation cost and may require less training data than DL models An additional beneﬁt is high level of interpretability since the predictive features can be related to the data On the other hand the feature extraction is time consuming and can translate human bias to the models downside is that manual feature extraction or engineering limits the models ability to features already known and understood by humans and prevents the utility and downstream discovery of new relevent features Moreover human perception can not be easily captured by set of mathematical operators often leading to simpler features Since the features are usually tailored to the speciﬁc disease the models can not be easily translated to other tasks or malignancies Despite the popularity of DL methods in many applications the methods are sufﬁcient and preferred due to their simplicity and ability to learn from smaller datasets Representation learning methods Representation learning methods such as deep learning DL are capable of learning rich feature representations from the raw data without the need for manual feature engineering Here we focus on convolutional neural networks CNNs the most mon DL strategy for image analysis In CNNs the predictive tures are not deﬁned and the model learnins which concepts and features are useful for explaining relations between inputs and outputs For instance in Figure each training image WSI is manually annotated to outline the tumor region The WSI is then partitioned into rectangular patches and each patch is assigned with label cancer or mined by the tumor annotation The majority of CNNs have similar architectures consisting of alternating convolutional pooling and activation layers followed by small number of fully connected layers convolution layer serves as feature extractor while the subsequent pooling layer denses the features into the most relevant ones The activation function allows the model to explore complex relations across features Fully connected layers then perform the end task such as classiﬁcation The main strength of CNNs is their ability to extract rich feature representations from raw data sulting in lower preprocessing cost higher ﬂexibility and often superior performance over models The potential limitations come from the model s reliance on Cancer Cell October Review tions which are time intensive and might be affected by rater variability and human bias Moreover predictive regions for many clinical outcomes such as survival or treatment tance may be unknown CNNs are also often criticized for their lack of interpretability while we are able to often examine gions used by the model to make predictive determinations the overall feature representations remain abstract Despite these limitations CNNs come with impressive performance contributing to widespread usage in many clinically relevent plications Weakly supervised methods Weakly supervised learning is of supervised learning with batch annotations on large clusters of data tially representing scenario where the supervisory signal is weak compared to the amount of noise in the dataset mon example of the utility of weak supervision is detection of small tumor regions in biopsy or resection in large gigapixel whole slide image with labels at the level of the slide or case Weakly supervised methods allow one to train models with weak labels such as diagnosis or survival ing the need for manual data annotations The most common weakly supervised methods include graph convolutional works GCNs learning MIL and vision formers VITs Graph convolutional networks Graphs can be used to explicitly capture structure within data and encode relations between objects making them ideal for analysis of tissue biospy images graph is deﬁned by nodes connected by edges In histology node can represent cell an image patch or even tissue region Edges encode spatial relations and interactions between nodes Zhang et The graph combined with the labels is processed by GCN et which can be seen as generalization of CNNs that operate on structured graphs feature representations of node are updated by aggregating information from neighboring nodes The updated representations then serve as input for the ﬁnal classiﬁer Figure GCNs can incorporate larger context and spatial tissue structure as compared to conventional deep models for digital pathology which patch the image into small regions which remain mutually exclusive This can be beneﬁcial in tasks where the spatial context spans beyond the scope of single patch Gleason score On the other hand the interdependence of the nodes in GCNs comes with higher training costs and memory requirements since the des can not be processed independently learning MIL is type of weakly supervised learning where multiple stances of the input are not individually labeled and the sory signal is only available collectively for set of instances commonly reffered to as bag Carbonneau et gina et The label of bag is assumed positive if there is at least one positive instance in the bag The goal of the model is to predict the bag label MIL models comprise three main modules feature learning or extraction aggregation and prediction The ﬁrst module is used to embed the images or other higher tional data into embeddings this module can be trained on the ﬂy Campanella et or In GCNs Review encoder from supervised or learning can be used to reduce training time and Lu et The embeddings are aggregated to create representations which serve as input for the ﬁnal classiﬁcation module commonly used aggrigation stratergy is based pooling Ilse et where two fully connected works are used to learn the relative importance of each instance Ilse et The representations weighted by the corresponding attention score are summed up to build the representation The attention scores can be also be used in understanding the predictive basis of the model see multimodal interpretability for additional details In large scale medical datasets ﬁne annotations are often not available which makes MIL an ideal approach for training deep models there are several recent examples in cancer pathology Campanella et Lu et b and genomics Sidhom et Vision transformers VITs Dosovitskiy et Vaswani et are type of learning which allows for the model to be fully context aware In contrast to MIL where patches are assumed dependent and identically distributed VITs account for correlation and context among patches The main components of VITs include positional encoding and multihead attention Positional encoding learns the spatial structure of the image and the relative distances between patches The tion mechanism determines the relevance of each patch while also accounting for the context and contributions from the other patches Multihead simultaneously deploys multiple blocks to account for different types of interactions between the patches and combines them into single tion output typical VIT architecture is shown in Figure WSI is converted into series of patches each coupled with positional information Learnable encoders map each patch and its position into single embedding vector referred to as token An tional tokens is introduced for the classiﬁcation task The class ken together with the patch tokens is fed into the transformer encoder to compute multihead and output the able embeddings of patches and the class The output class token serves as representation used for the ﬁnal tion The transformer encoder consists of several stacked tical blocks Each block includes multihead and MLP along with layer normalization and residual connections The positional encoding and multiple heads allow one to incorporate spatial information increase the context and robustness Li et Shamshad et of VIT methods over other methods On the other hand VITs tend to be more data hungry Dosovitskiy et limitation that the machine learning community is actively working to overcome Weakly supervised methods offer several beneﬁts The ration from manual annotations reduces the cost of data preprocessing and mitigates the bias and interrater variability Consequently the models can be easily applied to large sets diverse tasks and also situations where the predictive gions are unknown Since the models are free to learn from the entire scan they can identify predictive features even beyond the regions typically evaluated by pathologists The great mance demonstrated by weakly supervised methods suggests that many tasks can be addressed without expensive manual notations or features ll OPEN ACCESS Unsupervised methods Unsupervised methods explore structures patterns and groups in data without relying on any labels These include and fully unsupervised strategies methods methods aim to learn rich feature tions from within data by posing the learning problem as task the ground truth for which is deﬁned within the data Such encoders are often used to obtain high quality lower mentional embeddings of complex high dimentional datasets for making downstream tasks more efﬁcient interms of data and training efﬁciency For example in pathology images supervised methods exploit available unlabeled data to learn image features and then transfer this knowledge to supervised models To achieve this supervised methods such as CNNs are used to solve various pretext tasks Jing and Tian for which the labels are generated automatically from the data For instance patch can be removed from an age and deep network is trained to predict the missing part of the image from its surroundings using the actual patch as bel Figure The patch prediction direct clinical vance but it guides the model to learn features of image characteristics which can be beneﬁcial for other tical tasks The early layers of the network are usually capture general image features while the later layers pick features vant for the task at hand The later layers can be excluded while the early layers serve as feature extractors in for supervised models transfer learning Unsupervised feature analysis These methods allow for exploring structure similarity and mon features across data points For example using dings from encoder one could extract features from large dataset of diverse patients and cluster said dings to ﬁnd common features across the entire patient cohorts The most common unsupervised methods include clustering and dimensionality reduction Clustering methods Rokach and Maimon partition data into subgroups such that the larities within the subgroup and the separation between groups are maximized Although the output clusters are not task speciﬁc they can reveal different cancer subtypes or tient subgroups The aim of dimensionality reduction is to obtain representation capturing the main tics and correlations in the data MULTIMODAL DATA FUSION The aim of multimodal data fusion is to extract and combine complementary contextual information across different ities for better Zitnik et This is of particular relevance in medicine where similar ﬁndings in one modality may have diverse interpretations in combination with other modalities Iv et For instance an mutation status or histology proﬁle alone is insufﬁcient for explaining the variance in patient outcomes whereas the combination of both been recently used to redeﬁne the WHO cation of diffuse glioma Louis et AI offers an mated and objective way to incorporate complementary mation and clinical context from diverse data for improved predictions Multimodal AI models can also utilize Cancer Cell October ll OPEN ACCESS Review Figure Multimodal data fusion Early fusion builds joint representation from raw data or features at the input level before feeding it to the model B Late fusion trains separate model for each modality and aggregates the predictions from individual models at the decision level In intermediate fusion the prediction loss is propagated back to the feature extraction layer of each modality to iteratively learn improved feature sentations under the multimodal context The unimodal data can be fused C at single level or D gradually in different layers Guided fusion allows the model to use information from one modality to guide feature extraction from another modality F Key for the symbols used complementary and supplementary information in modalities if unimodal data are noisy or incomplete supplementing dant information from other modalities can improve the ness and accuracy of the predictions data fusion tegies Baltru saitis et can be divided as early late and intermediate see Figure Early fusion Early fusion integrates information from all modalities at the input level before feeding it into single model The modalities can be represented as raw data hand crafted or deep features The joint representation is built through operations such as vector concatenation sum multiplication Cancer Cell October Review Hadamard product or bilinear pooling Kronecker product Huang et Ramachandram and Taylor In early fusion only one model is trained which simpliﬁes the design cess However it is assumed that the single model is well suited to all modalities Early fusion requires certain level of alignment or synchronization between the modalities Although this is more obvious in other domains such as synchronization of audio and visual signals in speech recognition it is also relevant in clinical settings If the modalities come from signiﬁcantly different time points such as and postinterventions then early fusion might not be an appropriate choice Applications of early fusion include integration of similar ities such as multimodal multiview ultrasound images for breast cancer detection Qian et or fusion of structural computed tomography CT MRI data with metabolic tron emission tomography PET scans for cancer detection et treatment planning et or survival prediction Nie et Other examples include fusion of ing data with electronic medical records EMRs such as tion of dermoscopic images and patient data for skin lesion ﬁcation Yap et or fusion of cervigram and EMRs for cervical dysplasia diagnosis Xu et Several studies tigate the correlation between changes in gene expression and sue morphology integrating genomics data with histology radiology images for cancer classiﬁcation Khosravi et survival Chen et and treatment response Feng et Sammut et prediction Late fusion Late fusion also known as fusion trains separate model for each modality and aggregates the predictions from dividual models for the ﬁnal prediction The aggregation can be performed by averaging majority voting rules Ramanathan et or learned models such as MLP Late fusion allows one to use different model architecture for each modality and does not pose any constraints on data tion making it suitable for systems with large data heterogeneity or modalities from different time points In cases of missing or incomplete data late fusion retains the ability to make predictions since each model is trained separately and aggregations such as majority voting can be applied even if modality is missing larly inclusion of new modality can be performed without the need to retrain the full model Simple covariates such as age or gender are often included through late fusion due to its simplicity see Figure If the unimodal data do not complement one another or do not have strong interdependencies late fusion might be preferable thanks to the simpler architecture and smaller number of parameters compared with other fusion strategies This is also beneﬁcial in situations with limited data Furthermore rors from individual models tend to be uncorrelated resulting in potentially lower bias and variance in predictions In situations when information density varies signiﬁcantly across modalities predictions from shared representations can be ly inﬂuenced by the most dominant modality In late fusion the contribution from each modality can be accounted for in controlled manner by setting equal or diverse weights per ity in the aggregation step Examples of late fusion include integration of imaging data with inputs such as fusion of MRI scans and PSA ll OPEN ACCESS blood tests for prostate cancer diagnosis Reda et integration of histology scans and patient gender for inferring origin of metastatic tumors Lu et fusion of genomics and histology proﬁles for survival prediction Chen et Shao et combination of pretreatment MRI or CT scans with EMRs for chemotherapy response prediction Joo et and survival estimation Nie et the same level forcing the model Intermediate fusion This is strategy wherein the loss from the multimodal model propagates back to the feature extraction layer of each dality to iteratively improve feature representations under the multimodal context For comparison in early and late fusion the unimodal embeddings are not affected by the multimodal information Intermediate fusion can combine individual dalities at different levels of abstractions Moreover in tems with three or more modalities the data can be fused either all at once Figure or gradually across different levels Figure The intermediate fusion is similar to early fusion however in early fusion the unimodal embeddings are not affected by the multimodal context Gradual fusion allows one to combine data from highly lated channels at to consider the between speciﬁc modalities followed by fusion with less correlated data in later layers For instance in Figure genomics and histology data are fused ﬁrst to account for the interplay between mutations and changes in the tissue morphology while the relation with the macroscopic radiology data is considered in the later layer Gradual fusion shown improved performance over fusion in some applications Joze et pathy et Lastly allows model to use informaiton from one modality to guide feature extraction from another modality For instance in Figure genomics mation guides the selection of histology features The tion is that different tissue regions might be relevant in the presence of speciﬁc mutations Guided fusion learns attention scores that reﬂect the relevance of different ogy features in the presence of speciﬁc molecular information The scores are learned with the multimodal model where the genomics feature and the corresponding histology features are combined for the ﬁnal model predictions Examples of intermediate fusion include integration of diverse imaging modalities such as the fusion of PET and CT scans in lung cancer detection Kumar et fusion of MRI and ultrasound images in prostate cancer classiﬁcation Sedghi et or combination of multimodel MRI scans in glioma segmentation Havaei et Fusion of diverse multiomics data was used for cancer subtyping Liang et or survival prediction Lai et Genomics data have been used in tandem with histology and images for Rohr or mammogram Yala et improved survival prediction Guided fusion of different ology modalities was used to improve segmentation of liver sions Mo et and anomalies in breast tissue Lei et EMRs were used to guide feature extraction from dermoscopic Zhou and Luo and mammography to improve detection and Vo et images Cancer Cell October ll OPEN ACCESS Review Figure Multimodal interpretability and introspection and B Histology an MIL model was trained to classify subtypes of renal cell carcinoma in WSIs while CNN was trained to perform the same task in image patches Attention heatmaps and patches with the lowest and highest attention scores B GradCAM attributions for each class C and Integrated gradient attributions can be used to analyze C genomics or EMRs The attribution magnitude corresponds to the importance of each feature and direction indicates feature impact toward low left high right risk The color speciﬁes the value of the input features copy number gain and presence of mutation are shown in red while blue is used for copy number loss and status Attention scores can be used to analyze the importance of words in the medical text D and F Radiology an MIL model was trained to predict survival from MRI scans using axial slides as individual instances D Attention heatmaps mapped into the MRI scan and slides with the highest and lowest attention F GradCAM was used to obtain interpretability in each MRI slide interpretability is computed by weighting the GradCAM maps by the attention score of the respective slide classiﬁcation of lesions Chen et Chen et used genomics information to guide selection of histology features for improved survival prediction in multiple cancer types There is conclusive evidence that one fusion type is mately better than the others as each type is heavily data and task speciﬁc Cancer Cell October MULTIMODAL INTERPRETABILITY Interpretability and model introspection is crucial component of AI development deployment and validation With the ability of AI models to learn abstract feature representations there is concern that the models might use spurious shortcuts for Review predictions instead of learning clinically relevant aspects Such models might fail to generalize when presented with new data or discriminate against certain populations Banerjee et Chen et On the other hand the models can discover novel and clinical relevant insights Here we present brief view of different methods used for model introspection in oncology Figure more technical details can be found in recent review Arrieta et It is worth indicating that these methods allow us to introspect parts of the data deemed tant by the model in making predictive determinations yet the feature representation itself remains abstract Histopathology In histopathology VITs or MIL can reveal the relative importance of each image patch for the model predictions Depending on the model architecture attention or probability scores can be ped to obtain attention heatmaps as shown in Figure where an MIL model was trained to classify cancer subtypes in WSIs Although manual annotations were used the model learned to identify morphology speciﬁc for each cer type and to discriminate between normal and malignant sues Class activation methods CAMs such as GradCAM varaju et or Chattopadhay et allow one to determine the importance of the model inputs pixels by computing how the changes in the inputs affect the model outputs for each prediction class GradCAM is often used in tandem with the method the Selvaraju et where the guided backpropagation determines the importance inside the predictive regions speciﬁed by the GradCAM This is illustrated in Figure where CNN was trained to classify cer subtypes in image patches For comparison in the attention methods the importance of each instance is determined during the training while the methods are model agnostic independent of the model training Radiology In radiology the interpretability methods are similar to those used in histology The attention scores can reﬂect the tance of slides in scan For instance in Figure an MIL model was trained to predict survival in glioma patients Zhuang et The model considered the MRI scan as bag where the axial slides are modeled as individual instances Even in the absence of manual annotations the model placed high attention to the slides with tumor while low attention was assigned to healthy tissue methods can be quently deployed to localize the predictive regions within ual slides Figure Molecular data Molecular data can be analyzed by the integrated gradient method Sundararajan et which computes attribution values indicating how changes in speciﬁc inputs affect the model outputs For the regression tasks such as survival analysis the attribution values can reﬂect the magnitude of the importance as well as the direction of the impact features with positive bution increase the predicted output higher risk while tures with negative attribution reduce the predictive values lower risk At the patient level this is visualized as bar plot ll OPEN ACCESS where the axis corresponds to the speciﬁc features ordered by their absolute attribution value and the x axis shows the responding attribution values At the population level the tion plots depict the distribution of the attribution scores across all subjects Figure shows the attribution plots for most important genomics features used for survival prediction in oma patients Chen et Other tabular data such as features or values obtained from EMR can be terpreted in the same way EMRs can be also analyzed by natural language processing NLP methods such as transformers where the attention scores determine the importance of speciﬁc words in the text Figure Multimodal models In multimodal models the attribution plots can also determine the contribution of each modality toward the model predictions All previously mentioned methods can be used in multimodal models to explore interpretability within each modality over shifts in feature importance under unimodal and modal settings can be investigated to analyze the impact of the multimodal context The interpretability methods usually come without any racy measures and thus it is important not to overinterpret them While or methods can localize the predictive regions they can not specify which features are vant they can explain where but not why Moreover there is guarantee that all regions carry clinical relevance High scores just mean that the model considered these regions more important than others MULTIMODAL DATA INTERCONNECTION The aim of multimodal data interconnection is to reveal tions and shared information across modalities Such tions can provide new insights into cancer biology and guide the discovery of novel biomarkers Although there are many proaches for data exploration here we illustrate few possible directions Figure Morphologic associations Malignant changes often propagate across different scales genic mutations can affect cell behavior which in turn reshapes sue morphology or the tumor microenvironment visible in histology images Consequently the microscopic changes might have an impact on tumor metabolic activity and macroscopic appearance detectable by PET or MRI scans The feasibility of AI methods to identify associations across modalities was ﬁrst demonstrated by Coudray et Coudray et who showed that certain mutations in lung cancer can be inferred directly from hematoxylin and eosin H WSIs Other studies followed shortly dicting the mutation status from WSIs in liver Chen et bladder Loefﬂer et colorectal Jang et and roid cancer Tsou and Wu as well as tation studies attempting to predict any genetic alternation in any tumor type Fu et Kather et Additional ular biomarkers such as gene expression Anand et Binder et Schmauch et Naik et tumor mutational burden Jain and Massoud and microsatellite instability Cao et Echle et Cancer Cell October ll OPEN ACCESS Review Figure Multimodal data interconnection and B AI can identify associations across modalities such as the feasibility of inferring certain mutations from histology or radiology images or B the relation between and invasive modalities such as prediction of histology subtype from radiomics features C The models can uncover associations between clinical data and patient outcome contributing to the discovery of predictive features within and across modalities D Information acquired by EMRs or wearable devices can be analyzed to identify risk factors related to cancer onset or uncover patterns related with treatment response or resistance to support early interventions have also been inferred from WSIs Murchan et In radiology AI models have predicted IDH mutation and deletion status from preoperative brain MRI scans Bangalore gananda et Yogananda et and and mutational status from breast mammography et and MRI Vasileiou et scans while EGFR and KRAS mutations have been detected from CT scans in lung Wang et and colorectal et cancer By discovering the presence of morphological associations across modalities AI models can enhance exploratory studies and reduce the search space for possible biomarker candidates For instance in Figure AI revealed that one of the studied mutations can be reliably inferred from WSI Although the tive features used by the model might be unknown ability methods can provide additional insights Attention maps can reveal tissue regions relevant for the prediction of the speciﬁc mutation Distinct tissue structures and cell types in the regions with the and scores can be identiﬁed and their properties such as nucleus shape or ume can be further extracted and analyzed Clustering or dimensionality reduction methods can be deployed to examine the promising features potentially revealing associations tween mutation status and distinct morphological features The identiﬁed morphological associates can serve as biomarker surrogates to support screening in to come settings or reveal new therapeutic targets alternatives Similarly AI can discover relationships between and invasive modalities For instance AI models were used to predict histology subtypes or grades from radiomics features in lung Sha et brain Lasocki et liver Brancato et and other cancers et The tive image regions can be further analyzed to identify textures and patterns with possible diagnostic values see Figure which in turn can serve as surrogates for existing biomarkers Outcome associates Beneﬁts of personalized medicine are often limited by the paucity of biomarkers able to explain dichotomies in patient outcomes Cancer Cell October Review On the other hand AI models are demonstrating great mance in predicting clinical outcomes such as survival Lai et treatment response Echle et recurrence Yamamoto et and radiation toxicity Men et using unimodal and multimodal Chen et Joo et Mobadersany et data These works imply the feasibility of AI models to discover relevant nostic patterns in data which might be elucidated by ability methods For instance in Figure model is trained to predict survival from histology and genomics data Attention heatmaps reveal tissue regions related to and tient groups while the molecular proﬁles are analyzed through attribution plots The predictive tissue regions can be further analyzed by examining tissue morphology cell subtypes or other data characteristics phocytes can be estimated through of tumor and immune cells to specify immune hot and cold tumors Attribution of speciﬁc modalities as well as shifts in feature importance in imodal multimodal data can be explored to determine the ﬂuence of multimodal contextualization Such exploration studies have already provided new clinical insights For instance Geessink et Geessink et showed that the ratio can serve as an dent prognosticator in rectal cancer while the ratio of tumor area to metastatic lymph node regions prognostic value in gastric cancer Wang et Other morphological features such as the arrangement of collagen ﬁbers in breast histology Li et or spatial tissue organization in colorectal tissue Qi et have been identiﬁed as possible biomarkers for aggressiveness or recurrence Early predictors AI can also explore diverse data acquired prior to patient nosis to identify potential predictive risk factors EMRs provide rich information on patient history medication allergies or munizations which might contribute to patient outcome Such diverse data can be efﬁciently analyzed by AI models to search for distinct patient subgroups Figure Identiﬁed subgroups can be correlated with different patient outcomes while tion plots can identify the relevance of different factors at the tient and population level Recently Placido et Placido et showed the feasibility of AI to identify patients with higher risk of developing pancreatic cancer by exploration of EMR Similarly EMRs were used to predict treatment response Chu et or length of hospital stay Alsinglawi et The identiﬁed novel predictive risk factors can support scale population screenings and early preventive care Outside of the hospital setting smartphones and wearable vices offer another great opportunity for and uous patient monitoring Changes in the measured values such as decrease in patient step counts have been shown as robust predictors of worse clinical outcome and increased risk of hospitalization Low Furthermore the modern wearable devices are continually expanding their functionality including measurements of temperature stress levels or saturation or electrocardiograms These surements can be analyzed in tandem with clinical data to search for risk factors indicating early stages of increased toxicity or treatment resistance to allow personalized interventions during ll OPEN ACCESS the course of treatment Research on personalized monitoring and nanotechnologies is investigating novel directions such as the detection of patient measurements in sweat Xu et or ingestible sensors to monitor medication compliance and drug absorption Weeks et All these novel devices provide useful insights into the patient state which could be analyzed in larger clinical context through AI models CHALLENGES AND CLINICAL ADOPTION The path of AI into clinical practice is still laden with obstacles many of which are ampliﬁed in the presence of multimodal data Van der Laak et While several recent works discuss challenges such as fairness and dataset shifts Banerjee et Chen et Cirillo et Howard et Mehrabi et Zhang et limited ability Adebayo et Linardatos et Reyes et or regulatory guidelines Cruz Rivera et Topol Wu et here we focus on challenges speciﬁc to multimodal learning Missing data The challenge of missing data refers to the absence of part of modality or the complete unavailability of one or more ities The missing data affect both the model training and the deployment since the majority of existing AI models can not handle missing information Moreover the need to train models with complete multimodal data signiﬁcantly constrains the size of the training datasets Many multimodal datasets have large scale data missingness for example in the cancer genome atlas TCGA one of the largest publicly available multimodal datasets signiﬁcant missing data points The incomplete modalities still contain valuable information and the inability to deploy them poses signiﬁcant limitation Below we discuss two gies for handling missing data Synthetic data generation Given the paucity of medical data in general synthetic data is increasingly being used to train develop and augment AI models Chen et If part of an image is corrupted or if speciﬁc mutations are not reported the missing information can be synthesized from the remaining data If whole modality is missing its synthetic version can be derived from existing similar modalities For instance Haan et Haan et trained supervised model for translation of H stains into special stains using the special stains as ground truth bels The model was trained on pairs of perfectly aligned data obtained through of the same slides If paired data are not available unsupervised methods such as cycle ative adversarial networks GANs Zhu et can be used While synthetic data can improve the performance of detection and classiﬁcation methods they are less suitable for outcome prediction or biomarker exploration where the predictive features are not well understood and thus there is guarantee that the synthetic data contain the relevant ease characteristics Moreover the algorithms can also nate malignant features into the supposedly normal synthetic images Cohen et which can further hurt prediction results Cancer Cell October ll OPEN ACCESS methods methods aim to make models robust to missing information For instance Choi and Lee Choi and Lee posed the EmbraceNet model which can handle incomplete or missing data during training and deployment The EmbraceNet model probabilistically selects partial information from each dality and combines it into single representation vector which then serves as an input for the ﬁnal decision model When missing or invalid data are encountered they are not sampled instead other more complete modalities are used to compensate for the missing data The probabilistic data selection also zation effect similar to the dropout mechanism Data alignment To investigate cancer processes across different scales and dalities certain level of data alignment is required This might include alignment of similar or diverse modalities Alignment of similar modalities This method typically involves different imaging modalities of the same system This is usually achieved through image tion which is formulated as an optimization problem minimizing the difference between the modalities In radiology rigid anatomical structures can guide the data alignment For instance registration of MRI and PET brain scans is usually achieved with high accuracy even with simple afﬁne registration thanks to the rigid skull The situation is more plex in the presence of motion and deformations breathing in lung imaging or changes in the body posture between ning sessions Alignment of such data usually requires able registrations using natural or manually placed landmarks for guidance particularly challenging situation is the tion of scans between interventions registration of ative and postoperative scans which exhibit lot of changes due to tumor resection response to treatment or tissue compression Haskins et In histology each stained slide usually comes from different tissue cut Even in consecutive tissue cuts there are substantial differences in the tissue appearance caused by changes in the tissue microenvironment or artifacts such as tissue folding tearing or cutting Taqi et which all complicate data alignment Robust and automated registration of histology ages can be challenging Borovec et and thus many studies deploy strategies such as clearing and of the tissue slides Hinton et newly emerging direction is stainless imaging including approaches such as ultraviolet microscopy Fereidouni et lated Raman histology Hollon et or colorimetric ing Balaur et Alignment of diverse modalities This refers to the integration of data from different scales time points or measurements Often an acquisition of one modality results in the destruction of the sample preventing collection of multiple measurements from the same system For instance most omics measurements require tissue disintegration which inevitably affects the possibility of studying relations between cell appearance and corresponding gene expression Here autoencoders can be used to enable integration and translation between arbitrary modalities autoencoders Dai Yang et build pair of Cancer Cell October Review decoder networks for each modality where the encoder maps each modality into latent space while the decoder maps it back into the original space discriminative objective function is used to match the different modalities in the common latent space With the shared latent space in place one can combine an encoder of one modality with the decoder of another modality to align one modality to another one Dai Yang et demonstrated translations between matin images and data The feasibility and ity of the autoencoders are yet to be tested with large scale clinically relevent datasets However if proven potent they hold great potential to address challenges with alignment and harmonization of data from diverse sources Transparency and prospective clinical trials Given the complexity of representation modern AI methods and the fact that they use abstract feature tions it is possible that their mechanisms will not be fully stood in the near future However one may argue that many pects in medicine are not fully understood either Kirkpatrick Some of the interpretability methods discussed earlier are capable of indicating regions within data used to make tion determination yet the actual feature representation remains abstract And thus rather than dwelling on the full opacity of AI methods we should advocate for their rigorous validation under randomized clinical trials same as is done for other medical vices and drugs Ghassemi et Prospective trials will allow us to stress test the models under conditions compare their performance against and current practice estimate how clinicians will interact with the AI tool and ﬁnd the best way in which the models can enhance rather than disturb the clinical workﬂow In the case of biomarker surrogates discovered by AI methods regulation paths similar to drugs and devices Aronson and Green could be used to ensure comparable levels of performance Transparency about study design and the data used are necessary to determine the tended use and conditions under which the model performance been veriﬁed and evaluated et spective clinical trials are inevitable to truly demonstrate and quantify the added value of AI models which will in turn increase trust and motivation of practitioners toward the AI tools OUTLOOK AND DISCUSSION AI the potential to have an impact on the whole landscape of oncology ranging from prevention to intervention AI models can explore complex and diverse data to identify factors related to high risks of developing cancer to support large population ings and preventive care The models can further reveal tions across modalities to help identify diagnostic or prognostic biomarkers from easily accessible data to improve patient risk stratiﬁcation or selection for clinical trials In similar way the models can identify alternatives to existing markers to minimize invasive procedures Prognostic models can predict risk factors or adverse treatment outcomes prior to terventions to guide patient management Information acquired from personal wearable devices or nanotechnologies could be further analyzed by AI models to search for early signs of treatment toxicity or resistance with other great application yet to come Review ll OPEN ACCESS As with any great medical advance there is need for rigorous validation and examination via clinical studies prospective trials to verify the promises made by AI models The role of AI in advancing the ﬁeld of oncology is not autonomous rather it is partnership between models and human experience that will drive further progress AI models come with limitations and lenges however these should not intimidate but rather inspire us With increasing incidence rates of cancer it is our obligation to capitalize on beneﬁts offered by AI methods to accelerate covery and translation of advances into clinical practice to serve patients and health care providers ACKNOWLEDGMENTS This work was supported in part by the BWH President s Fund National tute of General Medical Sciences NIGMS to Google Cloud Research Grant Nvidia GPU Grant Program and internal funds from BWH and MGH Pathology was additionally supported by the Tau Beta Pi Fellowship and the Siebel Foundation was additionally ported by the NIH National Cancer Institute NCI Ruth Kirschstein National Service Award was additionally supported by the tional Science Foundation NSF graduate fellowship The content is solely the responsibility of the authors and does not reﬂect the ofﬁcial views of the NIH NSF NCI or NIGMS DECLARATION OF INTERESTS and are inventors on patent related to multimodal learning REFERENCES Adebayo Gilmer Muelly Goodfellow Hardt and Kim B Sanity checks for saliency maps In Advances in Neural Information Processing Systems Bengio Wallach Larochelle Grauman and Garnett eds Curran Associates Inc Armin Denman Fookes and Petersson Survey on Deep Learning for Computational pathology Computerized Medical Imaging and Graphics Alsinglawi Alshari Alorjani Mubin Alnajjar Novoa and Darwish An explainable machine learning framework for lung cer hospital length of stay prediction Sci Anand Kurian Dhage Kumar Rane Gann and Sethi Deep learning to estimate human epidermal growth factor receptor status from hematoxylin and breast tissue images Pathol Inform Aronson and Green pharmaceutical products tory deﬁnitions examples and relevance to drug shortages and essential medicines lists Br Clin Pharmacol Arrieta rıguez Ser Bennetot Tabik Barbado Gar cıa pez Barredo Arrieta et Explainable artiﬁcial intelligence xai concepts taxonomies nities and challenges toward responsible ai Inf Fusion Balaur Toole Spurling Mann Yeo Harvey najaﬁ Hanssen Orian Balaur et Colorimetric histology using plasmonically active microscope slides Nature Baltru saitis Ahuja and Morency Multimodal machine learning survey and taxonomy IEEE Trans Pattern Anal Mach Intell based method for classiﬁcation of idh mutation status in brain gliomas Neuro Oncol Bera Schalper Rimm Velcheti and Madabhushi Artiﬁcial intelligence in digital tools for diagnosis and sion oncology Nat Rev Clin Oncol Bertsimas and Wiberg Machine learning in oncology methods applications and challenges JCO Clin Cancer Inform Binder Bockmayr Wienert Heim Hellweg Ishii Stenzinger Hocke Denkert et Morphological and lecular breast cancer proﬁling through explainable machine learning Nat Mach Intell Patella Euler Baessler Martini von Spiczak Schneiter Opitz and felder Computed tomography radiomics for the prediction of thymic epithelial tumor histology tnm stage and myasthenia gravis PLoS One Boehm Khosravi Vanguri Gao and Shah nessing multimodal data integration to advance precision oncology Nat Rev Cancer Borovec Kybic Sorokin Bueno tikov Bakas Chang Heldmann Kartasalo et Anhir automatic histological image registration challenge IEEE Trans Med Imaging Brancato Garbino Salvatore and Cavaliere radiomic features help identify lesions and predict histopathological grade of hepatocellular carcinoma Diagnostics Campanella Hanna Geneslaw Miraﬂor Werneck Krauss Silva Busam Brogi Reuter Klimstra and Fuchs computational pathology using weakly supervised deep learning on whole slide images Nat Med Cao Yang Ma Liu Zhao Li Wu Wang Lu Cai et Development and interpretation of based model for the prediction of microsatellite instability in colorectal cancer Theranostics Carbonneau Cheplygina Granger and Gagnon tiple instance learning survey of problem characteristics and applications Pattern Recogn Chattopadhay Sarkar Howlader and Balasubramanian generalized based visual explanations for deep convolutional networks In In IEEE Winter Conference on tions of Computer Vision WACV IEEE pp Chen Zhang Topatana Cao Zhu Juengpanich Mao Yu and Cai X Classiﬁcation and mutation prediction based on histopathology h images in liver cancer using deep learning NPJ Precis Oncol Chen Lu Chen Williamson and Mahmood Synthetic data in machine learning for medicine and healthcare Nat Biomed Eng Chen Lu Wang Williamson Rodig Lindeman and Mahmood Pathomic Fusion An Integrated Framework for Fusing Histopathology and Genomic Features for Cancer Diagnosis and nosis IEEE Transactions on Medical Imaging Chen Chen Lipkova Wang Williamson Lu hai and Mahmood Algorithm fairness in ai for medicine and healthcare Preprint at arXiv Chen Lu Weng Chen Williamson Manz Shady and Mahmood Multimodal transformer for survival prediction in gigapixel whole slide images In In Proceedings of the International Conference on Computer Vision pp Banerjee Bhimireddy Burns Celi Chen Correa Dullerud Ghassemi Huang Kuo et Reading race ai recognises patient s racial identity in medical images Preprint at arXiv Chen Lu Williamson Chen Lipkova Shaban Shady Williams Joo Noor et tive analysis via interpretable multimodal deep learning Preprint at arXiv Bangalore Yogananda Shah Nalawade Murugesan Yu Bangalore Yogananda Shah Nalawade et novel fully automated Cheplygina Bruijne and Pluim survey of and transfer learning in medical age analysis Med Image Anal Cancer Cell October ll OPEN ACCESS Choi and Lee robust deep learning architecture for modal classiﬁcation Inf Fusion Chu Dong Wang and Huang Z Treatment effect prediction with adversarial deep learning using electronic health records BMC Med Inform Decis Mak Cirillo Morey Guney Subirats Mellino Gigante Valencia menteria Chadha and Mavridis intelligence for Sex and gender differences and biases in artiﬁcial biomedicine and healthcare NPJ Digit Med Cohen Luck and Honari Distribution matching losses can hallucinate features in medical image translation In In International conference on medical image computing and intervention Springer pp Coudray Ocampo Sakellaropoulos Narula Snuderl Moreira Razavian and Tsirigos Classiﬁcation and mutation prediction from cell lung cancer histopathology ages using deep learning Nat Med Cruz Rivera Liu Chan Denniston and Calvert and Working Group and Steering Group and Consensus Group Guidelines for clinical trial protocols for interventions involving artiﬁcial gence the extension Nat Med Dai Yang Belyaeva Venkatachalapathy Damodaran Katcoff Radhakrishnan ashankar and Uhler translation between imaging and sequencing data using coders Nat Commun Haan Zhang Zuckerman Liu Sisk Diaz Jen Nobori Liou Zhang et Deep formation of h stained tissues into special stains Nat Commun Dosovitskiy Beyer Kolesnikov Weissenborn Zhai terthiner Dehghani Minderer Heigold Gelly et An image is worth words transformers for image recognition at scale Preprint at arXiv Echle Grabsch Quirke van den Brandt West Hutchins Heij Tan Richman Krause et grade detection of microsatellite instability in colorectal tumors by deep learning Gastroenterology Epstein Zelefsky Sjoberg Nelson Egevad luzzi Vickers Parwani Reuter Fine and Eastham contemporary prostate cancer grading system validated native to the Gleason score Eur Urol Feng Liu Li Li Lou Shao Wang Huang Chen Pang et Development and validation of radiopathomics model to predict pathological complete response to neoadjuvant apy in locally advanced rectal cancer multicentre observational study cet Digital Health Fereidouni Harmany Tian Todd Kintner McPherson Borowsky Bishop Lechpammer Demos and Microscopy with ultraviolet surface excitation for rapid free histology Nat Biomed Eng rio leiros Garcia Cipriano Fabro Yoshida and radiomics for prediction of logic subtype and metastatic disease in primary malignant lung neoplasms Int Comput Assist Radiol Surg Fu Jung Torne Gonzalez hringer Shmatko Yates Moore and Gerstung cer computational histopathology reveals mutations tumor composition and prognosis Nat Cancer Geessink Baidoshvili Klaase Ehteshami Bejnordi Litjens van Pelt Mesker Nagtegaal Ciompi and van der Laak Computer aided quantiﬁcation of intratumoral stroma yields an independent prognosticator in rectal cancer Cell Oncol Cancer Cell October Review Ghassemi and Beam The false hope of current approaches to explainable artiﬁcial intelligence in health care Lancet Digit Health Chae Cha Kim Shin and Choi Association of brca mutation types imaging features and pathologic ﬁndings in patients with breast cancer with and mutations AJR Am Roentgenol Adam Hosny Khodakarami Waldron Wang McIntosh Goldenberg Kundaje Greene and Broderick Transparency and reproducibility in artiﬁcial intelligence Nature Haskins Kruger and Yan Deep learning in medical image registration survey Mach Vis Appl Havaei Guizard Chapados and Hemis image segmentation Image Computing and Intervention Springer pp In In national Conference on Medical Liu Li Li Yang and Zhang Noninvasive kras mutation estimation in colorectal cancer using deep learning method based on ct imaging BMC Med Imaging Hinton Dvorak Roberts French Grubbs Cress Tiwari and Nagle method to reuse archived h stained histology slides for multiplex protein biomarker analysis Methods Protoc Hollon Pandian Adapa Urias Save Khalsa Eichberg D Amico Farooq Lewis et Near intraoperative brain tumor diagnosis using stimulated raman ogy and deep neural networks Nat Med Hosny Parmar Quackenbush Schwartz and Aerts Artiﬁcial intelligence in radiology Nat Rev Cancer Howard Dolezal Kochanny Schulte Chen Heij Huo Nanda Olopade Kather et The impact of digital histology signatures on deep learning model accuracy and bias Nat Commun Huang Pareek Seyyedi Banerjee and Lungren Fusion of medical imaging and tronic health records using deep learning systematic review and implementation guidelines NPJ Digit Med Hyun Ahn Koh and Lee approach using radiomics to predict the histological subtypes of lung cancer Clin Nucl Med Ilse Tomczak and Welling deep multiple instance learning In In International conference on machine learning PMLR pp Iv Kapoor and Ghosh Multimodal Classiﬁcation rent Landscape Taxonomy and Future Directions ACM Computing veys CSUR Jain and Massoud Predicting tumour mutational burden from histopathological images using multiscale deep learning Nat Mach tell Jang Lee Kang Song and Lee Prediction of clinically actionable genetic alterations from colorectal cancer histopathology images using deep learning World Gastroenterol Jing and Tian visual feature learning with deep neural networks survey Preprint at arXiv Joo Ko Kwon Jeon Jung Kim Chung and Im Multimodal deep learning models for the prediction of pathologic response to neoadjuvant chemotherapy in breast cancer Sci Joze Shaban Iuzzolino and Koishida Mmtm modal transfer module for cnn fusion In In Proceedings of the ference on Computer Vision and Pattern Recognition pp Karpathy Toderici Shetty Leung Sukthankar and video classiﬁcation with convolutional neural networks In Review ll OPEN ACCESS In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition pp Kather Heij Grabsch Loefﬂer Echle Muti Krause Niehues Sommer Bankhead et detection of clinically actionable genetic alterations Nat Cancer Kennedy and Salama review of cancer immunotherapy toxicity CA Cancer Clin Khosravi Lysandrou Eljalby Li Kazemi Zisimopoulos Sigaras Brendel Barnes Ricketts et deep learning approach to diagnostic classiﬁcation of prostate cancer using ology fusion Magn Reson Imaging Kirkpatrick New clues in the acetaminophen mystery Nat Rev Drug Discov Kumar Fulham Feng and Kim J feature fusion maps from images of lung cancer IEEE Trans Med Imaging Lai Chen Hsu Lin Tsao and Wu Overall survival prediction of cell lung cancer by integrating microarray and clinical data with deep learning Sci Lasocki Tsui Tacey Drummond Field and Gaillard Mri grading versus histology dicting survival of world health nization grade astrocytomas AJNR Am Neuroradiol Chen Wang Wang Liu Cheng and Yang X Automated diagnosis of prostate cancer in mri based on multimodal convolutional neural networks Phys Med Biol Lei Huang Li Li Bian Chou Qin Zhou Gong and Cheng attention neural network for anatomy mentation in whole breast ultrasound Med Image Anal Li Bera Toro Fu Zhang Lu Feldman Ganesan Goldstein Davidson et Collagen ﬁber orientation disorder from h images is prognostic for early stage breast cancer clinical trial dation NPJ Breast Cancer Li Chen Tang Landman and Zhou Transforming medical imaging with ers comparative review of key properties current progresses and future perspectives Preprint at arXiv Marcus Lemery Keegan and Pazdur Fda approval summary pembrolizumab for the treatment of microsatellite solid tumors Clin Cancer Res Mehrabi Morstatter Saxena Lerman and Galstyan survey on bias and fairness in machine learning ACM Comput Surv Men Geng Zhong Fan Lin and Xiao deep learning model for predicting xerostomia due to radiation therapy for head and neck squamous cell carcinoma in the rtog clinical trial Int Radiat Oncol Biol Phys Miller Breaking Down Barriers Mo Cai Lin Tong Chen Wang Hu Iwamoto and Chen modal priors guided segmentation of liver lesions in mri using mutual information based graph networks In In International Conference on Medical Image Computing and Assisted Intervention Springer pp Mobadersany Youseﬁ Amgad Gutman zquez Vega Brat and Cooper Predicting cancer outcomes from histology and genomics using convolutional networks Proc Natl Acad Sci USA Murchan Brien Connell McNevin Baird Sheils Broin and Finn Deep learning of histopathological tures for the prediction of tumour molecular genetics Diagnostics Naik Madani Esteva Keskar Press Ruderman Agus and Socher Deep breast cancer monal receptor status determination from h stains Nat mun Nie Zhang Adeli Liu and Shen deep learning for vival time prediction of brain tumor patients In In International conference on medical image computing and sisted intervention Springer pp Nie Lu Zhang Adeli Wang Yu Liu Wang Wu and Shen deep feature learning for survival time prediction of brain tumor patients using neuroimages Sci Paik Shak Tang Kim Baker Cronin Baehner Walker Watson Park et multigene assay to predict recurrence of breast cancer Engl Med Liang Li Chen and Zeng J Integrative data analysis of cancer data with multimodal deep learning approach ACM Trans Comput Biol Bioinform Placido Yuan Hjaltelin Haue Yuan Kim Umeton Antell Chowdhury Franz et Pancreatic cancer risk dicted from disease trajectories using deep learning Preprint at bioRxiv Linardatos Papastefanopoulos and Kotsiantis review of machine learning pretability methods Entropy Angelikopoulos Wu Alberts Wiestler Diehl ibisch Pyka Combs Hadjidoukas et Personalized radiotherapy design for glioblastoma integrating mathematical tumor models multimodal scans and bayesian inference IEEE Trans Med Imaging Loefﬂer Bruechle Jung Seillier Rose Laleh Knuechel Brinker Trautwein Gaisa et Artiﬁcial Detection of Mutational Status Directly from Routine Histology in Bladder Cancer Possible Preselection for Molecular Testing European Urology Focus Louis Perry Reifenberger Von Deimling Cavenee Ohgaki Wiestler Kleihues and Ellison The world health organization classiﬁcation of tumors of the central nervous system summary Acta Neuropathol Low Harnessing consumer smartphone and wearable sensors for clinical cancer research NPJ Digit Med Lu Chen Williamson Zhao Shady Lipkova and Mahmood pathology predicts origins for cancers of known primary Nature Lu Williamson Chen Chen Barbieri and Mahmood and weakly supervised computational pathology on images Nat Biomed Eng Qi Ke Yu Cao Lai Chen Gao and Wang X Identiﬁcation of prognostic spatial organization features in colorectal cancer microenvironment using deep learning on histopathology images Med Omics Qian Pei Zheng Xie Yan Zhang Gao Zhang Zheng et Prospective assessment of breast cancer risk from multimodal multiview ultrasound images via clinically applicable deep learning Nat Biomed Eng Rakha Lee Elston Grainge Hodi Blamey and Ellis Prognostic signiﬁcance of Nottingham tologic grade in invasive breast carcinoma J Clin Oncol Ramachandram and Taylor Deep multimodal learning vey on recent advances and trends IEEE Signal Process Mag Ramanathan Hossen Sayeed et bayes based multiple parallel fuzzy reasoning method for medical diagnosis Eng Sci Technol Reda Khalil Elmogy Abou Shalaby Abou Elmaghraby Ghazal and Deep learning role in early diagnosis of prostate cancer Technol Cancer Res Treat Reyes Meier Pereira Silva Dahlweid von bligk Summers and Wiest On the interpretability of cial intelligence in radiology challenges and opportunities Radiol Artif Intell Cancer Cell October ll OPEN ACCESS Review Rokach and Maimon Clustering methods In In Data mining and knowledge discovery handbook Springer pp Roy Lahiri Maji and Biswas J Recurrent glioblastoma where we stand South Asian J Cancer Sammut Chin Provenzano Bardwell Ma Cope Dariush Dawson Abraham et machine learning predictor of breast cancer therapy response Nature Schmauch Romagnoni Pronier Saillard Calderaro Kamoun Sefta Toldo Zaslavskiy et deep learning model to predict expression of tumours from whole slide ages Nat Commun Sedghi Mehrtash Jamzad Amalou Wells Kapur Kwak Turkbey Choyke Pinto et Improving tion of prostate cancer foci via information fusion of mri and temporal enhanced ultrasound Int Comput Assist Radiol Surg Selvaraju Das Vedantam Cogswell Parikh and cam Why did you say that Preprint at arXiv Selvaraju Cogswell Das Vedantam Parikh and Batra visual explanations from deep networks via localization In In Proceedings of the IEEE international conference on puter vision pp Vo Yuan Wong and Nguyen Multimodal Breast Lesion Classiﬁcation Using Deep Networks In In IEEE EMBS International Conference on Biomedical and Health matics BHI IEEE pp Wang Shi Ye Dong Yu Zhou Liu Gevaert Wang Zhu et Predicting egfr mutation status in lung noma on computed tomography image using deep learning Eur Respir J Wang Chen Gao Zhang Guan Dong Zheng Jiang Yang Wang et ing gastric cancer outcome from sected lymph node histopathology images using deep learning Nat Commun Weeks Dua Hutchison Joshi Li Szejer and Azevedo gestible and wearable sensing platform to measure medication adherence and physiological signals In In Annual International Conference of the IEEE Engineering in Medicine and Biology Society EMBC IEEE pp Wu Wu Daneshjou Ouyang Ho and Zou J How medical ai devices are evaluated limitations and recommendations from an analysis of fda approvals Nat Med Xu Jayaraman and Rogers Skin Sensors Are the Future of Health Care Sha Gong Qiu Duan Li and Yin Identifying ological subtypes of lung cancer by using the radiomic features of positron emission computed tomography Transl Cancer Res Xu Zhang Huang Zhang and Metaxas Multimodal Deep Learning for Cervical Dysplasia Diagnosis In In International conference on medical image computing and intervention Springer pp Shamshad Khan Zamir Khan Hayat Khan and Fu Transformers in medical imaging survey Preprint at arXiv Yala Lehman Schuster Portnoi and Barzilay deep learning model for improved breast cancer risk tion Radiology Shao Cheng Cheng Wang Sun Lu Zhang Zhang and Huang Integrative analysis of pathological images and genomic data for cancer prognosis IEEE Trans Med Imaging Yamamoto Tsuzuki Akatsuka Ueki Morikawa Numata Takahara Tsuyuki sumi Nakazawa et mated acquisition of explainable knowledge from unannotated histopathology images Nat Commun Shergalis Bankhead Luesakul Muangsin and Neamati Current challenges and opportunities in treating glioblastoma Rev Sidhom Larman Pardoll and Baras DeepTCR is deep learning framework for revealing sequence concepts within repertoires Nat Commun Sundararajan Taly and Yan Q Axiomatic attribution for deep networks In In International conference on machine learning PMLR pp Taqi Sami Sami and Zaki review of artifacts in histopathology J Oral Maxillofac Pathol Topol Welcoming new guidelines for ai clinical research Nat Med Tsou and Wu Mapping driver mutations to histopathological subtypes in papillary thyroid carcinoma applying deep convolutional neural network Clin Med and Rohr cancer survival prediction ing multimodal deep learning Sci Van Cutsem hne Hitre Zaluski Chang Chien D Haens r Lim Bodoky et Cetuximab and chemotherapy as initial treatment for metastatic colorectal cancer Engl Med Overseas Ed Van der Laak Litjens and Ciompi Deep learning in thology the path to the clinic Nature medicine Vasileiou Costa Long Wetzler Hoyer Kraus Popp Emons Wunderle Wenkel et Breast mri texture ysis for prediction of genetic risk BMC Med Imaging Yan Zhang Zhang Cheng Liu Wang Dong Zhang Mo Chen et Quantitative radiomics for vasively predicting molecular subtypes and survival in glioma patients NPJ Precis Oncol Yap Yolland and Tschandl Multimodal skin lesion tion using deep learning Exp Dermatol Yogananda Shah Yu Pinho Nalawade gesan Wagner Mickey Patel Fei et novel fully automated method for classiﬁcation of status in brain gliomas Neurooncol Adv Supplement Zhang Lemoine and Mitchell Mitigating unwanted biases with adversarial learning In In Proceedings of the Conference on AI Ethics and Society pp Zhang Tong Xu and Maciejewski Graph convolutional networks comprehensive review Comput Soc Netw Zhou and Luo Deep Features Fusion with Mutual Attention Transformer for Skin Lesion Diagnosis In In IEEE International ence on Image Processing ICIP IEEE pp Zhu Park Isola and Efros Unpaired image translation using adversarial networks In In ceedings of the IEEE international conference on computer vision pp Zhuang Lipkova Chen and Mahmood Deep based integration of histology radiology and genomics for improved survival prediction in glioma patients In In Medical Imaging Digital and tational Pathology SPIE Vaswani Shazeer Parmar Uszkoreit Jones Gomez Kaiser and Polosukhin I tion is all you need Adv Neural Inf Process Syst Zitnik Nguyen Wang Leskovec Goldenberg and Hoffman Machine learning for integrating data in biology and medicine Principles practice and opportunities Inf Fusion Cancer Cell October,,[]
Multimodal Conversational AI Survey of Datasets and Approaches Anirudh S Sundar Larry Heck Department of Electrical and Computer Engineering Georgia Institute of Technology larryheck M G L s c v v i X r Abstract As humans we experience the world with all our senses or modalities sound sight touch smell and taste We use these modalities particularly sight and touch to convey and interpret speciﬁc meanings Multimodal pressions are central to conversations rich set of modalities amplify and often sate for each other multimodal tional AI system answers questions fulﬁlls tasks and emulates human conversations by understanding and expressing itself via ple modalities This paper motivates deﬁnes and mathematically formulates the multimodal conversational research objective We provide taxonomy of research required to solve the objective multimodal representation fusion alignment translation and We survey datasets and approaches for each research area and highlight their iting assumptions Finally we identify modal as promising direction for multimodal conversational AI research Introduction The proliferation of smartphones dramatically increased the frequency of interactions that humans have with digital content These interactions have expanded over the past decade to include tions with smartphones and smart ers Conversational AI systems Alexa Siri Google Assistant answer questions fulﬁll ciﬁc tasks and emulate natural human conversation et Gao et Early examples of conversational AI include those based on primitive methods such as ELIZA Weizenbaum More recently conversational systems were driven by cal machine translation systems translating input queries to responses Ritter et Tür et Orders of magnitude more data led to unprecedented advances in conversational technology in the of the last decade niques were developed to mine conversational ing data from the web search stream et Heck Tür et and knowledge graphs Heck and et With this increase in data deep networks gained momentum in conversational tems Mesnil et Heck and Huang Sordoni et Vinyals and Shang et Serban et Li et b Most recently specialized deep conversational agents were developed primarily for three tasks tasks in research systems Shah et Eric et Liu et Li et et Wu et Peng et Xu et and commercial products Siri tana Alexa and Google Assistant answering Yi et Raffel et heer et and tions Wolf et Zhou et wardana et Paranjape et Roller et Bao et Henderson et Zhang et However developing single system with uniﬁed approach that achieves performance on all three tasks proven elusive and is still an open problem in versational AI One limitation of existing agents is that they often rely exclusively on language to cate with users This contrasts with humans who converse with each other through multitude of senses These senses or modalities complement each other resolving ambiguities and emphasizing ideas to make conversations meaningful Prosody auditory expressions of emotion and backchannel agreement supplement speech biguates unclear words gesticulation makes spatial references and signify celebration Alleviating this unimodal limitation of sational AI systems requires developing methods to extract combine and understand information streams from multiple modalities and generate timodal responses while simultaneously ing an intelligent conversation Similar to the taxonomy of multimodal machine learning research Baltrušaitis et the research required to extend conversational AI tems to multiple modalities can be grouped into ﬁve areas Representation Fusion Translation Alignment and Representation and fusion involve learning mathematical constructs to mimic sensory modalities Translation maps relationships between modalities for reasoning Alignment identiﬁes regions of evance across modalities to identify dences between them exploits the synergies across modalities by leveraging rich modalities to train modalities Concurrently it is necessary for the research eas outlined above to address four main challenges in multimodal conversational reasoning biguation response generation coreference lution and dialogue state tracking Kottur et Multimodal disambiguation and response generation are challenges associated with fusion that determine whether available multimodal inputs are sufﬁcient for direct response or if queries are required Multimodal coreference lution is challenge in both translation and ment where the conversational agent must resolve referential mentions in dialogue to corresponding objects in other modalities Multimodal dialogue state tracking is holistic challenge across research areas typically associated with tems The goal is to parse multimodal signals to infer and update values for slots in user utterances In this paper we discuss the taxonomy of search challenges in multimodal Conversational AI as illustrated in Figure Section provides history of research in multimodal conversations In Section we mathematically formulate modal conversational AI as an optimization lem Sections and survey existing datasets and approaches for multimodal resentation and fusion translation and alignment Section highlights limitations of existing research in multimodal conversational AI and explores timodal as promising direction for research Figure Taxonomy of multimodal Conversational AI research Background Early work in multimodal conversational AI cused on the use of visual information to improve automatic speech recognition ASR One of the earliest papers along these lines is by Yuhas et followed by many papers including work by Meier et Duchnowski et Bregler and Konig and Ngiam et Advances in capabilities enabled ASR systems to utilize other modalities such as tactile voice and text inputs These systems ported more comprehensive interactions and tated higher degree of personalization Examples include ESPRIT s MASK Lamel et crosoft s MiPad Huang et and AT T s MATCH Johnston et tasks motivated research in adding visual understanding technology into tional AI systems Early work in reasoning over include work by Ramanathan et where they leveraged these combined modalities to address the problem of assigning names of people in the cast to tracks in TV videos Kong et leveraged natural language scrptions of videos for semantic ing Srivastava and Salakhutdinov oped multimodal Deep Boltzmann Machine for retrieval and ASR using videos Antol et introduced dataset and baselines for multimodal challenge bining computer vision and natural language ImageTextVideoAudioScene MaskDialogueSpeechAction T T T hmn TMultimodal FusionMultimodal Representation LearningImageEmbedderTextEmbedderVideo Embedder AudioEmbedder T T T zmn TMultimodalAlignment MultimodalAlignment MultimodalAlignment MultimodalTranslation MultimodalTranslation Multimodal cessing More recent work by Zhang et and Selvaraju et leveraged conversational explanations to make vision and language models more grounded resulting in improved visual tion answering While modalities most commonly considered in the conversational AI literature are text vision tile and speech other sources of information are gaining popularity within the research community These include scans emotion action and dialogue history and virtual reality Heck et and et use gesture speech and to resolve and infer intent in conversational systems Grauman et presents video ing Padmakumar et and Shridhar et present task completion from tions and Gao et presents multisensory object recognition Processing conventional and new modalities brings forth numerous challenges for multimodal conversations To answer these challenges we will ﬁrst mathematically formulate the multimodal versational AI problem then detail fundamental research required to solve it Mathematical Formulation We formulate multimodal conversational AI as an optimization problem The objective is to ﬁnd the optimal response S to message m given ing multimodal context Based on the sufﬁciency of the context the optimal response could be statement of fact or question to resolve ambiguities Statistically S is estimated as S argmax p m r The probability of an arbitrary response r can be expressed as product of the probabilities of sponses ri T over T turns of conversation doni et p m T p c m It is also possible for conversational AI to spond through multiple modalities We represent the multimodality of output responses by trix R i over l permissible output modalities i rl i S argmax p m R Learning from multimodal data requires lating information from all modalities using tion f consisting of ﬁve tion fusion translation alignment and We include these modiﬁcations and present the nal multimodal conversational objective below S argmax p c m R In the following sections we describe each task contained in f Multimodal Representation Fusion Multimodal representation learning and fusion are primary challenges in multimodal conversations Multimodal representation is the encoding of modal data in format amenable to computational processing Multimodal fusion concerns joining features from multiple modalities to make tions Multimodal Representations Using multimodal information of varying larity for conversations necessitates techniques to represent signals in latent space These latent multimodal representations encode man senses to improve conversational AI s ception of the Success in multimodal tasks requires that representations satisfy three desiderata Srivastava and Salakhutdinov Similarity in the representation space implies similarity of the corresponding concepts The representation is easy to obtain in the absence of some modalities It is possible to infer missing information from observed modalities There exist numerous representation methods for the range of problems multimodal tional AI addresses Multimodal representations are broadly classiﬁed as either joint representations or coordinated representations Baltrušaitis et Joint Representations Joint representations combine unimodal signals into the same representation space Traditional techniques to learn joint representations include multimodal autoencoders Ngiam et multimodal deep belief networks Srivastava and Salakhutdinov and sequential networks Nicolaou et The success of the Transformer to represent text Vaswani et and BERT when modeling language Devlin et have inspired ety of multimodal architectures for understanding Sun et Lu et Gabeur et Chen et Tan and Bansal Singh et speech tion Baevski et Hsu et Chan et and User Interface UI standing Bapna et et Bai et Li et Xu et Heck and Heck to create latent features zik n models used as joint modal representations can be described as trated in the taxonomy of Figure Modality ciﬁc encoders ji n embed unimodal tokens cik n tion Decoder networks use latent features to produce output symbols transformer Ψ sists of stacked encoders and decoders with modality attention Attention heads compute tionships within elements of modality producing multimodal representations hik n Equation zin ji cin hmn Ψ zmn Coordinated Representations In contrast coordinated representations model each modality separately Constraints coordinate resentations of separate modalities by enforcing similarity over concepts For ple the audio representation ga of dog s bark would be closer to the dog s image representation gi and further away from car s Equation notion of distance d between modalities in the coordinated space enables retrieval d ga dog gi dog d ga dog gi car In practice contrastive objectives are used to coordinate representations between pairs of ities Contrastive learning been successful in relating separate views of the same image Becker and Hinton Chen et et Grill et Radford et ages and their natural language descriptions ston et Kiros et Zhang et Li et and videos with their sponding audio and natural language descriptions Owens et Korbar et Sun et Miech et Alayrac et Akbari et Xu et Qian et Morgado et Multimodal Fusion Multimodal fusion combines features from ple modalities to make decisions denoted by the nal block before the outputs in Figure Fusion proaches are broadly classiﬁed into and methods methods are independent of ciﬁc algorithms and are split into early late and hybrid fusion Early fusion integrates features lowing extraction projecting features into shared space Potamianos et Ngiam et Nicolaou et Jansen et In trast late fusion integrates decisions from unimodal predictors Becker and Hinton Korbar et Shuster et Alayrac et Akbari et Early fusion is predominantly used to combine features extracted in joint tations while late fusion combines decisions made in coordinated representations Hybrid fusion ploits both low and high level modality interactions Wu et Schwartz et vanni et Goyal et methods consist of graphical niques like Hidden Markov Models Neﬁan et Gurban et neural networks laou et Antol et Gao et Malinowski et Kottur et Qian et and transformers Xu and Saenko Hori et Peng et Zhang et Shuster et Chen et Geng et Xu et Models for Conversational AI Having introduced the multimodal representation and fusion challenges we present the art in these for conversational AI Factor Graph Attention Schwartz et develops Factor Graph tention FGA joint representation for question answering grounded in images FGA beds images using Simonyan and serman or Ren et and textual modalities using LSTMs Nodes in the tor graph represent attention distributions over ements of each modality and factors capture tionships between nodes There are two types of factors local and joint Local factors capture interactions between nodes of single modality words in the same sentence while joint factors capture interactions between different modalities word in sentence and an object in an image Representations from all modalities are nated via hybrid fusion and passed through layer perceptron network to retrieve the best date answer Table compares the R k of criminative models on VisDial The version of FGA is the Model LF Das et HRE Das et Memory Network Das et CorefNMN Kottur et NMN Hu et FGA Schwartz et R R R Table Comparison of models on VisDial Recall k Schwartz et TRANSRESNET Shuster et presents TRANSRESNET for dialogue dialogue is the task of choosing the optimal response on alogue turn given an image an agent personality and dialogue history TRANSRESNET consists of separately learned to represent input modalities Images are encoded using ResNeXt trained on billion Instagram images Xie et personalities are embedded using linear layer and dialogue is encoded by former pretrained on Reddit Mazaré et to create joint representation TRANSRESNET compares and fusion by using either concatenation or attention networks to combine representations Like FGA the chosen dialogue response is the candidate closest to the fused representation On the ﬁrst turn TRANSRESNET uses only style and image information to produce responses alogue history serves as an additional modality on subsequent rounds Ablation of one or more modalities diminishes the ability of the model to retrieve the correct response Optimal performance on Shuster et is achieved using multimodal concatenation of jointly sented modalities Table Modalities Image Only Style Only Dialogue History Only Style Dialogue Image Dialogue Image Style Style Dialogue Image Turn Turn Turn All Table TRANSRESNETRET Recall on using MultiModal Versatile Networks MMV Alayrac et presents training strategy to learn coordinated representations using supervised contrastive learning from instructional videos Videos are encoded using TSM with backbone Lin et audio is encoded using log MEL spectrograms from and text is encoded using Google News Mikolov et Alayrac et deﬁnes three types of dinated spaces shared disjoint and The shared space enables direct comparison and navigation between modalities by assuming equal granularity The disjoint space sidesteps tion to solve the granularity problem by creating space for each pair of modalities The space solves both issues by learning two spaces space compares audio and video while space compares embeddings with text We further discuss the MMV model in Section Multimodal Translation Multimodal translation maps embeddings from one modality to signals from another for reasoning Figure reasoning ables multimodal conversational AI to hold ingful conversations and resolve references across multiple senses speciﬁcally language and vision To this end we survey existing work addressing the translation of images and videos to text We discuss multimodal and timodal dialogue translation tasks that extend to multimodal conversations Image Antol et and Zhu et present sual VQA and for multimodal question answering MQA The MQA challenge requires responding to textual queries about an image Both datasets collect questions and answers using crowd workers encouraging trained models to learn natural responses Heck and Heck presents the Visual Slot dataset where trained models learn answers to questions grounded in UIs The objective of MQA is simpliﬁcation of Equation to nario T producing response to question mq given multimodal context ci n SM QA argmax R p cn mq MTQA is the next step towards multimodal conversational AI VisDial Das et extends VQA to multiple turns translating over QA history in addition to images GuessWhat Vries et is guessing game discovering objects in scene through dialogue MANYMODALQA Hannan et requires reasoning over prior edge images and databases MIMOQA Singh et is an example of multimodal sponses where answers are pairs is The objective of MTQA Equation an extension of MQA to include QA history hqa SM T QA argmax R p cn hqa Conversations IGC Mostafazadeh et builds on MTQA by presenting dataset for multimodal dialogue MD machine perception and conversation through language Shuster et extends IGC to agents with personalities Crowd workers hold conversations about an image with one of emotions peaceful erratic skeptical Motivated by the popularity of visual content in Meme incorporated Dialogue MOD Fei et contains natural language conversations interspersed with behavioral stickers SIMMC Moon et and Kottur et present dialogue for shopping The challenge requires leveraging dialogue and state of the world to resolve references track dialogue state and recommend the correct object IGC MOD SIMMC and solve the MD objective that depends on previous dialogue responses hd SM D argmax R p cn hd Video An extension of VQA to the video domain cludes TVQA Lei et built on TV shows MovieQA Tapaswi et based on movies and Audio Visual log AVSD Alamri et based on RADES Sigurdsson et DVD et presents over videos synthesized from the CATER dataset Girdhar and Ramanan Besides visual reasoning quires temporal reasoning challenge addressed by multimodal alignment that we discuss in the following section Multimodal Alignment While dialogue revolves around jects cats and dogs dialogue volves around objects and associated actions jumping cats and barking dogs where spatial and temporal features serve as building blocks for versations Extracting these spatiotemporal tures requires multimodal alignment aligning of different modalities to ﬁnd respondences We identify action recognition and action from modalities as alignment challenges evant to multimodal conversations Action Recognition Action recognition is the task of extracting ral language descriptions from videos Soomro et Kuehne et and Carreira et volve extracting actions from short YouTube and Hollywood movie clips Miech et Xu et and Zhou et are datasets taining instructional videos on the internet and quire learning embeddings and are annotated by hand while uses existing video subtitles or ASR Mathematically the goal is to retrieve the rect natural language description to query video x Equation Video and text tion functions g video and g text embed ties into coordinated space where they are pared using distance measure argmin d gvideo x gtext yj Action from Modalities Equipping multimodal conversational agents with the ability to perform actions from multiple ities provides them with an understanding of the real world improving their conversational utility Talk the Walk Vries et presents the task of navigation conditioned on partial formation tourist provides descriptions of environment to guide who termines actions Navigation Thomason et contains natural dialogues grounded in simulated environment The task is to predict sequence of actions to goal state given the world scene dialogue and previous actions TEACh Padmakumar et extends Navigation to complete tasks in an THOR simulation The challenge involves aligning information from language video as well as action and dialogue history to solve daily tasks Grauman et contains text annotated centric ﬁrst person videos in scenarios includes scans multiple camera views and eye gaze presenting new representation sion translation and alignment challenges It is associated with ﬁve benchmarks Video QA object state tracking diarization social cue detection and camera trajectory forecasting Multimodal Versatile Networks MMV In addition to representation Alayrac et presents task to train ity embedding graphs for multimodal alignment Sampling temporally aligned audio visual clips and narrations from the same video creates itive training examples while those from ent videos comprise negative training examples Estimation NCE loss mann and Hyvärinen is minimized to ensure similarity between embeddings of positive training examples while forcing negative pairs further apart Multiple Instance Learning MIL Miech et variant of NCE measures loss on pairs of modalities of different granularity MIL accounts for misalignment between and text by measuring the loss of information with multiple temporally close narrations The network is trained on Miech et and AudioSet Gemmeke et Table compares the performance of MMV on action classiﬁcation audio classiﬁcation and shot retrieval Discussion The current datasets used for research in modal conversational AI are summarized in Table While MQA and MTQA are promising starting points for multimodal natural language tasks tending QA to conversations is not straightforward Inherently MQA limits itself to direct questions targeting visible content whereas multimodal versations require understanding information that is often implied Mostafazadeh et terances in dialogue represent speech acts and are classiﬁed as constatives directives commissives or acknowledgments Bach and Harnish Answers belong to single speech act constatives and represent subset of natural conversations Similarly the work on action recognition is incomplete and insufﬁcient for conversational systems Conversational AI must represent and understand spatiotemporal interactions However current research in action recognition attempts to learn relationships between videos and their natural language descriptions These descriptions are not speech acts themselves Therefore they do not adequately represent dialogue but rather only serve as anchor points in the interaction In contrast Shuster et presents learning challenge directly aligned with the multimodal dialogue objective in Equation treats dialogue as an cussion grounded in the visual modality ing in the task requires jointly optimizing visual and conversational performance The use of crowd workers that adopt personalities during data lection encourages natural dialogue and captures conversational intricacies and implicatures MQA answers explicit questions about an age this at farm and action tion describes videos biking On the other hand requires both implicit knowledge Halloween Exercise and reasoning Halloween Holiday Exercise Fitness Despite its advantages over other datasets makes three assumptions about modal conversations limiting its extension to the multimodal conversational objective Model Miech et AVTS Korbar et CC Jansen et CVRL Qian et XDC Alwassel et ELo Piergiovanni et AVID Morgado et GDT Patrick et MMV FAC Alayrac et FT FT Linear AS Table Comparison of learnt representations on AudioSet and Accuracy for mean Average Precision mAP for AudioSet Recall for and Alayrac et Dataset VQA Antol et Zhu et Visual Slot Heck and Heck TVQA Lei et MovieQA Tapaswi et MANYMODALQA Hannan et MIMOQA Singh et VisDial Das et Guesswhat Vries et AVSD Alamri et DVD et SIMMC Moon et Kottur et IGC Mostafazadeh et Shuster et MOD Fei et Soomro et Kuehne et Kinetics Carreira et Miech et Zhou et MSRVTT Xu et Talk the Walk Vries et CVDN Thomason et TEACh Padmakumar et Grauman et Modalities I Q I Q UI Q V Q S V C Q T S I C Q T Tables I Q T I HQ C Q I HQ V HQ C Q V Q HQ HD Q VR HD Q VR I Q D I D Personality D Personality V partial V V V T S V T V T I Actions D VR Actions HQ Scene Actions D V T Scan S Data Collection Task Question Answering Question Answering Human Question Answering Question Answering Human Question Answering Question Answering Machine Question Answering Question Answering Question Answering Question Answering Machine Question Answering Machine Shopping Machine Shopping Question Answering Visual YouTube Action Recognition Action Recognition YouTube Action Recognition Embeddings YouTube retrieval activity recognition YouTube Navigation from Actions and Dialogue Navigation from Dialogue History Action prediction Task from language Spatial Reasoning Web videos Human POV Third Person Third Person X Third Person Third Person Third Person X Third person Third Person Third Person X First Person First Person Third Person X Third Person Third Person Third Person Person Third Person Person First Person First Person Person First Person Table Datasets for multimodal representations Interface HQ Question history history history Reality Conversations are limited to three turns void of dialogue dependencies Language and images are the only modalities Personalities are independent of previous sponses This differs from natural human versations where humans tend to understand and reciprocate the personality of dialogue partner Rashkin et In addition algorithmic improvements are quired to advance the ﬁeld of multimodal tional AI particularly with respect to the objective function Current approaches such as MQA and action recognition models optimize limited jective compared to Equation We postulate that the degradation of these methods when applied to multimodal conversations is largely caused by this and therefore motivates investigation The discussion above highlights the limitations of existing datasets for the multimodal tional AI task Datasets need to be improved to better capture and represent more natural turn dialogues over multiple modalities dialogues that more closely resemble how humans converse with each other and their environment Another open research problem is to improve performance on The current TRANSRESNETRET is limited The model often hallucinates referring to content missing in the image and previous dialogue turns The model also struggles when answering questions and ing extended conversations We suspect these lems are reﬂection of the limiting assumptions makes and the absence of multimodal to extract relationships between ties For further details we refer readers to example conversations in Appendix Different modalities often contain tary information when grounded in the same cept Multimodal exploits this modality synergy to model ities using modalities An example of in context of Figure is the use of visual information and audio to generate alized text representations Blum and Mitchell introduced an early approach to multimodal using tion from hyperlinked pages for cation Socher and and Duan et presented techniques to tag images given information from other ties Kiela et grounded natural language descriptions in olfactory data More recently hyay et jointly trains bilingual models to accelerate spoken language understanding in low resource languages Selvaraju et uses human attention maps to teach QA agents where to look Despite the rich history of work in modal extending these techniques to develop multimodal conversational AI that stands and leverages relationships is still an open challenge Conclusions We deﬁne multimodal conversational AI and line the objective function required for its tion Solving this objective requires multimodal representation and fusion translation and ment We survey existing datasets and methods for each We identify simplifying assumptions made by existing research preventing the realization of multimodal tional AI Finally we outline the collection of suitable dataset and an approach that utilizes modal as future steps Hassan Akbari Liangzhe Yuan Rui Qian Chuang Chang Yin Cui and Boqing Gong VATT Transformers for Multimodal Learning from Raw Video Audio cs eess ArXiv and Text Huda Alamri Vincent Cartillier Abhishek Das Jue Wang Anoop Cherian Irfan Essa Dhruv Batra Tim Marks Chiori Hori Peter Anderson Stefan Lee and Devi Parikh cs ArXiv Aware Dialog Alayrac Adrià Recasens Rosalia Schneider Relja Jason Ramapuram Jeffrey Fauw Lucas Smaira Sander Dieleman and Andrew Zisserman tiModal Versatile Networks cs ArXiv Humam Alwassel Dhruv Mahajan Bruno Korbar Lorenzo Torresani Bernard Ghanem and Du Tran Learning by cs Clustering ArXiv Stanislaw Antol Aishwarya Agrawal Jiasen Lu garet Mitchell Dhruv Batra Lawrence Zitnick and Devi Parikh VQA Visual Question swering In IEEE International Conference on Computer Vision ICCV pages ago Chile IEEE Kent Bach and Robert Harnish Linguistic Communication and Speech Acts Cambridge MIT Press Alexei Baevski Henry Zhou Abdelrahman Mohamed and Michael Auli work for Learning of Speech resentations cs eess ArXiv Chongyang Bai Xiaoxue Zang Ying Xu Srinivas Sunkara Abhinav Rastogi Jindong Chen and Blaise Aguera Arcas UIBert ing Generic Multimodal Representations for UI cs ArXiv Understanding Tadas Baltrušaitis Chaitanya Ahuja Philippe Morency chine Learning cs ArXiv and Multimodal Survey and Taxonomy References Daniel Adiwardana Luong David So Jamie Hall Noah Fiedel Romal Thoppilan Zi Yang Apoorv Kulshreshtha Gaurav Nemade Yifeng Lu and Quoc Towards Chatbot cs stat ArXiv Siqi Bao Huang Fan Wang Hua Wu and Haifeng Wang PLATO logue Generation Model with Discrete Latent able cs ArXiv Ankur Bapna Gokhan Tur Dilek and Larry Heck Towards frame mantic parsing for domain scaling arXiv preprint Suzanna Becker and Geoffrey Hinton organizing neural network that discovers surfaces in stereograms Nature Avrim Blum and Tom Mitchell Combining In labeled and unlabeled data with Proceedings of the eleventh annual conference on Computational learning theory COLT pages Madison Wisconsin United States ACM Press Bregler and Konig Eigenlips for bust speech recognition In Proceedings of ICASSP IEEE International Conference on Acoustics Speech and Signal Processing volume ii pages Joao Carreira Eric Noland Chloe Hillier and Andrew Zisserman Short Note on the Human Action Dataset cs ArXiv David Chan Shalini Ghosh Debmalya Chakrabarty and Björn Hoffmeister for Automated Speech Recognition cs eess ArXiv Ting Chen Simon Kornblith Mohammad Norouzi and Geoffrey Hinton Simple Framework for Contrastive Learning of Visual Representations cs stat ArXiv Chen Linjie Li Licheng Yu Ahmed Kholy Faisal Ahmed Zhe Gan Yu Cheng and Jingjing Liu UNITER UNiversal TExt Representation Learning cs ArXiv Abhishek Das Satwik Kottur Khushi Gupta Avi Singh Deshraj Yadav José Moura Devi Parikh and Dhruv Batra Visual Dialog cs ArXiv Harm Vries Kurt Shuster Dhruv Batra Devi Parikh Jason Weston and Douwe Kiela Talk the Walk Navigating New York City through Grounded Dialogue ArXiv cs Harm Vries Florian Strub Sarath Chandar Olivier Pietquin Hugo Larochelle and Aaron Courville Visual object discovery through dialogue cs ArXiv GuessWhat Jacob Devlin Chang Kenton Lee and Kristina Toutanova BERT of Deep Bidirectional Transformers for Language cs ArXiv Understanding Kun Duan David Crandall and Dhruv Batra Multimodal Learning in Web In IEEE Conference on Computer Images Vision and Pattern Recognition pages Columbus OH USA IEEE Duchnowski Uwe Meier and Alex Waibel See Hear Integrating Automatic Speech Recognition and In Proceedings of International Conference on Spoken Language cessing ICSLP pages Ali Xiaohu Liu Ruhi Sarikaya Gokhan Tur Dilek and Larry Heck tending domain coverage of language ing systems via intent transfer between domains ing knowledge graphs and search query click logs In IEEE International Conference on tics Speech and Signal Processing ICASSP pages IEEE Mihail Eric Lakshmi Krishnan Francois Charette and Christopher Manning Retrieval Networks for Dialogue In ings of the Annual SIGdial Meeting on course and Dialogue pages Saarbrücken Germany Association for Computational tics Zhengcong Fei Zekang Li Jinchao Zhang Yang Feng and Jie Zhou Towards Expressive Communication with Internet Memes New Multimodal Conversation Dataset and Benchmark cs ArXiv Valentin Gabeur Chen Sun Karteek Alahari and Cordelia Schmid Transformer for Video Retrieval cs ArXiv Haoyuan Gao Junhua Mao Jie Zhou Zhiheng Huang Lei Wang and Wei Xu Are You Talking to Machine Dataset and Methods for Multilingual Image Question Answering cs ArXiv Jianfeng Gao Michel Galley and Lihong Li Neural Approaches to Conversational AI cs ArXiv Ruohan Gao Chang Shivani Mall Li and Jiajun Wu ObjectFolder Dataset of Objects with Implicit Visual Auditory and Tactile Representations cs ArXiv Jort Gemmeke Daniel Ellis Dylan Freedman Aren Jansen Wade Lawrence Channing Moore Manoj Plakal and Marvin Ritter Audio Set An ontology and dataset for audio events In Proc IEEE ICASSP New Orleans Shijie Geng Peng Gao Moitreya Chatterjee Chiori Hori Jonathan Roux Yongfeng Zhang sheng Li namic Graph Representation Learning for Video Dialog via Shufﬂed Transformers cs ArXiv and Anoop Cherian Rohit Girdhar and Deva Ramanan Cater agnostic dataset for compositional actions and In International Conference on poral reasoning Learning Representations of the Thirteenth International Conference on cial Intelligence and Statistics volume of ings of Machine Learning Research pages Chia Laguna Resort Sardinia Italy PMLR Palash Goyal Saurabh Sahu Shalini Ghosh and Chul Lee Learning for modal Video Categorization cs ArXiv Kristen Grauman Andrew Westbury Eugene Byrne Zachary Chavis Antonino Furnari Rohit har Jackson Hamburger Hao Jiang Miao Liu Xingyu Liu Miguel Martin Tushar Nagarajan ija Radosavovic Santhosh Kumar Ramakrishnan Fiona Ryan Jayant Sharma Michael Wray meng Xu Eric Zhongcong Xu Chen Zhao dhant Bansal Dhruv Batra Vincent Cartillier Crane Tien Do Morrie Doulaty Akshay palli Christoph Feichtenhofer Adriano Fragomeni Qichen Fu Christian Fuegen Abrham Gebreselasie Cristina Gonzalez James Hillis Xuhua Huang Yifei Huang Wenqi Jia Weslie Khoo Jachym lar Satwik Kottur Anurag Kumar Federico dini Chao Li Yanghao Li Zhenqiang Li tikeya Mangalam Raghava Modhugu Jonathan Munro Tullie Murrell Takumi Nishiyasu Will Price Paola Ruiz Puentes Merey Ramazanova Leda Sari Kiran Somasundaram Audrey land Yusuke Sugano Ruijie Tao Minh Vo Yuchen Wang Xindi Wu Takuma Yagi Yunyi Zhu Pablo Arbelaez David Crandall Dima Damen vanni Maria Farinella Bernard Ghanem Vamsi ishna Ithapu Jawahar Hanbyul Joo Kris tani Haizhou Li Richard Newcombe Aude Oliva Hyun Soo Park James Rehg Yoichi Sato Jianbo Shi Mike Zheng Shou Antonio Torralba Lorenzo Torresani Mingfei Yan and Jitendra Malik Around the World in Hours of cs ArXiv centric Video Pierre Richemond Grill Florian Strub Florent Altché Corentin Tallec Elena Buchatskaya Carl Doersch Bernardo Avila Pires Zhaohan Guo Mohammad Gheshlaghi Azar Bilal Piot koray kavukcuoglu Remi Munos and Michal Valko Bootstrap Your Own Latent New In Approach to Learning vances in Neural Information Processing Systems volume pages Curran Associates Mihai Gurban Thiran Thomas man and Thierry Dutoit Dynamic ity weighting for hmms In Proceedings of the speech recognition ternational conference on Multimodal interfaces IMCI page Chania Crete Greece ACM Press Dilek Larry Heck and Gokhan Tur Exploiting query click logs for utterance main detection in spoken language understanding In IEEE International Conference on tics Speech and Signal Processing ICASSP pages IEEE Dilek Larry Heck and Gokhan Tur Using knowledge graph and query click logs for In unsupervised learning of relation detection IEEE International Conference on Acoustics Speech and Signal Processing pages IEEE Dilek Gokhan Tur Rukmini Iyer and Larry Heck Translating natural language terances to search queries for slu domain detection using query click logs In IEEE International Conference on Acoustics Speech and Signal cessing ICASSP pages IEEE Dilek Malcolm Slaney Asli maz and Larry Heck Eye Gaze for Spoken Language Understanding in In Proceedings of the tional Interactions ternational Conference on Multimodal Interaction ICMI pages New York NY USA sociation for Computing Machinery Istanbul Turkey Dilek Gokhan Tur and Larry Heck Research Challenges and Opportunities in Mobile Applications DSP Education IEEE Signal Processing Magazine Conference Name IEEE Signal Processing Magazine Darryl Hannan Akshay Jain and Mohit Bansal ManyModalQA Modality Disambiguation and QA over Diverse Inputs cs ArXiv Kaiming Haoqi Fan Yuxin Wu Saining Xie and Ross Girshick Momentum Contrast for supervised Visual Representation Learning In Conference on Computer Vision and tern Recognition CVPR pages Seattle WA USA IEEE Zecheng Srinivas Sunkara Xiaoxue Zang Ying Xu Lijuan Liu Nevan Wichers Gabriel Schubiner Ruby Lee Jindong Chen and Blaise Agüera Arcas ActionBert Leveraging User tions for Semantic Understanding of User Interfaces cs ArXiv Michael Gutmann and Aapo Hyvärinen contrastive estimation new estimation principle for unnormalized statistical models In Proceedings Larry Heck The conversational web In Keynote IEEE Workshop on Spoken Language Technology Miami FL USA Larry Heck and Dilek ing the semantic web for unsupervised spoken In IEEE Spoken guage understanding guage Technology Workshop SLT pages IEEE Larry Heck Dilek Madhu Chinthakunta Gokhan Tur Rukmini Iyer Partha Parthasacarthy Lisa Stifelman Elizabeth Shriberg and Ashley dler Multimodal Conversational Search and Browse IEEE Workshop on Speech Language and Audio in Multimedia Larry Heck and Simon Heck Shot Visual Slot Filling as Question Answering cs ArXiv Larry Heck and Hongzhao Huang Deep ing of knowledge graph embeddings for semantic In IEEE Global parsing of twitter dialogs Conference on Signal and Information Processing GlobalSIP pages IEEE Matthew Henderson Iñigo Casanueva Nikola Wen and Ivan ConveRT Efﬁcient and Accurate tional Representations from Transformers In ings of the Association for Computational tics EMNLP pages Online sociation for Computational Linguistics Chiori Hori Takaaki Hori Lee Kazuhiro John Hershey and Tim Marks Sumi Multimodal Fusion for Video Description cs ArXiv Ehsan Bryan McCann Wu Semih Yavuz and Richard Socher ple Language Model for Dialogue cs ArXiv Hsu Hubert Tsai Benjamin Bolte Ruslan Salakhutdinov and Abdelrahman hamed Hubert How Much Can Bad In ICASSP Teacher Beneﬁt ASR IEEE International Conference on Acoustics Speech and Signal Processing ICASSP pages ISSN Ronghang Hu Jacob Andreas Marcus Rohrbach Trevor Darrell and Kate Saenko Learning to Reason Module Networks for Visual Question Answering Huang Acero Chelba Deng Droppo Duchene Goodman Hon Jacoby Jiang Loynd Mahajan Mau ith Mughal Neto Plumpe Steury Venolia Wang and Wang In Pad IEEE International Conference on Acoustics Speech and Signal Processing Proceedings Cat volume pages interaction prototype multimodal and Rif Saurous Aren Jansen Daniel Ellis Shawn Hershey Channing Moore Manoj Plakal Ashok Popat dence Categorization and Consolidation ing to Recognize Sounds with Minimal cs eess stat ArXiv sion Michael Johnston Srinivas Bangalore Gunaranjan Vasireddy Amanda Stent Patrick Ehlen Marilyn Walker Steve Whittaker and Preetam Maloor MATCH An Architecture for Multimodal Dialogue Systems In Proceedings of the Annual ing of the Association for Computational Linguistics pages Philadelphia Pennsylvania USA Association for Computational Linguistics Sebastian Kalkowski Christian Schulze Andreas gel and Damian Borth Analysis In and Visualization of the Dataset Proceedings of the Workshop on Organized Multimodal Mining Opportunities for Novel Solutions MMCommons pages Brisbane Australia ACM Press Douwe Kiela Luana Bulat and Stephen Clark Grounding Semantics in Olfactory Perception In Proceedings of the Annual Meeting of the ciation for Computational Linguistics and the ternational Joint Conference on Natural Language Processing Volume Short Papers pages Beijing China Association for Computational Linguistics Ryan Kiros Ruslan Salakhutdinov and Richard Zemel Unifying dings with Multimodal Neural Language Models cs ArXiv Chen Kong Dahua Lin Mohit Bansal Raquel sun and Sanja Fidler What Are You Talking About Coreference In IEEE Conference on Computer Vision and Pattern nition pages ISSN Bruno Korbar Du Tran and Lorenzo Torresani Cooperative Learning of Audio and Video Models from tion cs ArXiv Satwik Kottur Seungwhan Moon Alborz Geramifard and Babak Damavandi SIMMC oriented Dialog Dataset for Immersive Multimodal cs ArXiv Conversations Satwik Kottur José Moura Devi Parikh Dhruv Batra and Marcus Rohrbach Visual erence Resolution in Visual Dialog using Neural Module Networks cs ArXiv Kuehne Jhuang Garrote Poggio and Serre HMDB large video database for In International human motion recognition Conference on Computer Vision pages ISSN Lori Lamel Samir Bennacef Gauvain Hervé Dartigues and Temem User uation Of The Mask Kiosk Hung Chinnadhurai Sankar Seungwhan Moon Ahmad Beirami Alborz Geramifard and Satwik Kottur DVD Diagnostic Dataset for Reasoning in Video Grounded Dialogue cs ArXiv Jie Lei Licheng Yu Mohit Bansal and Tamara TVQA Localized Compositional Berg Video Question Answering cs ArXiv Jie Lei Licheng Yu Tamara Berg and Mohit Bansal Grounding for Video Question Answering cs ArXiv Jiwei Li Michel Galley Chris Brockett Jianfeng Gao and Bill Dolan jective Function for Neural Conversation Models cs ArXiv Jiwei Li Michel Galley Chris Brockett and Bill gios Spithourakis Dolan Neural cs ArXiv sation Model Jianfeng Gao Toby Li Marissa Radensky Justin Jia Kirielle Singarajah Tom Mitchell and Brad Myers PUMICE Agent that Learns Concepts and Conditionals from Natural Language and Demonstrations In Proceedings of the nual ACM Symposium on User Interface Software and Technology pages New Orleans USA ACM Xiujun Li Xi Yin Chunyuan Li Pengchuan Zhang Xiaowei Hu Lei Zhang Lijuan Wang Houdong Hu Li Dong Furu Wei Yejin Choi and Jianfeng Gao Oscar Aligned training for Tasks In Computer sion ECCV pages Cham Springer International Publishing Yang Li Gang Li Xin Zhou Mostafa Dehghani and Alexey Gritsenko VUT Versatile UI Transformer for User terface Modeling cs ArXiv Ji Lin Chuang Gan and Song TSM poral Shift Module for Efﬁcient Video ing cs ArXiv Bing Liu Gokhan Tur Dilek Pararth Shah and Larry Heck timization of dialogue model with arXiv preprint deep reinforcement learning Bing Liu Gokhan Tur Dilek Pararth Shah and Larry Heck Dialogue learning with human teaching and feedback in able dialogue systems Jiasen Lu Dhruv Batra Devi Parikh and fan Lee ViLBERT Pretraining Agnostic Visiolinguistic Representations for cs Tasks ArXiv Mateusz Malinowski Marcus Rohrbach and Mario Fritz Ask Your Neurons Approach to Answering Questions about Images cs ArXiv Mazaré Samuel Humeau Martin Raison and Antoine Bordes Training lions of Personalized Dialogue Agents In ings of the Conference on Empirical Methods in Natural Language Processing pages Brussels Belgium Association for Computational Linguistics Meier Hurst and Duchnowski tive bimodal sensor fusion for automatic In IEEE International Conference on ing Acoustics Speech and Signal Processing ence Proceedings volume pages vol Grégoire Mesnil Yann Dauphin Kaisheng Yao Yoshua Bengio Li Deng Dilek aodong Larry Heck Gokhan Tur Dong Yu et Using recurrent neural networks for slot ing in spoken language understanding Transactions on Audio Speech and Language cessing Antoine Miech Alayrac Lucas Smaira Josef Sivic and Andrew Ivan Laptev man Learning of Visual resentations from Uncurated Instructional Videos cs ArXiv Antoine Miech Dimitri Zhukov Alayrac Makarand Tapaswi Ivan Laptev and Josef Sivic Learning ding by Watching Hundred Million Narrated Video Clips cs ArXiv Tomas Mikolov Kai Chen Greg Corrado and Jeffrey Dean Efﬁcient Estimation of Word sentations in Vector Space cs ArXiv Seungwhan Moon Satwik Kottur Paul Crook Ankita Shivani Poddar Theodore Levin David Whitney Daniel Difranco Ahmad Beirami njoon Cho Rajen Subba and Alborz fard Situated and Interactive Multimodal cs ArXiv Conversations Pedro Morgado Nuno Vasconcelos and Ishan Misra Instance Discrimination with cs Agreement ArXiv Nasrin Mostafazadeh Chris Brockett Bill Dolan Jianfeng Gao Georgios Michel Galley ithourakis and Lucy Vanderwende Grounded Conversations Multimodal Context for Natural Question and Response Generation cs ArXiv Ara Neﬁan Luhong Liang Xiaobo Pi Liu ang Crusoe Mao and Kevin Murphy pled HMM for speech recognition In IEEE International Conference on Acoustics Speech and Signal Processing volume pages ISSN Jiquan Ngiam Aditya Khosla Mingyu Kim Juhan Nam Honglak Lee and Andrew Ng timodal Deep Learning In Proceedings of the International Conference on International ence on Machine Learning ICML pages Madison WI USA Omnipress Bellevue Washington USA Mihalis Nicolaou Hatice Gunes and Maja Pantic Continuous Prediction of Spontaneous fect from Multiple Cues and Modalities in IEEE Transactions on Affective Arousal Space Computing Conference Name IEEE Transactions on Affective Computing Andrew Owens Jiajun Wu Josh McDermott William Freeman and Antonio Torralba Ambient Sound Provides Supervision for Visual ArXiv Learning cs Aishwarya Padmakumar Jesse Thomason Ayush vastava Patrick Lange Anjali dana Gella Robinson Piramithu Gokhan Tur and Dilek TEACh bodied Agents that Chat cs ArXiv Ashwin Paranjape Abigail See Kathleen Kenealy Haojun Li Amelia Hardy Peng Qi Kaushik Ram Sadagopan Nguyet Minh Phu Dilara Soylu and Christopher Manning Neural Generation Meets Real People Towards Emotionally Engaging Conversations cs ArXiv Mandela Patrick Yuki Markus Asano Ruth Fong João Henriques Geoffrey Zweig and drea Vedaldi CoRR from generalized data transformations Jinchao Li Shahin Baolin Peng Chunyuan Li and Jianfeng Gao Shayandeh Lars Liden SOLOIST Building Task Bots at Scale with Transfer Learning and Machine Teaching cs ArXiv Gao Peng Zhengkai Jiang Haoxuan You Pan Lu Steven Hoi Xiaogang Wang and Hongsheng Li Dynamic Fusion with and ity Attention Flow for Visual Question Answering cs eess ArXiv Piergiovanni Anelia Angelova and Michael Ryoo Evolving Losses for Unsupervised Video Representation Learning cs ArXiv Potamianos Neti Gravier Garg and Senior Recent advances in the matic recognition of audiovisual speech ings of the IEEE Conference Name Proceedings of the IEEE Rui Qian Tianjian Meng Boqing Gong Yang Huisheng Wang Serge Belongie and Yin Cui Spatiotemporal Contrastive Video tation Learning cs ArXiv Alec Radford Jong Wook Kim Chris Hallacy Aditya Ramesh Gabriel Goh Sandhini Agarwal Girish Sastry Amanda Askell Pamela Mishkin Jack Clark Gretchen Krueger and Ilya Sutskever ing Transferable Visual Models From Natural guage Supervision cs ArXiv Colin Raffel Noam Shazeer Adam Roberts Katherine Lee Sharan Narang Michael Matena Yanqi Zhou Wei Li and Peter Liu Exploring the its of Transfer Learning with Uniﬁed Transformer cs stat ArXiv Vignesh Ramanathan Armand Joulin Percy Liang and Li Linking People in Videos with Their Names Using Coreference Resolution In Computer Vision ECCV pages Cham Springer International Publishing Hannah Rashkin Eric Michael Smith Margaret Li and Boureau Towards Empathetic domain Conversation Models New Benchmark In Proceedings of the Annual and Dataset Meeting of the Association for Computational guistics pages Florence Italy tion for Computational Linguistics Shaoqing Ren Kaiming Ross Girshick and Jian Sun Faster Towards Object Detection with Region Proposal Networks cs ArXiv Alan Ritter Colin Cherry and William Dolan Response Generation in Social Media In Proceedings of the Conference on cal Methods in Natural Language Processing pages Edinburgh Scotland UK Association for Computational Linguistics Stephen Roller Boureau Jason Weston toine Bordes Emily Dinan Angela Fan David Gunning Da Ju Margaret Li Spencer Poff Pratik Ringshia Kurt Shuster Eric Michael Smith Arthur Szlam Jack Urbanek and Mary Williamson Conversational Agents rent Progress Open Problems and Future tions cs ArXiv Idan Schwartz Seunghak Yu Tamir Hazan and Alexander Schwing Factor Graph Attention cs ArXiv Ramprasaath Ramasamy Selvaraju Stefan Lee Yilin Shen Hongxia Jin Shalini Ghosh Larry Heck Dhruv Batra and Devi Parikh Taking HINT Leveraging Explanations to Make Vision In and Language Models More Grounded International Conference on Computer Vision ICCV pages Seoul Korea South IEEE Iulian Serban Alessandro Sordoni Yoshua gio Aaron Courville and Joelle Pineau Building Dialogue Systems Using Generative Hierarchical Neural Network Models cs ArXiv Pararth Shah Dilek and Larry Heck Interactive reinforcement learning for oriented dialogue management Lifeng Shang Zhengdong Lu and Hang Li Neural Responding Machine for In Proceedings of the Annual sation ing of the Association for Computational Linguistics and the International Joint Conference on ral Language Processing Volume Long Papers pages Beijing China Association for Computational Linguistics Mohit Shridhar Jesse Thomason Daniel Gordon Yonatan Bisk Winson Roozbeh Mottaghi Luke Zettlemoyer and Dieter Fox ALFRED Benchmark for Interpreting Grounded tions for Everyday Tasks cs ArXiv Kurt Shuster Samuel Humeau Antoine Bordes and Weston Image Chat Engaging Grounded cs ArXiv Conversations Gunnar Sigurdsson Gül Varol Xiaolong Wang Ali Farhadi Ivan Laptev and Abhinav Gupta Hollywood in Homes Crowdsourcing Data tion for Activity Understanding cs ArXiv Karen Simonyan and Andrew Zisserman Very Deep Convolutional Networks for cs ArXiv age Recognition Hrituraj Singh Anshul Nasery Denil Mehta warya Agarwal Jatin Lamba and Balaji Vasan vasan MIMOQA Multimodal Input modal Output Question Answering In Proceedings of the Conference of the North American ter of the Association for Computational Linguistics Human Language Technologies pages Online Association for Computational Linguistics Richard Socher and Li Connecting modalities segmentation and notation of images using unaligned text corpora In IEEE Computer Society Conference on puter Vision and Pattern Recognition pages ISSN Khurram Soomro Amir Roshan Zamir and Mubarak Shah Dataset of man Actions Classes From Videos in The Wild cs ArXiv Alessandro Sordoni Michel Galley Michael Auli Chris Brockett Yangfeng Ji Margaret Mitchell Jianfeng Gao and Bill Dolan Nie Neural Network Approach to Sensitive Generation of Conversational Responses cs ArXiv Nitish Srivastava and Ruslan Salakhutdinov Multimodal Learning with Deep Boltzmann Journal of Machine Learning Research chines Chen Sun Fabien Baradel Kevin Murphy and Cordelia Schmid Learning Video tations using Contrastive Bidirectional Transformer cs stat ArXiv Chen Sun Austin Myers Carl Vondrick Kevin phy and Cordelia Schmid VideoBERT Joint Model for Video and Language tation Learning cs ArXiv Hao Tan and Mohit Bansal Lxmert Learning encoder representations from formers arXiv preprint Makarand Tapaswi Yukun Zhu Rainer Stiefelhagen Antonio Torralba Raquel Urtasun and Sanja Fidler MovieQA Understanding Stories in Movies through cs ArXiv Jesse Thomason Michael Murray Maya Cakmak and Luke Zettlemoyer tion cs ArXiv Amanpreet Singh Ronghang Hu Vedanuj Goswami Guillaume Couairon Wojciech Galuba Marcus FLAVA Rohrbach and Douwe Kiela Foundational Language And Vision ment Model ArXiv cs Shyam Upadhyay Manaal Faruqui Gokhan Tür Dilek and Larry Heck Almost Spoken Language standing In IEEE International Conference on Acoustics Speech and Signal Processing ICASSP pages ISSN Ashish Vaswani Noam Shazeer Niki Parmar Jakob Uszkoreit Llion Jones Aidan N Gomez Łukasz Kaiser and Illia Polosukhin Attention is All you Need In Advances in Neural Information cessing Systems volume Curran Associates Oriol Vinyals and Quoc Neural cs ArXiv sational Model Joseph Weizenbaum computer gram for the study of natural language tion between man and machine Communications of the ACM Jason Weston Samy Bengio and Nicolas Usunier Large Scale Image Annotation Learning to Rank with Joint Embeddings In pean Conference on Machine Learning Thomas Wolf Victor Sanh Julien Chaumond and Clement Delangue TransferTransfo fer Learning Approach for Neural Network Based cs Conversational Agents ArXiv Wu Steven Hoi Richard Socher and Caiming Xiong Natural Language Understanding for Dialogue In Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP pages Online Association for Computational Linguistics Zhiyong Wu Lianhong Cai and Helen Meng Fusion of Audio and Visual Features for Speaker Identiﬁcation In David Hutchison Takeo Kanade Josef Kittler Jon Kleinberg mann Mattern John Mitchell Moni Naor car Nierstrasz Pandu Rangan Bernhard Steffen Madhu Sudan Demetri Terzopoulos Dough Tygar Moshe Vardi Gerhard Weikum David Zhang and Anil Jain editors Advances in Biometrics volume pages Springer Berlin delberg Berlin Heidelberg Series Title Lecture Notes in Computer Science Saining Xie Ross Girshick Piotr Dollár Zhuowen and Kaiming Aggregated ual Transformations for Deep Neural Networks cs ArXiv Hu Xu Gargi Ghosh Huang Dmytro Okhonko Armen Aghajanyan Florian Metze Luke Zettlemoyer and Christoph Feichtenhofer VideoCLIP Contrastive for Understanding cs ArXiv Jun Xu Tao Mei Ting Yao and Yong Rui VTT Large Video Description Dataset for In IEEE ing Video and Language ence on Computer Vision and Pattern Recognition CVPR pages Vegas NV USA IEEE Nancy Xu Sam Masling Michael Du Giovanni pagna Larry Heck James Landay and Monica Lam Grounding Instructions to Automate Web Support Tasks cs ArXiv Kexin Yi Jiajun Wu Chuang Gan Antonio ralba Pushmeet Kohli and Joshua Tenenbaum VQA Disentangling soning from Vision and Language Understanding cs ArXiv Yuhas Goldstein and Sejnowski Integration of acoustic and visual speech signals ing neural networks IEEE Communications zine Conference Name IEEE munications Magazine Manzil Zaheer Guru Guruganesh Avinava Dubey Joshua Ainslie Chris Alberti Santiago Ontanon Philip Pham Anirudh Ravula Qifan Wang Li Yang and Amr Ahmed Big Bird Transformers for Longer Sequences cs stat ArXiv Heming Zhang Shalini Ghosh Larry Heck Stephen Walsh Junting Zhang Jie Zhang and Jay Kuo Generative Visual Dialogue tem via Adaptive Reasoning and Weighted hood Estimation cs ArXiv Heming Zhang Shalini Ghosh Larry P Heck Stephen Walsh Junting Zhang Jie Zhang and Jay Kuo Generative visual dialogue system via In IJCAI pages weighted likelihood estimation Yizhe Zhang Siqi Sun Michel Galley Chen Chris Brockett Xiang Gao Jianfeng Gao Jingjing Liu and Bill Dolan DIALOGPT Scale Generative for Conversational sponse Generation In Proceedings of the nual Meeting of the Association for Computational Linguistics System Demonstrations pages Online Association for Computational tics Yuhao Zhang Hang Jiang Yasuhide Miura pher Manning and Curtis Langlotz Contrastive Learning of Medical Visual resentations and Text from Paired cs ArXiv Images Huijuan Xu and Kate Saenko Ask tend and Answer Exploring Spatial Attention for Visual Question Answering cs ArXiv Li Zhou Jianfeng Gao Di Li and Shum The Design and Implementation of XiaoIce an Empathetic Social Chatbot Computational guistics Luowei Zhou Chenliang Xu and Jason Corso Towards Automatic Learning of Procedures from Web Instructional Videos cs ArXiv Yuke Zhu Oliver Groth Michael Bernstein and Li Fei Grounded Question cs ArXiv ing in Images Conversations with TRANSRESNET Image Personality Conversations Glamorous Glamorous Glamorous The star of the show is the man and his fabulous car I don t see man That tends to be the case with men that buy ﬂashy cars and BIG houses I would like to drive that car That tends to be the case with men that buy ﬂashy cars and BIG houses Would you drive that car Glamorous And I would pay him to do that in my backyard as well Wise Wise I love going to new restaurants Well easy come easy go with your food then raw is dangerous but it s your life Do you like spicy food It is most likely too spicy for you Businesslike Suave Considerate This is Lucy my labrador I should start dog walking business That s good idea would make lovely companion for Do you have any pets I think I will Dogs are nice animal for the regular person to have Caring Extreme Silly I hope you re injured too If not I can help you with that I m not injured At least if you crash you ll beat the paramedics to the scene by at least minutes Haha that s nice joke I would die laughing if you did fall Angry Angry Angry Angry Angry Angry That doesn t look great It makes mad Why did they have to hang it before the paint was dry It s not painting Oh come on you re so negative How is that NOT artistic I didn t say it wasn t artistic Oh come on you re so negative How is that NOT artistic I m not being negative Oh come on you re so negative How is that NOT artistic Why are you repeating yourself Because you have ruined the artistic integrity of the image If you had the talent that I have you would understand this I agree with you Why are people so judgemental about art It s infuriating Conversations from TRANSRESNET Images are chosen from using the browser by Kalkowski et and the internet and personalities are chosen at random indicates lines spoken by the author,,[]
MAISON Multimodal Sensor platform for Older Individuals Ali Faranak Charlene Shehroz Toronto Rehabilitation Institute University Health Network Canada of Biomedical Engineering University of Toronto Canada Bloomberg Faculty of Nursing University of Toronto Canada v N G L s c v v i X r is global aging population requiring the need for the right tools that can enable older adults greater independence and the ability to age at home as well as assist healthcare workers It is feasible to achieve this objective by building predictive models that assist healthcare workers in monitoring and analyzing older adults behavioral functional and psychological data To develop such models large amount of multimodal sensor data is typically required In this paper we propose MAISON scalable platform of mercially available smart devices capable of collecting desired multimodal sensor data from older adults and patients living in their own homes The MAISON platform is novel due to its ability to collect greater variety of data modalities than the existing platforms as well as its new features that result in seamless data collection and ease of use for older adults who may not be digitally literate We demonstrated the feasibility of the MAISON platform with two older adults discharged home from large rehabilitation center The results indicate that the MAISON platform was able to collect and store sensor data in cloud without functional glitches or performance degradation This paper will also discuss the challenges faced during the development of the platform and data collection in the homes of the older adults MAISON is novel platform designed to collect multimodal data and facilitate the development of predictive models for detecting key health indicators including social isolation depression and functional decline and is feasible to use with older adults in the community Index sensors digital health machine learning Internet of Things IoT smart home I INTRODUCTION The global number of people aged and over is expected to double to more than billion in The aging process increases one s likelihood of developing variety of health problems including dementia diabetes cardiovascular disease osteoarthritis or other chronic conditions The World Health Organization data indicates that there was million healthcare worker shortage in and that number is expected to rise to over million by The discrepancy between the number of older adults who may need care and healthcare workers who can provide that care will result in essential healthcare needs that will be unmet resulting in poorer health and Recent advancements in IoT provide the opportunity to collect multimodal sensor data from the homes of patients and older adults and store it in cloud servers Data from multiple sensing modalities are often required to model complex behavioral physiological and social iors of individuals living in the community including faceted health issues such as social isolation mental health issues and frailty Artiﬁcial Intelligence AI approaches can be used on this plethora of multimodal data to detect the onset of various conditions issue alerts to caregivers and potentially provide recommendations for treatments or healthcare services The various sensing modalities that AI models leverage include but are not limited to motion from an accelerometer heart rate from photoplethysmography PPG sensor captured from cameras ambient light levels motion sensors and smartphone phone ambient sound recordings It is challenging to collect multimodal data from multiple locations to train AI models devices located in different Oftentimes researchers have to manually combine information from different modalities into single database In order to meet their speciﬁc needs researchers sometimes develop this bespoke their own smart sensors however ment potentially increases the costs of projects In order to mitigate these limitations researchers have looked towards commercially available smart devices One of the challenges in using these devices is limited interoperability because each its own Application Programming Interface API and private cloud that limits researcher access to the collected data For example Apple products use their own protected communication protocols making access to the raw data impossible Another issue with commercial sensors and devices is that in most cases the devices provide processed sensor data average heart rate and the number of steps which may not be ideal for developing predictive models From machine learning standpoint raw sensor data can be used to extract informative features required for training machine learning or learn features using deep learning methods More recently many sensor manufacturers provide access to raw sensor data through their APIs that enables the development of new based IoT platforms There are currently number of existing digital health platforms that integrate various sensing devices to vide backend infrastructure and management interfaces However very few of them provide comprehensive tionality and often lack essential features that play major role in preventing gaps in the data and battery depletion including location geofencing and compatibility with Wear Operating System for smartwatches watches In order to take advantage of the potential of different types of commercial sensors our goal is to develop scalable digital health platform that is able to provide holistic perspective of active aging in the home by i seamlessly collecting data from multiple smart devices from different manufacturers and ii storing the data in an organized manner in the cloud to facilitate the development of predictive models that support In this paper we introduce MAISON Multimodal Sensor platform for Older iNdividuals novel digital health platform for securely collecting raw sensor data from various devices including smartphones smartwatches and ambient sensors We deployed MAISON in the homes of two older adults living in the community with surgery for period of two months each and successfully collected multimodal sensor data The following sections describe the related work details of the MAISON platform data collection and preliminary data analysis II RELATED WORK In this section we present brief overview of previous digital health platforms that include phones watches and IoT sensors We limit our description to focus on the research and development work of each platform rather than their predictive or analytics aspects Khan et presented DAAD multimodal sensing framework to detect incidences of agitation and aggression in people with dementia using multiple video cameras an Empatica wristband pressure mat and motion sensors to collect various types of data from people living with dementia The Empatica wristband continuously collected data and stored the information in its internal memory from where it was periodically pushed to its own cloud The DAAD system does not use central server to direct various data from various modalities automatically research assistant was required to manually download the data from the cloud on daily basis for further analysis For research study or lengthy data collection period this can be very and lead to cost and resource overruns Ranjan et developed Remote Assessment of Disease and Relapse that can collect monitor and analyze data via wearable and mobile devices does not utilize external IoT devices Ferreira et developed AWARE purpose Android phone application designed to capture infer and provide contextual information on mobile devices The major limitation of AWARE is that it does not support other endpoint devices watches and motion tectors outside of phones Kennedy et developed mobile application that collects phone sensor data location physical activity and light and surveys to detect the onset of depression Since is built on AWARE it does not support wearable devices or external sensors Torous et reported on the development and initial utilization of Beiwe that can collect location data accelerometer phone TABLE I Comparing DAAD AWARE Beiwe HOPES JTrack and our proposed platform MAISON described in Section III Platform DAAD AWARE Beiwe HOPES JTrack MAISON proposed Phone support Watch support sensors Cloud backend Geofencing feature calls and text messages audio recording and screen and phone status Wang et developed HOPES Health Outcomes through Positive Engagement and for the collection of data from phones and wearable devices Fitbit watch Based on the Beiwe platform HOPES expands data collection to include wearable devices and additional phone sensors Neither the Beiwe nor the HOPES support external ambient sensors Far et developed JTrack an Android phone application to collect phone data and dashboard to manage and create studies JTrack is not equipped to connect and collect data from any other device besides Android phones Table I provides comparison of MAISON with other digital health platforms Besides DAAD which does not offer cloud server for automatic data collection and relies on manual data transfer from sensors to computers none of the available platforms are able to collect data from external ambient sensors such as motion sensors and sleep tracking devices In addition none of the available platforms is developed to integrate and connect the Google Wear watches with Android phones The MAISON system addresses all the above features to continuously collect data from ticipants in their homes with zero effort III MAISON PLATFORM An overview of the MAISON digital health platform is shown in Figure which consists of four main components the MAISON phone application the MAISON watch application sensors and their associated clouds the MAISON central cloud Smartphone Application The smartphone application is developed to collect and store sensor data using phone sensors establish connection with the watch to receive and temporarily store data collected by the watch sensors and send the collected data to the central cloud The main Android activities of the phone and watch applications are shown in Figure The application simple and straightforward design to improve usability which is an important consideration for older adult Consented participants are empowered with the ability to stop and start MAISON s data collection when they choose Both the phone and watch applications are programmed to be launched and start data collection automatically after each reboot of the devices to increase ease of use for older adults local database At these timestamps the phone also checks its local database to determine whether any data been received from the watch in the past time interval In case of data from the watch the phone displays notiﬁcation to instruct the participant to wear the watch and turn on the watch application Smartwatch application The phone and the watch collect complementary tion The software architecture of the MAISON Wear watch application is similar to that of the Android phone application with few subtle differences At predetermined time intervals the watch attempts to transmit data to the cloud In case the watch is not connected to the Internet it will send its collected data via BLE to the phone which will be transferred to the cloud afterward Ambient Sensors and Clouds The MAISON platform currently supports the Insteon tion sensors and Withings sleep sensors Using the manufacturer provided APIs and secure authentication the data from these devices is collected from their clouds and stored on the MAISON central cloud These sensors can be added or removed as per study s requirement MAISON central cloud As per research ethics requirements MAISON s multimodal sensor data is to be stored on Google Cloud servers located in Canada All the recommended practices by Google Cloud are taken into consideration in MAISON central cloud opment to meet the HIPAA requirements The collected data modalities are anonymously transferred to the MAISON central cloud and saved as CSV ﬁles In the central cloud data merging algorithm is deployed to merge data received from the phone and watch at different times and output one CSV ﬁle for each data modality at each time segment an hour or day In addition we developed Google Cloud Functions to securely authenticate through OAuth and access the participants multimodal sensor data on the party clouds see section and transfer data from those clouds to the MAISON central cloud IV EXPERIMENTS The MAISON platform was deployed into the homes of two older adult patients who were discharged home hip fracture surgery and we collected data for eight weeks Both the patients were females in their who had recently undergone hip fracture surgery lived alone with pets in their homes The MAISON phone and and had WiFi watch applications were installed on two Motorola Moto Android phones and two Mobvoi TicWatch Pro Wear watches respectively The Withings sleep tracking mattress and the Insteon motion sensor were installed underneath the bed mattress and in the living rooms of the participants homes respectively clinical questionnaires were also collected including the Social Isolation Scale and Fig Block diagram of the MAISON platform for collecting data from phones watches and external sensors see Section III Smartphone Data Collection Android phones have ious sensors GPS accelerometer light and other sensors that provide information on position motion and environment Developers have access to both the raw and processed data generated by the sensors MAISON can include any of the Android phone s sensors as part of the data collection and storage on the cloud Sampling frequency can be determined for each sensor depending on the study quirements MAISON uses Google s Fused Location Provider API to collect location data In order to avoid rapid battery drain due to continuous location data collection MAISON uses Geofencing which leverages GPS signals to identify participant s location perimeter can be created using the Geofencing feature that can surround the house of the participant and data collection only triggers when the participants leave the geofence All the sensor data collected on the phone is inserted into its local database in and periodically sent to the cloud Smartphone and Watch Connectivity Embedded sensors in the smartwatch can collect physiological and activity data in the MAISON platform see Section The watch sends the collected data directly to the cloud When the watch is not connected to the Internet it sends the collected data to the phone using Bluetooth Low Energy BLE at speciﬁc intervals Contrary to the previous works see Section II in MAISON both the watch and phone operate on Google Android and Wear that are fully compatible and enable constant connectivity between them using BLE Once the phone receives data from the watch it temporarily stores the data in its local database and then sends the data to the cloud Data Transfer to the Cloud Data stored in the local database of the phone application is sent to the MAISON central cloud Google Firebase Cloud Firestore using secure Hypertext Transfer Protocol Secure see Section at speciﬁed time intervals as per the study design Once the phone receives an acknowledgement that data been successfully stored in the cloud it permanently deletes the data from its Oxford hip score This is an ongoing study and the overall goal is to leverage the MAISON platform to build predictive models from this multimodal sensor data The following multimodal sensor data were collected from the two participants On the phone location GPS data at frequency of Hz with the house of each participant as the geofence On the watch the accelerometer data at frequency of Hz the PPG heart rate sensor was activated every minutes and collected data for seconds and number of steps collected by the step detector of the watch The timestamp of motion events is collected from the Insteon Motion sensor Sleep data using Withings mattress at each nap and night sleep including total sleep duration deep sleep duration average heart rate and snoring time Figure shows the magnitude of the watch ter b the number of steps per minute detected by the watch c the average value of the watch heart rate every minute d the total number of motion sensor events every minute along with the sleep time the time period between the two blue dashed lines of one of the participants for hours It also shows the latitude and longitude coordinates of the location data of the participant on map while the participant is outside geofence the time period between the two red dashed lines Heat map colours in Figure represent the frequency of occurrence of location data at each location The complementary information of the collected data can be observed in Figure The sleep began at approximately and ended at between the two blue dashed lines There were motion events during the sleep period After motion events began occurring until around when the participant left geofence The location data collection began at and ended at when the participant geofence Motion data collection started again at this time During the wake time of the participant acceleration heart rate and step data were continuously collected except when the watch was being charged or when it was not worn DISCUSSION During the development of the system we encountered minor practical challenges One technical challenge was that the regular updates to the Android and its dependencies led to modifying the code of the phone and watch applications to enable running on different Android devices Another challenge was frequent updates in the APIs of external wearable ambient sensors leading to code modiﬁcations variety of strategies were used to optimize the performance of the phone and watch applications in order to minimize their battery consumption The Google Geofencing API is used in conjunction with the location API to collect the location data only when the participants are outside their homes geofences Even though the phone application does not greatly affect the battery consumption of the phone we found that the MAISON watch application resulted in shorter battery life of the watch Since participants needed to recharge b c d Fig visual representation of different sensor data modalities collected from one of the older adult participants described in Section IV over period of hours the watch for approximately one and half hours there were gaps in the data collected from the watch during this period Some participants did not wear the watch on certain occasions despite the periodic notiﬁcation sent on the watch One participant highlighted the discomfort of wearing the watch Additionally motion sensor and sleep mattress data could be noisy if other people are present alongside the study participant Lastly participant may unplug the external sensors mistakenly resulting in missing data VI CONCLUSION AND FUTURE WORK MAISON is feasible digital health platform that can collect multimodal sensor data from various wearable and ambient devices to provide holistic perspective of active aging To date it been employed in the homes of two older adults with minor issues The system usability by older adults living in the community is being evaluated in an ongoing research study As MAISON is easy to use it will appeal to both older adults and their caregivers who may be younger Signiﬁcantly MAISON could lead to an entire cloud pipeline from data collection to and model building to further speed up research In future iterations MAISON will be customized to add different clinical questionnaires or other types of wearable watches phones and external ambient sensors REFERENCES Chang Angela Vegard Skirbekk Stefanos Tyrovolas Nicholas Kassebaum and Joseph Dieleman Measuring population ageing an analysis of the Global Burden of Disease Study The Lancet Public Health Kim Eunsaem and Cory Bolkan Attitudes toward aging as chological resource among caregivers of persons living with Geriatric Nursing Turner Paul health service without health In Talent Management in Healthcare pp Palgrave Macmillan Cham https Khan Latif Walid Saad Zhu Ekram Hossain and Choong Seon Hong Federated learning for internet of things Recent advances taxonomy and open IEEE Communications Surveys Tutorials Khan Shehroz Tong Zhu Bing Ye Alex Mihailidis Andrea Iaboni Kristine Newman Angel Wang and Lori Schindel Martin Daad framework for detecting agitation and aggression in people living with dementia using novel sensor In IEEE International Conference on Data Mining Workshops ICDMW pp IEEE Lyndsey Miller Zachary Beattie Rose May Hailey Cray Zachary Kabelac Dina Katabi Jeffrey Kaye and Ipsit Vahia Monitoring behaviors of patients with dementia using passive environmental sensing approaches case The American Journal of Geriatric Psychiatry Hernando David Surya Roca Jorge Sancho Alesanco and Raquel Validation of the apple watch for heart rate variability measurements during relax and mental stress in healthy Sensors Nguyen Dinh Ming Ding Pubudu Pathirana Aruna Seneviratne Jun Li and Vincent Poor Federated learning for internet of things comprehensive IEEE Communications Surveys Tutorials Zambotti Massimiliano Aimee Goldstone Stephanie Claudatos Ian Colrain and Fiona Baker validation study of Fitbit Charge compared with polysomnography in Chronobiology international Khan Shehroz and Babak Taati Detecting unseen falls from wearable devices using ensemble of Expert Systems with Applications Van Berkel Niels Denzil Ferreira and Vassilis Kostakos The rience sampling method on mobile ACM Computing Surveys CSUR Spasojevic Soﬁja Jacob Nogas Andrea Iaboni Bing Ye Alex lidis Angel Wang Shu Jie Li Lori Schindel Martin Kristine Newman and Shehroz Khan pilot study to detect agitation in people living with dementia using Journal of Healthcare Informatics Research Chikersal Prerna Afsaneh Doryab Michael Tumminia Daniella Villalba Janine Dutcher Xinwen Liu Sheldon Cohen et tecting depression and predicting its onset using longitudinal symptoms captured by passive sensing machine learning approach with robust feature ACM Transactions on Interaction TOCHI Kim Meelim Jaeyeong Yang Ahn and Hyung Jin Choi Machine Learning Analysis to Identify Digital Behavioral Phenotypes for Engagement and Health Outcome Efﬁcacy of an mHealth vention for Obesity Randomized Controlled Journal of medical Internet research Nogas Jacob Shehroz Khan and Alex Mihailidis Deepfall invasive fall detection with deep convolutional Journal of Healthcare Informatics Research https https https https https https Sahandi Far Mehran Michael Stolz Jona Fischer Simon hoff and Juergen Dukart JTrack Digital Biomarker Platform for Remote Monitoring of Behaviour in Health and Frontiers in Public Health Ferreira Denzil Vassilis Kostakos and Anind Dey AWARE mobile context instrumentation Frontiers in ICT Ranjan Yatharth Zulqarnain Rashid Callum Stewart Pauline Conde Mark Begale Denny Verbeeck Sebastian Boettcher Richard Dobson Amos Folarin and Consortium open source mobile health platform for collecting monitoring and analyzing data using sensors wearables and mobile JMIR mHealth and uHealth Opoku Asare Kennedy Aku Visuri Julio Vega and Denzil Ferreira in the Wild An Exploratory Study Using Smartphones to Detect the Onset of In International Conference on Wireless Mobile Communication and Healthcare pp Springer Cham Torous John Mathew Kiang Jeanette Lorme and Onnela New tools for new research in psychiatry scalable and customizable platform to empower data driven smartphone JMIR mental health Wang Xuancong Nikola Vouk Creighton Heaukulani Thisum dhika Wijaya Martanto Jimmy Lee and Robert JT Morris HOPES an integrative digital phenotyping platform for data collection monitoring and machine Journal of medical Internet research https Ranjan Sushant and Rama Shankar Yadav Social isolation opment and validation of Benchmarking An International Journal Wylde Vikki Ian Learmonth and Victoria Cavendish The Oxford hip score the patient s Health and quality of life outcomes Lewis James The system usability scale past present and International Journal of Interaction,,[]
t r h t f t t S v r u S S C N L Joanna Kołodziej Horacio Eds Modelling and Simulation for Big Data Applications Selected Results of the COST Action cHiPSet Lecture Notes in Computer Science Commenced Publication in Founding and Former Series Editors Gerhard Goos Juris Hartmanis and Jan van Leeuwen Editorial Board Members David Hutchison Lancaster University Lancaster UK Takeo Kanade Carnegie Mellon University Pittsburgh PA USA Josef Kittler University of Surrey Guildford UK Jon Kleinberg Cornell University Ithaca NY USA Friedemann Mattern ETH Zurich Zurich Switzerland John Mitchell Stanford University Stanford CA USA Moni Naor Weizmann Institute of Science Rehovot Israel Pandu Rangan Indian Institute of Technology Madras Chennai India Bernhard Steffen Dortmund University Dortmund Germany Demetri Terzopoulos University of California Angeles CA USA Doug Tygar University of California Berkeley CA USA More information about this series at http Joanna Kołodziej Horacio Eds Modelling and Simulation for Big Data Applications Selected Results of the COST Action cHiPSet Editors Joanna Kołodziej Cracow University of Technology Cracow Poland Horacio National College of Ireland Dublin Ireland ISSN Lecture Notes in Computer Science ISBN https ISSN electronic ISBN eBook Library of Congress Control Number LNCS Sublibrary Theoretical Computer Science and General Issues Acknowledgement and Disclaimer This publication is based upon work from the COST Action Modelling and Simulation for Big Data Applications cHiPSet supported by COST European Cooperation in Science and Technology The book reﬂects only the authors views Neither the COST Association nor any person acting on its behalf is responsible for the use which might be made of the information contained in this publication The COST Association is not responsible for external websites or sources referred to in this publication The Editor s if applicable and The Author s This book is an open access publication Open Access This book is licensed under the terms of the Creative Commons Attribution International License http which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give appropriate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this book are included in the book s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the book s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder The use of general descriptive names registered names trademarks service marks etc in this publication does not imply even in the absence of speciﬁc statement that such names are exempt from the relevant protective laws and regulations and therefore free for general use The publisher the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication Neither the publisher nor the authors or the editors give warranty express or implied with respect to the material contained herein or for any errors or omissions that may have been made The publisher remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations This Springer imprint is published by the registered company Springer Nature Switzerland AG The registered company address is Gewerbestrasse Cham Switzerland European Cooperation in Science and Technology COST This publication is based upon work from COST Action Modelling and Simulation for Big Data Applications cHiPSet supported by COST European Cooperation in Science and Technology COST European Cooperation in Science and Technology is funding agency for research and innovation networks Our Actions help connect research initiatives across Europe and enable scientists to grow their ideas by sharing them with their peers This boosts their research career and innovation Funded by the Horizon Framework Programme of the European Union Preface Long considered important components of the scientiﬁc method Modelling and Simulation have evolved from traditional discrete numerical methods to complex continuous analytical optimisations Resolution scale and accuracy have become essential to predict and analyse natural and complex systems in science and engineering When their level of abstraction rises to have better discernment of the domain at hand their computational representation becomes increasingly demanding for computational and data resources It is not unusual to have models with hundreds of features conditions and constraints coupled with complex simulations with large data sets These complex systems do not easily lend themselves to straightforward modular decompositions that support parallelisation and trivial putational support They often require signiﬁcant amount of computational resources with associated data sets scattered across multiple sources and different geographical locations It is then required to have seamless interaction of large computational resources with in order to store compute analyze and visualize large data sets in science and engineering To nurture such computational the cHiPSet Action enabled the of distinct expert communities High Performance Computing HPC Modelling and Simulation Domain scientists and practitioners Funded by the European Commission cHiPSet provided dynamic forum for their members and distinguished guests to openly discuss novel perspectives and topics of interest for the and HPC communities Figure presents schematic representation of cHiPSet Structured into complementary chapters this cHiPSet compendium presents set of selected case studies related to computational in artiﬁcial intelligence bioinformatics electrical engineering pharmacology ﬁnance healthcare Its purpose is to illustrate the tangible social media and telecommunications improvement in the understanding of those domains which been faciliated by cHiPSet collaborations All case studies are by individuals associated with cHiPSet who have worked together through dedicated meetings workshops scientiﬁc missions and summer schools Grelck et introduce the overall cHiPSet aims and objectives with special emphasis on how Big Data both posed challenge and offered development opportunities for HPC how to efﬁciently turn massively large data into valuable information and meaningful knowledge They also describe how offer suitable abstractions to manage the complexity of analysing Big Data in various scientiﬁc and engineering domains viii Preface Fig Schematic representation of the cHiPSet action Larsson et describe two classes of algorithms for the numerical solution of large electromagnetic scattering problems and include results for parallel implementation They conclude by providing various perspectives on simulations that require parallel implementation Rached and Larsson have furnished survey of currently used parametric and methods for the estimation of tail distributions and extreme quantiles They also render some perspectives on how to move forward with estimation Kilanioti et describe why social data streams raise variety of practical lenges including the generation of meaningful insights They present methodological approach to work with social media streams in order to efﬁciently perform content distribution data diffusion and data replication Nejkovic et look into the forthcoming ﬁfth generation of mobile networks commonly known as which will signiﬁcantly improve the bandwidth capacity and reliability of current telecommunication operators They enumerate some key use cases from home entertainment to product manufacturing and healthcare Brdar et focus on location data collected in mobile cellular networks and on how to identify patterns of human activity interactions and mobility They also point out the critical challenges to unlock the value of data coming from mobile cellular networks Vitabile et report on recent developments in health monitoring technologies to enable diagnosis and treatment of patients with chronic diseases older adults and people with disabilities They also highlight the value of pervasive healthcare to facilitate independent living and disease management Spolaor et enquire into model reduction techniques to uncover emergent behaviors in complex cellular systems Their comprehensive approach aims to improve the understanding of cell behavior using multiple scales of temporal and spatial organisation Preface ix Olgac et survey tools and workﬂows for virtual screening computational method for rational drug design and development They explain its relevance to medicinal chemists pharmacologists and tional scientists for the discovery and testing of novel pharmaceuticals Kołodziej et introduce ultra wide band for wireless body area interconnected structures of nodes attached to human body an external base station and possibly the Internet Their survey describes data storage security and analysis in the context of mobile health monitoring Marechal et discuss the challenges involved in automatic human emotion recognition As an evolving area in artiﬁcial intelligence it entails the development of systems for emotion representation recognition and prediction Finally Zamuda et outline the state of the art in forecasting cryptocurrency value by sentiment analysis Their contribution includes current challenges in the deployment of distributed ledgers blockchains and sentiment analysis metrics using large data streams and cloud platforms In summary the cHiPSet COST Action facilitated the exchange of knowledge experiences and best practices between individuals with track record in HPC and domain scientists and engineers Its tangible collaborative endeavors have arguably enhanced the resolution and accuracy of by deploying smart algorithms and techniques on infrastructures The cHiPSet COST Action contributed toward improving the understanding of natural phenomena and complex systems for European academics and industry practitioners This compendium is based on collaborative endeavors partly supported by Modelling and Simulation for Big Data Applications cHiPSet cHiPSet an Action supported by COST European Cooperation in Science and Technology from to as part of Horizon EU Framework gramme for Research and Innovation We are grateful to all the contributors of this book for their willingness to work on this interdisciplinary book project We thank the authors for their interesting proposals of the book chapters their time efforts and their research ideas which makes this volume an interesting complete monograph of the latest research advances and technology developments on the next generation of distributed and complex HPC in the big data We also would like to express our sincere thanks to the reviewers who helped us ensure the quality of this volume We gratefully acknowledge their time and valuable remarks and comments Our special thanks go to Alfred Hoffmann Aliaksandr Birukou Anna Kramer and all the editorial team of Springer for their patience valuable editorial assistance and excellent cooperative collaboration in this book project Finally we would like to send our warmest gratitude to our friends and families for their patience love and support in the preparation of this volume January Joanna Kołodziej Horacio Committee Action Scientiﬁc Ofﬁcer Ralph Stübner Action Chair Joanna Kołodziej Action Vice Chair Cost Association Brussels Belgium Cracow University of Technology Poland Horacio Cloud Competency Centre National College of Ireland Dublin Ireland Scientiﬁc Coordinator Clemens Grelck University of Amsterdam The Netherlands Working Group WG Leaders Leader Ewa Szynkiewicz Leader Marco Aldinucci Leader Andrea Bracciali Leader Warsaw University of Technology Poland University of Turin Italy University of Stirling UK Elisabeth Larsson University of Uppsala Sweden STSM Coordinator Juan Carlos University of Vigo Spain TS Coordinator Ciprian Dobre University Politehnica of Bucharest Romania xii Committee Dissemination Coordinator Peter Kilpatrick Queen s University Belfast UK Industry Collaboration Coordinator Dave Feenan Technology Ireland ICT Skillnet Ireland Contents Why Modelling and Simulation for Big Data Applications Matters Clemens Grelck Ewa Marco Aldinucci Andrea Bracciali and Elisabeth Larsson Parallelization of Hierarchical Matrix Algorithms for Electromagnetic Scattering Problems Elisabeth Larsson Afshin Zafari Marco Righero Alessandro Francavilla Giorgio Giordanengo Francesca Vipiana Giuseppe Vecchi Christoph Kessler Corinne Ancourt and Clemens Grelck Tail Distribution and Extreme Quantile Estimation Using Approaches Imen Rached and Elisabeth Larsson Towards Efficient and Scalable Content Delivery Issues and Challenges Irene Kilanioti Alejandro Damián Anthony Karageorgos Christos Mettouris Valentina Nejkovic Nikolas Albanis Rabih Bashroush and George Papadopoulos Big Data in Distributed Applications Valentina Nejkovic Ari Visa Milorad Tosic Nenad Petrovic Mikko Valkama Mike Koivisto Jukka Talvitie Svetozar Rancic Daniel Grzonka Jacek Tchorzewski Pierre Kuonen and Francisco Gortazar Big Data Processing Analysis and Applications in Mobile Cellular Networks Sanja Brdar Olivera Novović Nastasija Grujić Horacio Truică Siegfried Benkner Enes Bajrovic and Apostolos Papadopoulos Medical Data Processing and Analysis for Remote Health and Activities Monitoring Salvatore Vitabile Michal Marks Dragan Stojanovic Sabri Pllana Jose Molina Mateusz Krzyszton Andrzej Sikora Andrzej Jarynowski Farhoud Hosseinpour Agnieszka Jakobik Aleksandra Stojnev Ilic Ana Respicio Dorin Moldovan Cristina Pop and Ioan Salomie xiv Contents Towards Human Cell Simulation Simone Spolaor Marco Gribaudo Mauro Iacono Tomas Kadavy Zuzana Komínková Oplatková Giancarlo Mauri Sabri Pllana Roman Senkerik Natalija Stojanovic Esko Turunen Adam Viktorin Salvatore Vitabile Aleš Zamuda and Marco Nobile High Throughput Virtual Screening in Novel Drug Discovery Abdurrahman Olğaç Aslı Türe Simla Olğaç and Steffen Möller Ultra Wide Band Body Area Networks Design and Integration with Computational Clouds Joanna Kołodziej Daniel Grzonka Adrian Widłak and Paweł Kisielewicz Survey on Multimodal Methods for Emotion Detection Catherine Marechal Dariusz Mikołajewski Krzysztof Tyburek Piotr Prokopowicz Lamine Bougueroua Corinne Ancourt and Katarzyna Forecasting Cryptocurrency Value by Sentiment Analysis An Survey of the in the Cloud Aleš Zamuda Vincenzo Crescimanna Juan Burguillo Joana Matos Dias Katarzyna Imen Rached Horacio Roman Senkerik Claudia Pop Tudor Cioara Ioan Salomie and Andrea Bracciali Author Index Why Modelling and Simulation for Big Data Applications Matters Clemens B Ewa Marco Andrea and Elisabeth University of Amsterdam Amsterdam Netherlands Warsaw University of Technology Warsaw Poland ens University of Turin Turin Italy University of Stirling Stirling UK abb Uppsala University Uppsala Sweden Abstract Modelling and Simulation M S oﬀer adequate tions to manage the complexity of analysing big data in scientiﬁc and engineering domains Unfortunately big data problems are often not ily amenable to eﬃcient and eﬀective use of High Performance ing HPC facilities and technologies Furthermore M S communities typically lack the detailed expertise required to exploit the full potential of HPC solutions while HPC specialists may not be fully aware of speciﬁc modelling and simulation requirements and applications The COST Action Modelling and tion for Big Data Applications created strategic framework to ter interaction between M S experts from various application domains on the one hand and HPC experts on the other hand to develop tive solutions for big data applications One of the tangible outcomes of the COST Action is collection of case studies from various computing domains Each case study brought together both HPC and M S experts giving witness of the eﬀective facilitated by the COST Action In this introductory article we argue why joining forces between M S and HPC communities is both timely in the big data and crucial for success in many application domains Moreover we provide an overview on the state of the art in the various research areas concerned Introduction The big data poses critically diﬃcult challenge for puting HPC how to eﬃciently turn massively large and often unstructured or c The Author s Ko lodziej and Eds cHiPSet LNCS pp https Grelck et data ﬁrst into valuable information and then into meaningful knowledge HPC facilities and technologies are eﬀectively required in rapidly increasing number of domains from life and physical sciences to socioeconomic systems Thus the big data likewise oﬀers striking nities for HPC to widen its scope and to strengthen its societal and economic impact Computing HPC and high throughput computing pin the large scale processing of grand challenge problems with requirements in order to enable complex applications in distinct scientiﬁc and technical ﬁelds such as physics genomics systems and synthetic biology industrial automation social and economic data analytics and medical informatics This led to substantial improvement in the understanding of diverse domains ranging from the evolution of the physical world to human eties Application performance in HPC systems is nowadays largely dominated by remote and local data movement overhead network messages memory and storage accesses This poses new challenges to HPC modelling and ming languages which should enhance data locality where possible and enable fast data transition where needed When investigating the behaviour and complexity of abstractions for scale big data systems one employs series of technologies that have their roots in large compute cluster environments With the advent of ware accelerators GPU FPGA cloud services and the increased performance of processors HPC become an option for many scientiﬁc disciplines The COST Action Modelling and Simulation for Big Data Applications facilitates between the HPC nity both developers and users and M S disciplines for which the use of HPC facilities technologies and methodologies still is novel if any phenomenon domains make the issue of eﬃciency particularly relevant for problems such as and integration and model state explosion Furthermore these complex systems do not straightforwardly lend themselves to modular decomposition crucial prerequisite for parallelisation and hence HPC support They often require signiﬁcant amount of tational resources with data sets scattered across multiple sources and diﬀerent geographical locations Modelling and Simulation M S are widely considered essential tools in science and engineering to substantiate the prediction and analysis of complex systems and natural phenomena Modelling traditionally addressed ity by raising the level of abstraction and aiming at an essential representation of the domain at hand This resulted in complicated between accuracy and eﬃciency That is to say the properties of system can be studied by reproducing simulating its behaviour through its abstract tion Arguably the context of the application level should be reconsidered For instance Monte Carlo simulations must be fed with input data store ate results and ﬁlter and merge output data in an adjusted and reliably robust Why HPC Modelling and Simulation for Big Data Applications Matters manner Thus M S approaches are particularly aﬀected by the data deluge nomenon since they need to use large data sets to enhance resolution and scale and distribute and analyse data in the diﬀerent phases of the pipeline Both HPC and M S are well established research areas each by themselves However better integration of the two aimed at applications from various domains will bring substantial progress in addressing big data problems On the one hand domain experts need HPC for simulation modelling and data analysis but are often unaware of performance and parallelism tion pitfalls in their designs On the other hand designers of HPC development tools and systems primarily focus on absolute performance measures by nition the raison d ˆetre for HPC However MIPS FLOPS and speedups need not be the only measures considerations may put some more or even almost all emphasis on other factors such as usability productivity nomic cost and time to solution By further improving collaboration with domain experts HPC architects ought to be able to develop programming models and architectures better tailored to speciﬁc problems Likewise analysis and tion tools ought to be improved for better understanding of HPC systems by domain experts The COST Action Modelling and Simulation for Big Data Applications is based on the idea that key aspects of M S must be jointly addressed by considering the needs and issues posed by the two communities together When multidimensional heterogeneous massive data sets need to be analysed in speciﬁc big data application domain the methods required to suitably process the data are necessarily determined by the kind of data and analysis to be performed Consequently the features of programming language library or execution machinery supporting the eﬃcient implementation of the analysis should not be thought of as independent of the speciﬁc data and analysis themselves gously data characteristics must drive the design and implementation of data storage systems enabling eﬃcient storage access and manipulation Within this vision the COST Action addresses the speciﬁc challenges of both M S and HPC in uniﬁed way The participants of the COST Action jointly work towards uniﬁed work for the systematic advancement of M S and big data endeavours supported by leading models and tools through coordinated eﬀort of HPC and M S experts The main objective is to create sustainable reference network of research links between the HPC community on the one hand and the multiple M S research communities addressing big data problems on the other hand Such links enable novel and persisting collaboration framework across HPC and M S communities covering both academia and industry across Europe and beyond with common agenda turning huge amounts of raw data into useful knowledge The remainder of this paper is organised as follows We ﬁrst illustrate the background of our work and review the current state of the art in Sect Grelck et Following this introductory part we have closer look at the subjects relevant to the four working groups that make up the COST Action We focus on Enabling Infrastructures and Middleware for Modelling and Simulation in Sect Parallel Programming Models for Modelling and Simulation in Sect Modelling and Simulation for Life Sciences in Sect Modelling and Simulation for and Physical ences in Sect respectively Last but not least we draw some conclusions in Sect Background and State of the Art Computing is currently undergoing major change with exascale systems expected for the early They will be very diﬀerent from today s HPC systems and pose number of technological challenges from energy consumption to the development of adequate programming models for millions of computing elements Several current exascale research programmes therefore span to period Major experiments depend on HPC for the analysis and interpretation of data and the simulation of models Modelling and Simulation have traditionally been used where the complexity of the problem makes more direct analytical approaches unsuitable or ble This is particularly true for big data problems where the support of HPC infrastructures and programming models is essential The design and tion of big data experiments and large scale HPC systems require the realistic description and modelling of the data access patterns the data ﬂow across the local and wide area networks and the scheduling and workload presented by hundreds of jobs running concurrently and exchanging very large amounts of data big data HPC is arguably fundamental to address M S problems In fact several M S approaches are based on frameworks due to their eﬃciency and scalability M S have addressed problems such as ing in distributed heterogeneous environments resource tion big data access in distributed environments and more generic HPC rent distributed and cloud architecture As described in the CERN Big Data HPC infrastructure stochastic data traﬃc management of virtual machines and job allocation in data centers represent problems which require extensive use of M S and HPC itself Attempts to describe and analyse hardware middleware and application an important ment direction for HPC have been made but they currently appear too costly The complexity can be reduced by means of models which need precise measures of uncertainty and associated errors and statistical inference Simulations have been run in this context for systems with one million cores Recent trends aim to empower programmers to more easily control the hardware performance Examples include the embedding of HPC facilities in standard distributions From an application perspective M S started to play crucial role in number of diverse knowledge domains Preliminary proposals Why HPC Modelling and Simulation for Big Data Applications Matters with direct porting of existing techniques in HPC in climate modelling and further developments are being sought In computational electromagnetics modelling problems with up to one billion variables have been addressed with both and algorithms solving major longstanding lems More structured approaches based on parallel programming eﬀectively cater for the design and development of parallel pipelines for M S in systems biology and next generation sequencing providing developers with portability across variety of HPC platforms like clusters of as well as cloud infrastructures However M S still not reached fully satisfactory rity facing relevant problems in terms of computational eﬃciency and lack of generality and expressiveness when addressing scenarios The development of new complex M S applications requires rative eﬀorts from researchers with diﬀerent domain knowledge and expertise Since most of these applications belong to domains within the life social and physical sciences their mainstream approaches are rooted in abstractions and they are typically not Recent surveys of the use of HPC in life sciences illustrate possible new scenarios for knowledge extraction and the management of and erogeneous data collections with numerous applications in medical ics Valuable big data diagnostic applications are being developed with the aim of improving diagnosis by integrating images and large data sets These come at the extraordinary price of infrastructure and suﬀer from the lack of standard protocols for big data representation and processing Once computational results are obtained large amounts of information need validation For instance in studies tion typically involves additional work that to be geared towards statistically signiﬁcant distilled fragment of the computational results suitable to conﬁrm the hypotheses and compatible with the available resources Big data is an emerging paradigm whose size and features are beyond the ity of the current M S tools Datasets are heterogeneous they are duced by diﬀerent sources and are of large size with high rates big data accessibility and the capability to eﬃciently bring and combine data together will be extremely valuable Currently many M S eﬀorts have been proposed in several big data contexts as diverse as performance evaluation and the management of HPC frameworks research on blood the numerical evaluation of quantum dynamics computational social network sis the relationship between Internet use and speciﬁc emotions human sity or happiness and genomic sequence discovery Some approaches have been successful leading to potential industrial impact and supporting experiments that generate petabytes of data like those performed at CERN for instance Furthermore there are growing number of new implementations of demanding applications that have not yet been adapted for HPC environments mainly because of limited communication between ﬁeld experts and those with Grelck et suitable skills for the parallel implementation of applications Therefore another natural objective of our work is to intelligently transfer the heterogeneous workﬂows in M S to HPC which will boost those scientiﬁc ﬁelds that are essential for both M S and HPC societies Beneﬁts will be cal M S experts are be supported in their investigations by HPC frameworks currently sought but missing HPC architects in turn obtain access to wealth of application domains by means of which they will better understand the speciﬁc requirements of HPC in the big data Among others we aim at the design of improved oriented programming models and frameworks for M S Enabling Infrastructures and Middleware for Modelling and Simulation From the inception of the Internet one witnessed an explosive growth in the volume speed and variety of electronic data created on daily basis Raw data currently originates from numerous sources including mobile devices sensors instruments CERN LHC MR scanners etc computer ﬁles Internet of Things data archives system software logs social networks commercial datasets etc The challenge is how to collect integrate and store with less hardware and software requirements tremendous data sets generated from distributed sources The big data problem requires the continuous improvement of servers storage and the whole network infrastructure in order to enable the eﬃcient analysis and interpretation of data through data management applications solutions in Agent Component in Oracle Data Integrator ODI The main challenge in big data modelling and simulation is to deﬁne complete framework which includes intelligent management and munication data fusion mapping algorithms and protocols The programming abstractions and data manipulation techniques must therefore be designed for the seamless implementation of application solutions with eﬃcient levels of virtualisation of computational resources communications storage and servers and b the eﬀective normalisation and merging of data with dissimilar types into consistent format wide class of data services is an important aspect of big data computing and ulation The goal is to reduce the gap between the capacity provided by tributed computing environments and application requirements especially ing low workload periods Various eﬀorts are undertaken to develop energy eﬃcient task scheduling and balancing of loads and frequency scaling techniques Infrastructure and Middleware for Big Data Processing Numerous algorithms computing infrastructures and middleware for HPC and big data processing have been developed during previous decades In general current middleware Why HPC Modelling and Simulation for Big Data Applications Matters for parallel computating focuses on providing powerful mechanisms for ing communication between processors and environments for parallel machines and computer networks High Performance Fortran HPF OpenMP OpenACC PVM and MPI were designed to support communications for scalable tions The application paradigms were developed to perform calculations on shared memory machines and clusters of machines with distributed memory However the easy access to information oﬀered by the internet led to new idea extending the connection between computers so that distributed resources including computing power storage applications etc could be accessed as ily as information on web pages The idea was implemented in many forms but lately it grown into three main computing environments computing ters grids and clouds survey of software tools for supporting cluster grid and cloud computing is provided in Examples of commonly known kernels for cluster computing are MOSIX OpenSSI and Kerrighed Uniform interfaces to computing resources in grids and toolkits for building grids UNICORE or Globus Toolkit are described in literature Cloud computing infrastructures consisting of services delivered through mon centers and built on servers are discussed in An alternative to supercomputers and computing Purpose Graphics Procession Unit GPGPU widely used in HPC tion Using both CPU and GPU through CUDA or OpenCL many applications can rather easily be implemented and run signiﬁcantly faster than on or systems Tools and Platforms for Big Data Processing Job scheduling load balancing and management play crucial role in HPC and big data simulation TORQUE is distributed resource manager providing control over batch jobs and distributed compute nodes Slurm is an open source and highly scalable cluster management and job scheduling system for large and small Linux clusters MapReduce is framework that simpliﬁes the ing of massive volumes of data through using two subsequent functions the Map function that sorts and splits the input data and the Reduce function that is responsible for processing the intermediate output data Resource management and job scheduling technology like YARN allows multiple data processing engines such as batch processing streaming interactive SQL and data science to handle data stored in single platform The Apache Hadoop software library supports the distributed scalable batch processing of large data sets across clusters of computers using simple programming model The power of the Hadoop platform is based on the Hadoop Distributed File System HDFS distributed and scalable database HBase MapReduce YARN and many other open source projects Some of the include Spark Pig Hive JAQL Sqoop Oozie Mahout etc Apache Spark uniﬁed engine for big data processing provides an alternative to MapReduce that enables workloads to execute in memory instead of on disk Thus Spark avoids the disk operations that MapReduce requires It processes data in RAM utilizing data model based on the Resilient Distributed Dataset RDD Grelck et abstraction Apache Storm is scalable rapid platform for distributed computing that the advantage of handling real time data ing downloaded from synchronous and asynchronous systems Apache Flink can be used to batch and stream processing processing and stateful computations for simulation Platforms for Big Data Visualisation and Machine Learning Numerous tools for big data analysis visualisation and machine learning have been made available RapidMiner Studio Orange and Weka belong to this group New software applications have been developed for browsing visualizing interpreting and analyzing sequencing data Some of them have been designed speciﬁcally for visualisation of genome sequence assemblies including Tablet Other tools such as BamView have been developed speciﬁcally to visualise mapped read alignment data in the context of the reference sequence Artemis is freely available integrated platform for visualisation and ysis of experimental data The survey of platforms and packages for social network analysis simulation and visualisation that have wide applications including biology ﬁnance and sociology is presented in Frameworks for Big Data Systems Simulation Another issue is concerned with systems simulation The combination of eﬃcient and reliable tion software and hardware optimized for simulation workloads is crucial to fully exploit the value of simulation and big data Synchronous and asynchronous distributed simulation have been one of the options that could improve the scalability of simulator both in term of application size and cution speed enabling large scale systems to be simulated in real time ScalaTion provides comprehensive support for simulation and big data analytics software framework for federated simulation of WSN and mobile networks is presented in The paper reviews several scale military simulations and describes two frameworks for data management based on layered and service oriented architectures simulation forms are mainly dedicated to massive data processing neural network simulators simulation of P systems ume of data simulation and visualisation Numerous software platforms have been designed to simulate tributed data centers and computer networks JADE is the heterogeneous multiprocessor design simulation environment that allows to simulate networks and networks using optical and electrical interconnects SimGrid can be used to simulate grids clouds HPC or systems and evaluate heuristics or prototype applications CloudSim is one of the most popular open source framework for modeling and simulation of cloud computing infrastructures and services is software platform for simulation of new CPU and GPU technologies Why HPC Modelling and Simulation for Big Data Applications Matters Parallel Programming Models for Modelling and Simulation core challenge in modelling and simulation is the need to combine software expertise and domain expertise Even starting from mathematical models manual coding is inevitable When parallel or distributed computing is required the coding becomes much harder This may impair performance and performance portability across diﬀerent platforms These lems have been traditionally addressed by trying to lift software design and development to higher level of abstraction In the languages DSL approach abstractions aim to vide domain experts with programming primitives that match speciﬁc concepts in their domain Performance and portability issues are ideally moved with various degrees of eﬀectiveness to development tools Examples include Verilog and VHDL hardware description languages MATLAB for matrix programming Mathematica and Maxima for symbolic mathematics etc In approaches such as engineering MDE programming concepts are abstracted into constructs enforcing features by design compositionality portability parallelisability In this regard the number and the quality of programming els enabling the management of parallelism have steadily increased and in some cases these approaches have become mainstream for range of HPC and big data workloads streaming Storm and Spark structured parallel programming and MapReduce Hadoop Intel TBB OpenMP MPI SIMD OpenACC This list can be extended by various academic approaches including ones proposed and advocated by members of the COST Action FastFlow SkePU SaC sensible result achieved by the working group on Parallel Programming Models for Modelling and Simulation been the assessment of the state of the art selection of the mainstream approaches in this area are sented in Sect namely Google MapReduce Apache Spark Apache Flink Apache Storm and Apache Beam In Sect we describe systematic mapping study aimed to capture and categorise DSLs Languages and Frameworks for Big Data Analysis Boosted by big data popularity new languages and frameworks for data lytics are appearing at an increasing pace Each of them introduces its own concepts and terminology and advocates real or alleged superiority in terms of performance or expressiveness against its predecessors In this hype for user approaching big data analytics even an educated computer scientist it is increasingly diﬃcult to retain clear picture of the programming model neath these tools and the expressiveness they provide to solve some problem Grelck et To provide some order in the world of big data processing toolkit of models to identify their common features is introduced starting from data layout applications are divided into batch stream processing Batch programs process one or more ﬁnite datasets to produce resulting ﬁnite output dataset whereas stream programs process possibly unbounded sequences of data called streams in an incremental manner Operations over streams may also have to respect total data ordering for instance to represent time ordering The comparison of diﬀerent languages for big data analytics in terms of the expressiveness of their programming models is exercise malised approach requires to map them onto an unifying and putation model the Dataﬂow model As shown in it is able to capture the distinctive features of all frameworks at all levels of abstraction from the API to the execution model In the Dataﬂow model cations as directed graph of actors In its modern ﬂow version it naturally models independent thus parallelizable kernels starting from graph of true data dependencies where kernel s execution is triggered by data availability The Dataﬂow model is expressive enough to describe batch and streaming models that are implemented in most tools for big data ing Also the Dataﬂow model helps in maturing the awareness that many big data analytics tools share almost the same base concepts diﬀering mostly in their implementation choices For complete description of the Dataﬂow model we refer back to where the main features of mainstream languages are presented Google MapReduce Google can be considered the pioneer of big data cessing as the publication of the MapReduce framework paper made this model mainstream Based on the map and reduce functions commonly used in parallel and functional programming MapReduce provides native value model and sorting facilities These made MapReduce successful for several big data analytics scenarios MapReduce program is built on the following functions map function that is independently applied to each item from an input dataset to produce an intermediate dataset reduce function that combines all the intermediate values associated with each key together with the key itself into lists of reduced values one per key partitioner function that is used while sorting the intermediate dataset before being reduced so that the order over the key space is respected within each partition identiﬁed by the partitioner Parallel Execution simple form of data parallelism can be exploited on the side by partitioning the input collection into n chunks and having n executors process chunk In Dataﬂow terms this corresponds to graph with n actors each processing token that represents chunk Each executor Why HPC Modelling and Simulation for Big Data Applications Matters emits R the number of intermediate partitions chunks each containing the intermediate pairs mapped to given partition The intermediate chunks are processed by R reduce executors Each executor receives n chunks one from each executor merges the chunks into an intermediate partition and partially sorts it based on keys as discussed above performs the reduction on basis Finally downstream collector gathers R tokens from the reduce executors and merges them into the ﬁnal result key aspect in the depicted parallelisation is the shuﬄe phase in which data is distributed between and reduce operators according to an communication schema This poses severe challenges from the implementation perspective Support The most widespread implementation Hadoop is based on approach in which the master retains the control over the global state of the computation and informs the workers about the tasks to execute cornerstone of Hadoop is its distributed ﬁle system HDFS which is used to exchange data among workers in particular upon shuﬄing As key feature HDFS exposes the locality for stored data thus enabling the principle of moving the computation towards the data and to minimise communication However communication leads to performance problems when dealing with iterative computations such as machine learning algorithms Apache Spark Apache Spark was proposed to overcome some tions in Google s MapReduce Instead of ﬁxed processing schema Spark allows datasets to be processed by means of arbitrarily composed primitives ing directed acyclic graph DAG Moreover instead of exclusively relying on disks for communicating data among the processing units caching is exploited to boost performance in particular for iterative processing Parallel Execution and Runtime Support From the application DAG Spark infers parallel execution dataﬂow in which many parallel instances of actors are created for each function and independent actors are grouped into stages Due to the Spark implementation each stage that depends on some previous stages to wait for their completion before execution commences equivalent to the classical Bulk Synchronous Parallelism BSP approach Thus computation proceeds in series of global supersteps each consisting of concurrent computation in which each actor processes its own partition communication where actors exchange data between themselves if necessary the shuﬄe phase barrier synchronization where actors wait until all other actors have reached the same barrier Grelck et Similar to the MapReduce implementation Spark s execution model relies on the model cluster manager YARN manages resources and supervises the execution of the program It manages application scheduling to worker nodes which execute the application logic the DAG that been serialized and sent by the master Apache Flink Apache Flink is similar to Spark in particular from the API standpoint However Flink is based on streaming as primary concept rather than mere linguistic extension on top of batch processing as Spark With the exception of iterative processing stream parallelism is exploited to avoid expensive synchronizations among successive phases when executing both batch and stream programs Parallel Execution and Runtime Support Flink transforms JobGraph into an ExecutionGraph in which the JobVertex contains ExecutionVerteces actors one per parallel key diﬀerence compared to the Spark execution graph is that apart from iterative processing that is still executed under BSP there is barrier among actors or verteces Instead there is eﬀective pipelining Also Flink s execution model relies on the model ment at least one job manager process that receives Flink jobs and nates checkpointing and recovery The job manager also schedules work across the task manager processes the workers which usually reside on separate machines and in turn execute the code Apache Storm Apache Storm is framework that exclusively targets stream processing It is perhaps the ﬁrst widely used stream ing framework in the open source world Whereas Spark and Flink are based on declarative data processing model they provide as building blocks data collections and operations on those collections Storm in contrast is based on topological approach in that it provides an API to explicitly build graphs Parallel Execution and Runtime Support At execution level each actor is cated to increase the parallelism and each group of replicas sponds to the in the semantics Dataﬂow Each of these actors sents independent tasks on which pipeline parallelism is exploited Eventually tasks are executed by engine as in the previously discussed frameworks Google Cloud Platform and Apache Beam Google Dataﬂow SDK is part of the Google Cloud Platform Google Dataﬂow supports simpliﬁed pipeline development via Java and Python APIs in the Apache Beam SDK which provides set of windowing and session analysis primitives as well as an ecosystem of source and sink connectors Apache Beam allows the user to create pipelines that are executed by one of Beam s supported distributed ing which are called runners Currently they include among others Apache Flink Apache Spark and Google Cloud Dataﬂow Why HPC Modelling and Simulation for Big Data Applications Matters Parallel Execution and Runtime Support The bounded or unbounded nature of PCollection also aﬀects how data is processed Bounded PCollections can be processed using batch jobs that might read the entire data set once and perform processing as ﬁnite job Unbounded PCollections must be processed using streaming the entire collection will never be available for processing at any one bounded subcollections can be obtained through logical ﬁnite size windows As mentioned Beam relies on the runner speciﬁed by the user When cuted an entity called Beam Pipeline Runner related to execution translates the data processing pipeline into the API compatible with the selected distributed processing Hence it creates an execution graph from the Pipeline including all the Transforms and processing functions That graph is then executed using the appropriate distributed processing becoming an asynchronous on that Thus the ﬁnal parallel execution graph is generated by the The parallel execution data ﬂow is similar to the one in Spark and Flink Parallelism is expressed in terms of data parallelism in Transforms ParDo function and parallelism on independent Transforms In Beam s nomenclature this graph is called the Execution Graph Similar to Flink pipeline parallelism is exploited among successive actors The Systematic Mapping Study on Parallel Programming Models for Modelling and Simulation major challenge undertaken within the working group on Parallel ming Models for Modelling and Simulation was that of trying to stand and classify the state of the art in this area and to better understand the lines of future development In order to minimize the bias given that many Action participants actively design programming models and tools the ing group reﬁned and adopted systematic methodology to study the state of the art called systematic mapping study SMS The mapping study focused on the main paradigms and properties of programming languages used in performance computing for gig data processing The SMS started from the deﬁnition of workﬂow based on the methodology proposed in that is organised in ﬁve successive steps Research Questions aiming at formulating the research questions the SMS should answer Search of Primary Studies aiming at detecting the largest number of primary articles related to the proposed research questions Selection of Primary Studies aiming at sieving false positive by driven abstract inspection Quality Assessment aiming at validating the ﬁtness of the articles against the aims of the SMS Data Extraction and Synthesis which aims at answering each research tion for all selected articles Grelck et Speciﬁcally the SMS focused on languages and explicitly excluded languages such as C OpenMP Fortan Java Python Scala combined with parallel exploitation libraries such as MPI Quantitatively in the SMS the initial literature search resulted in cles articles were retained for ﬁnal review after the evaluation of initial search results by domain experts Results of our mapping study indicate for instance that the majority of the used HPC languages in the context of big data are programming languages and target the user community To evaluate the outcome of the mapping study we developed questionnaire and collected the opinions of domain experts comparison of mapping study outcome with opinions of domain experts reveals that the key tures of HPC programming languages for big data are portability performance and usability We identiﬁed the language learning curve and interoperability as the key issues that need more attention in future research Modelling and Simulation for Life Sciences Life Sciences typically deal with and generate large amounts of data the ﬂux of terabytes about genes and their expression produced by state of the art sequencing and microarray equipment or data relating to the dynamics of cell biochemistry or organ functionality Some modelling and simulation techniques require the investigation of large numbers of diﬀerent virtual experiments those addressing probabilistic noise and robustness aspects or based on statistical approaches Curation and mining of large typically multimedia medical datasets for therapeutic and analytics purposes are computationally expensive Recent and future developments such as personalised medicine need to integrate mix of genomics Systems and Synthetic Biology and medical information in systemic description of single individual surge of scale computational needs in these areas spans from the BBMRI Biobanking and Biomolecular Resources Research Infrastructure and the ﬂagship eﬀort Human Brain Project which targets simulating the behaviour of human brain to projects like and TRANSFORM In fact this COST Action integrates well with the goals of the ESFRI Roadmap promoted by the EC Requirements go from pure computational eﬃciency to large data ﬁle management and storage capabilities and vast computational power This section focuses on the integration of HPC architects and Life Sciences modellers with the goal of letting them develop and diﬀuse coordinated mature and productive use of HPC facilities In order to bridge these two communities some big data problems applications and modelling techniques in the broad context of live sciences are discussed We will consider approaches for modelling healthcare and diseases as well as problems in systems and synthetic biology We will survey some themes on genomics and metabolic networks then discuss eﬃcient modelling and learning techniques and ﬁnally consider also the modelling of the management of healthcare Why HPC Modelling and Simulation for Big Data Applications Matters Healthcare and Disease Modelling Understanding disease complexity is the inite scientiﬁc challenge of the century medicine Using tional models is the path to technological medical revolution where modelling will have truly catalytic eﬀect in biomedical big data by bridging the large body of knowledge produced by next generation genomic data with the clinical quantities and the functional observable for instance through self monitor and implantable sensor devices Biomedicine is essentially big data ﬁeld and elling is superior to the correlative approach in transforming data into knowledge Taking into account only the DNA sequencing data its rate of tion is much larger than other major generators of big data such as astronomy YouTube and Twitter Recent estimates show that the total amount of ing data produced is doubling approximately every seven months The growth is driven by three main factors Biomedicine is heavily interdisciplinary and requires physicians bioinformaticians computer scientists and engineers to team up Although they continuously produce results that are underutilised in medical tice such interdisciplinarity generates the need for data tion Areas such as systems medicine clinical informatics systems biology and bioinformatics have large overlaps with classical ﬁelds of medicine and extensively use biological information and computational methods to infer new knowledge towards understanding disease mechanism and diagnosis Many acute and chronic diseases originate as network diseases patient s condition is characterised by multiple complex and interrelated conditions disorders or diseases state of health can be deﬁned as the ity of absorbing accidents and showing metabolic ﬂexibility and is altered by infections and ageing that cause comorbidities to emerge Therefore stratiﬁcation of patient requires lots of information The bridge between the characterisation of disease mechanism and the stratiﬁcation of patient would require computational model Current successful approaches focus on hybrid modelling approaches including cellular automata stochastic diﬀerential equations and els The more eﬀective the diagnostic and prognostic markers are the less information will be needed to correctly stratify patient This aspect makes precision medicine highly intensive In particular plex disease management is mostly based on electronic health records lection and analysis which are expensive processes Analyses are presented in rather empirical and sometimes simplistic way completely missing the opportunity of uncovering patterns of predictive relationships and meaningful proﬁles Our chances to make the data the drivers of paths to cures for many complex diseases depends in good percentage on extracting evidences from electronic records comparison and on models of disease ries The medical approach to comorbidities represents an impressive tational challenge mainly because of data synergies leading to the integration of heterogeneous sources of information the deﬁnition of deep phenotyping Grelck et and markers the establishment of clinical decision support tems Computational model development is further complicated by aspects of and ethnic balance protocols for sharing of digital mation interoperability between diﬀerent record types structured and structured to optimize the process of decision making in an actionable way third important factor is the nature of the biomedical mation The genome sequence is only the ﬁrst level of understanding of the human biology Bioinformatics data resources should be much more populated with longitudinal information gathered at intracellular cell intercellular and tissue levels The longitudinal sampling could happen for important clinical events such as hospitalisation or routinely perhaps few times from uterine to elderly age At the bioinformatics level genome wide information of all the diﬀerent levels of biological information will be integrated and this may include Genomic sequence variations haplotypes levels of gene functioning for diﬀerent tissues and conditions circadian and longitudinal data gene expression Epigenetic changes for diﬀerent tissues methylations and tonic modiﬁcations information on chromatin conformation for diﬀerent cell types and conditions FISH microscopy protein and metabolites abundances for diﬀerent cell types and conditions interaction variations longitudinal data For instance by using the Next Generation Sequencing technology approaches cancer clones subtypes and metastasis could be appropriately traced biome data number type and multi Omics for diﬀerent part of the body and diﬀerent conditions Furthermore gut microbiome could be regularly sampled monitoring the diets and nutritional shifts This could be of great importance for epigenetic data which shows alteration with ageing inﬂammatory diseases sity cardiovascular and neurodegenerative diseases Gene expression may vary in relation to the circadian cycle or ageing Sampling may focus on determining the actual level of inﬂammation that is related to ageing rate inﬂammaging Large number of images of the diﬀerent parts of the patient s body such as MRI PET CT scan including intravital microscopy techniques can be used The images will tend to be progressively enriched with genomics and proteomics data information disease ﬁrst emerges as dysfunction at the nucleus level then metabolic and signalling cell level and is then translated at the tissue level due to change in the cell response The tissue level is central to stem cells organisation in maintaining the mechanical properties of the tissue the current thinking is that the dominant eﬀect of reduced stem cell activity and failing tissue maintenance is due to changes in the niches that support and trol stem cell activity Therefore tissue modelling can be thought as the missing link between basic research and clinical practice and will require conceptual framework for an eﬃcient analysis between the cell and tissue levels The cell level will be represented with or ODE models that will be speciﬁcally developed to handle millions of single cells The tissue level will be represented using ﬁnite element modelling partial diﬀerential equation PDE Why HPC Modelling and Simulation for Big Data Applications Matters An important example is the bone system which is also related to the immune and endocrine systems The osteocytes in the bone act as sensitive sors so they react to microdamages that alter the tension with their ﬂattened morphology and long processes they form sensory network which allows the detection of abnormal strain situations such as generated by microcracks mal locomotion is thought to cause microdamage to bone material and thus stimulate osteoclasts and osteoblasts to remove and then replace damaged tissue They can be modelled as agents driven by signals and could reﬂect tions and velocities Osteocytes are connected to one another and to surface osteoblasts via gap junctions In general mechanical forces are experienced by many osteoprogenitor cells which are present in the bone marrow and in the soft mesenchymal tissues subjected to mechanical strain Dependant on the nitude of mechanical stress osteoprogenitors diﬀerentiate or transdiﬀerentiate into osteoblastlike cells that express characteristic proteins and can form bone matrix Under physiological mechanical stimuli osteocytes prevent bone tion by changing the OPG ratio By communicating these signals to bone lining cells the second terminally diﬀerentiated osteoblast cell type or secrete factors that recruit osteoclasts osteocytes initiate the repair of damaged bone The functional behaviour of bone tissues is primarily described in term of physical quantities such as pressures and forces to reﬂect tion loading stress strain etc Such quantities are usually considered to vary across space and time in continuous fashion and can be thus represented using ﬁelds and systems of partial diﬀerential equations PDE The transition between continuous representation and discrete representation makes the coupling of the models across the scale particularly diﬃcult tional homogenisation approaches frequently used as relation models to link to component models deﬁned at diﬀerent scales are computationally resource demanding Modelling Problems in System and Synthetic Biology Systems Biology approaches and methodologies are also very interesting in Synthetic Biology pipelines in minimal cells for instance liposomes are sized with some metabolic networks entrapped inside These devices called protocells share some properties in common with real biological cells and can perform some biological action In the problem is which metabolic component to choose among the several diﬀerent ones that can perform the same biological action combinatorial experimental approach is not aﬀordable since it requires lot of time budget and lab resources computational approach instead is very useful as it can score the diﬀerent hypotheses about the tocell to synthesize sorting out the best theoretically performing Along this research line several papers have been published based on computer simulation of the metabolic networks entrapped in the protocells to understand the solute distribution and enrichments processes and the energetic balance of complex biological processes like DNA transcription and RNA translation Grelck et Genomics In recent years thanks to faster and cheaper sequencing machines huge amount of whole genomic sequences within the same population become available Modern genomes analyses workﬂows have thus to face new challenges to perform functional annotations and comparative sis as there is longer just reference genome but rather many of them that can have to be used all together as control sequence collection of genomic sequences to be analysed jointly or to be jointly used as reference is called pangenome The reference genome is representative example of the whole genomic sequence of species acting as control sequence against which ments of newly sequenced individual are mapped to be located or against which another whole genome is compared single well annotated reference genome was and mostly still is traditionally used as control sequence as it could provide good approximation of any individual genome However in loci where polymorphic variations occur polymorphism is genetic variation of an individual or population such mappings and comparisons are likely to fail this is where multiple reference reference be better control In the data structure literature several diﬀerent compressed representations have been considered for sets of similar texts as well as algorithmic methods for their investigation We present here natural representation of whole genomes or their fragments texts An text is sequence compactly representing ple alignment of several sequences substrings that match exactly are collapsed while those in positions where the sequences diﬀer by means of substitutions insertions and deletions of substrings are called degenerate and therein all possible variants observed at that location are listed Actually correspond exactly to the Variant Call Format the standard for ﬁles storing genomic variations As an example consider the ing three sequences where their similarity is highlighted by their alignment and where the symbol represents deletion CAATGTGTGAC C These sequences can be compacted into the single AA AG ε GTG CAA AC C T T G ε AC where ε is the empty string The length n of is the total number of segments and its size N is the total number of letters that all belong to an alphabet Σ Due to biotechnologies limitations sequencing that is giving as input the in vitro DNA and getting out an in silico text ﬁle can only be done on genome fragment of limited size For this reason before the sequencing process genome is actually broken into many fragments of such limited size and then whenever Why HPC Modelling and Simulation for Big Data Applications Matters reference is available the resulting in silico fragments named reads are mapped onto it This mapping is critical step and there is an ample literature aiming at making as eﬃcient as possible When the reference is an the reads mapping problem translates into the problem of ﬁnding all matches of ministic pattern P that is P in text We call this the EDSM problem In the problem been solved for the simplest case of in which degenerate segment can only contain single letters In the problem been eﬃciently solved for the more general case of introducing i an algorithmic framework that been conserved also by more recent papers and ii adding very fast based version of the same algorithm that requires the pattern to have size longer than the machine word In the algorithmic framework been extended to ﬁnd approximate occurrences of P under both the Hamming and the edit distance model In other words occurrences of P are detected allowing up to given amount of mismatches Hamming distance model or even insertions or deletions edit distance model In the algorithm of been extended to work with collection of patterns Ph rather than single string P and in step of the algorithm presented in been improved by factor Another natural problem that arises is the comparison of two and in particular whether the sets of strings the actually represent non empty intersection This problem been eﬃciently solved in with linear time algorithm for the case of degenerate segment can only contain strings of the same size Once that set of DNA fragments of an individual have been aligned lotype phasing is an important problem in the analysis of genomics tion It consists of determining which one of the possible alleles alternative forms of gene each fragment comes from Haplotype information is relevant to gene regulation epigenetics association studies evolutionary and population studies and the study of mutations Haplotyping is currently addressed as an optimisation problem aiming at solutions that minimise for instance error correction costs where costs are measure of the conﬁdence in the accuracy of the information acquired from DNA sequencing Solutions have typically an exponential computational complexity WHATSHAP is framework returning exact solutions to the problem of haplotyping which moves computational complexity from DNA fragment length to fragment overlap coverage and is hence of particular interest when considering sequencing nology s current trends that are producing longer fragments Nonetheless the combinatorial nature of the problem makes larger coverages quickly intractable An interesting experiment paradigmatic of modelling tion is pWHATSHAP multicore parallelisation of WHATSHAP based on the FastFlow parallel programming framework This parallel implementation on architectures allows for relevant reduction of the execution time for haplotyping while the provided results enjoy the same high accuracy as that provided by WHATSHAP which increases with coverage Grelck et Metabolic Network Robustness Analysis Many functional modules are linked together in Metabolic Network for reproducing metabolic pathways and describing the entire cellular metabolism of an organism An enormous ciplinary interest grown for metabolic networks robustness studies in terms of errors and attacks tolerance networks based approaches suggest that metabolic networks are tolerant to errors but very vulnerable to targeted attacks against highly connected nodes An integrated approach based on tistical topological and functional analysis allows for obtaining deep edge on overall metabolic network robustness With more details several ware frameworks were developed to model metabolic network and perform the Topological Analysis the Flux Balance Analysis and the Extreme ways Analysis over it The simulation trials have demonstrated that metabolic network robustness is not simply associated to the network local erties node or node but also to functional network properties So nodes can assume fundamental role for network survival if they belong to network extreme ways while hub nodes can have limited impact on networks if they can be replaced by alternative nodes and paths The same approach have been applied as optimisation method to diﬀerent application domains In the use of the previous niques allows for analysing the structural aspects of road network ﬁnding its extreme pathways and outlining the balanced ﬂow combinations The approach optimises traﬃc ﬂows over road network minimises road congestion and imises the number of vehicles reaching their destination target In the inspired methodology been applied to class of digital ecosystems based on architecture for both maximum information ﬂow and erance detection Highly connected nodes connectors and peripheral nodes can be identiﬁed by evaluating their impact on digital ecosystems behavior and addressing their strengthen fault tolerance and protection measures Modelling Methodologies The computational analysis of complex biological tems can be hindered by three main factors modelling the system so that it can be easily understood and analysed by users is not always possible When the system is composed of hundreds or thousands of reactions and chemical species the classic simulators could not be appropriate to eﬃciently derive the behaviour of the system To overcome these ﬁrst two limitations proposes novel approach that combines the descriptive power of Stochastic Symmetric Nets graphical mathematical formalism with LASSIE deterministic simulator that oﬄoads onto the GPU the calculations required to execute many simulations by following both and parallelisation strategies The eﬀectiveness of this approach was showed on case study aimed at understanding the role of possible malfunctions in the mechanisms that regulate Why HPC Modelling and Simulation for Big Data Applications Matters peripheral tolerance of T lymphocytes in case of remitting multiple sclerosis From our experiments LASSIE achieves around with respect to the sequential execution of the same number of simulations The determination of model structure and model parameters is diﬃcult Due to economical and technical reasons only part of these details are well acterised while the rest remains unknown To deal with this aspect many parameter estimation and reverse engineering methods were developed ever these methods often need an amount of experimental data that not always is available An alternative approach to deal with situations in which insuﬃcient imental data hamper the application of PE and RE methods was proposed in To overcome the lack of information concerning undetermined reactions an empirical biological knowledge was exploited to overcome model indetermination solving an optimisation problem OP with an objective function that similarly to Flux Balance Analysis is derived from empirical biological knowledge and does not require large amounts of data The system behaviour is described in detail by system of ordinary diﬀerential equations ODE while model mination is resolved selecting coeﬃcients that the objective function at each ODE integration step As discussed by the authors in this context approximation techniques in which OP is not solved at every integration step parallelisation strategies are mandatory to the solution process Modelling Approaches Some interesting applications in this text are based on the study of integrated biological data and how they are organised in complex systems In particular these approaches focus on omic spaces and analysis They are very complex applications that require analysis based on advanced machine learning ML and more recently deep learning DL One of the several applications in this ﬁeld is described by Bardozzo et where high throughput omic analysis HTO is adopted with the aim to the end of describing the antibiotics eﬃcacy with respect to the bacterial adaptive mechanisms Moreover speciﬁc survey on HTO is introduced by Suravajhala et Nevertheless general survey oriented to high throughput biomedical data analysis with ML and DL is widely described in the work of Serra et Healthcare Management Modelling Globally healthcare faces many challenges that result in increasing healthcare costs and poor outcomes ity or mortality depending on the setting and demographic These challenges have been traced to weak health systems whose symptoms can manifest in low productivity poor ﬁnancial management inadequate information for decision making insuﬃcient strategic management issues with managed care and other systems dynamics The persistent challenges in the healthcare sector call for urgent review of strategies Several industry application of operations management have been Grelck et documented There also been diverse application of operations agement techniques in several domains including the health sector While there is still room for health modiﬁcation adoption of innovations used in other domains been slow major classiﬁcation identiﬁed resource and ity management demand forecasting inventory and supply chain management and cost measurement as application groupings to prioritise An area of increasing interest is human resource planning that captures recruitment rostering scheduling and management of clinical and staﬀ their retention training payment and incentives as well as performance appraisal Challenges do also arise around patient workﬂow admission ing and resource allocation To model solutions to these process and workﬂow challenges simple statistics stochastic mathematical artiﬁcial ligence lean agile and total quality management based models have been variously proposed and used Sometimes quate data may warrant simulation to ﬁll in deterministic and data gaps This obviously comes with the need for adequate computing and storage capabilities The optimum framework for modelling and simulating particular depends on the availability structure and size of data Other considerations will be if the system should be automated or not if they are sophisticated ministic or not The choice of model simulation technique can ultimately be inﬂuenced by available computing power and storage space How such system is will be major consideration as well Opportunities for cation of one or more of these modelling simulation and prediction techniques to address some of the lingering healthcare challenges is huge Modelling and Simulation for and Physical Sciences Many types of decisions in society are supported by modelling and simulation Some examples are political decisions based on predictive simulations of future climate changes evacuation planning based on simulation of tsunamis and ﬁnancial market decisions based on mathematical models lating current market conditions In all of these situations large amounts of data such as global geographical information measurements of the current physical or ﬁnancial state and historical data are used both in the model building and model calibration processes We can roughly divide the applications within the large and diverse area that we here call and physical sciences into two groups Classical HPC applications where we build complex model and simulate this in order to produce data as basis for decisions and Big data applications where the starting point is data set that is processed and analyzed to learn the behaviour of system to ﬁnd relevant features and to make predictions or decisions Why HPC Modelling and Simulation for Big Data Applications Matters In classical HPC applications the need for HPC arises from the fact that we have model or computationally heavy software implementation that needs to make use of computational resources and potentially also storage resources in order to deliver timely results Some particularly challenging problem features are in ﬁnance or quantum physics where the computational costs grow exponentially with the dimension physics in climate and tsunami simulations where scales that diﬀer in orders of magnitude need to be resolved to capture the relevant physical processes and computations under uncertainty where the impact of uncertain measurements parameters and models is quantiﬁed through multiple evaluations or extended models leading to an increased computational cost in safety critical decision problems Highly advanced algorithms and implementations for many diﬀerent cation areas have been developed over decades huge challenge is that these legacy codes are not optimized for modern computer architectures and can not eﬃciently exploit massively parallel systems HPC knowledge and tion is needed to merge the software and hardware into highly eﬃcient application simulation tools An opportunity that is brought by the increase in available computer power is instead that the limits of what can be simulated are expanding outwards The recent increase in research on uncertainty quantiﬁcation is one example of how this changed the computational research landscape Big data processing as opposed to classical HPC simulation is relatively young ﬁeld The amount of data that is being harvested is following an tial trend while hardware development often in relation to cloud environments and software development with speciﬁc focus on machine learning and AI is struggling to keep up The opportunities for using data in new ways are endless but as is suggested in data and algorithms together can provide the whats while the innovation and imagination of human interpreters is still needed to answer the whys Areas where we see rapidly growing need for HPC solutions is the internet of things where the expected vast amounts of data provides new challenges for the extraction of knowledge as well as in the social media context where all kinds of real world events or personal preferences provide footprints that can be tracked and exploited In the following paragraphs of this section we highlight some of the work and contributions of the participants in this Action within the diverse subﬁelds in the wider physical and application area Some of the topics are also represented as individual chapters later in this volume Classical HPC Applications In this the interplay of the algorithms with the parallel implementation is crucial and we provide two examples both with industrial design applications Wing design is one of the essential procedures of aircraft manufactures and it is compromise between many competing factors and constraints Eﬃcient numerical optimization methods are important to the design dure especially for design parameters of In wing shape optimization Grelck et necessary derivatives can easily be calculated by applying ods However ﬁnite diﬀerence methods are in general signiﬁcantly more sive requiring at least one additional ﬂow solution per parameter By using the method of modular analysis and uniﬁed derivatives MAUD we can unify all methods for computing total derivatives using single equation with associated sparse schemes Moreover the wing design requires set of benchmark cases for the shape optimization to ﬁnd solutions of many candidate shapes by applying computational ﬂuid dynamics CFD ysis with turbulence models CFD simulations must be carried out in parallel to reduce the total using HPC resources An application problem that is also discussed later in an individual chapter is electromagnetic scattering with applications to aircraft antenna design These problems have millions or billions of unknown variables and the code needs to run on cluster due to the memory requirements However few of the existing legacy implementations are parallelised for tional nodes We show results from pilot implementation using programming model and discuss how to develop this further into plete distributed implementation HPC in Computational Intelligence As thriving application platform HPC excels in supporting execution and it s speedup through parallellisation when running Computational Intelligence CI algorithms The likes of CI algorithms supported by this action includes development of some of most eﬃcient mization algorithms for continuous optimization as deﬁned with benchmark functions competition framework from Congress on Evolutionary Computation CEC Speciﬁcally useful in Diﬀerential Evolution algorithm is enhanced with new mechanism the distance based parameter adaptation in the context of based SHADE the winner strategy of several previous CEC competitions An important contribution of an expert system for underwater glider path planning using was published in where the application of SHADE strategy enabled signiﬁcant advances in improved path planning over mesoscale ocean current structures Another CI technique in learning pipeline is Stability Selection SS yet another tionally demanding technique like and SS was improved through discrete optimization algorithm In recent whole pipeline survey overview for discrete optimization benchmarking is provided ing taxonomy evaluation and ranking for algorithms Also in the case of EU project RIVR Upgrading National Research Structures in Slovenia ported by European Regional Development Fund ERDF an important eﬀect of cHiPSet COST action was leveraging it s experts inclusiveness to gain capacity recognition at national ministry for HPC In the view of future possibilities for modelling and simulation in CI context gain from HPC is clearly seen in improving upon techniques with like in https Why HPC Modelling and Simulation for Big Data Applications Matters energy applications constrained trajectory planning artiﬁcial life of full ecosystems including evolutionary computer vision in and many other well recognized optimization challenges or even insight to deep inner dynamics of over full marks requiring large HPC capacities IoT Smart Cities and Big Data Applications Monitoring the ronment is big challenge given the number of variables that can be sensed days in IoT environments as for example temperature humidity presence people location ultraviolet radiation air quality hazardous gases sure proximity acceleration IoT assumes that multiple sensors can be used to monitor the and this information can be stored and processed jointly with information from RSS web etc to for example assist elderly people in the street develop intelligent interfaces or detect anomalies in industrial environments In any case the developed global system needs to fuse heterogeneous data for obtaining complete view of actual situation and inferring future dangerous situations These two tasks need Cloud capabilities and HPC One of the critical aspects of management within the smart city concept is Intelligent Transport Systems and in particular road traﬃc control traﬃc simulation is perfect for designing road systems and planning traﬃc light timings but does not allow to tackle unexpected or rare situation in real time traﬃc forecasting especially using learning methods provides complementary approach With the availability of smart sensing technologies like automatic vehicle counting from standard surveillance cameras it is possible to devise decentralised solutions that measure the rent situation of traﬃc ﬂow on each road perform local communication between nodes and forecast the conditions for the immediate future using machine ing algorithms These may be augmented with evaluations of edness and traﬃc jam prediction Concentration of these data at location may also allow travel time estimation exploitation of network locality information as well as comparison with the estimates provided by traﬃc management system which can be evaluated for eﬀectiveness on the medium term and possibly tuned accordingly Further topics that are discussed in later chapters of this volume look at such diverse questions as how to use data from mobile cellular networks for tions such as urban sensing and event detection and how sentiment analysis can be applied to forecast the value of cryptocurrencies Small Data Applications The growing big data processing ﬁeld is well known but in parallel there is also growing interest in speciﬁc type of small data applications With the increasing instability of the ﬁnancial and political tems and of the global climate there is an increased occurrence of extreme events Within the big data sets there are small data sets that sample the extreme events To understand their behaviour applications in ﬁnancial Grelck et risk management in insurance and in prediction of catastrophic climate events In later chapter methods for extreme value estimation are surveyed Summary and Conclusion HPC and M S form two previously largely disjoint and disconnected research communities The COST Action Modelling and lation for Big Data Applications brings these two communities together to tackle the challenges of big data applications from diverse application domains Experts from both communities jointly study these applications and application ios and cooperatively develop solutions that beneﬁt from the of expertise Diﬀerent perspectives on the same topic lead to creative solutions and ultimately to the common goal of M S The purpose of this paper is to set the scene for individual applications ing together HPC and M S We have argued why modelling matters for big data applications Following this line of reasoning we looked at the subject matter from the four angles of the four working groups into which the COST Action is organised Throughout the previous two sections we have presented myriad of application opportunities and technological challenges for modelling and simulation in life and physical sciences These are complemented by comprehensive surveys of the current state of the art with respect to HPC technology and tools both from the perspective of programming models as well as from middleware solutions Bringing together specialists from all these communities is the central bution of the COST Action Having set the scene in this paper the other papers of this volume exemplify the achievements of the COST Action Each addresses speciﬁc application or application scenario from the life or physical sciences and explores how the application of HPC tools and technologies may lead to superior solutions in the near future Acknowledgments This work was supported by the ICT COST Action Modelling and Simulation for Big Data Applications We would like to thank all members of the COST Action for their direct or indirect contributions to this work and the success of the COST Action in general We particularly thank Christoph Kessler University Sweden tore Vitabile University of Palermo Italy Marco Beccuti University of Torino Italy Lalit Garg University of Malta Malta Jing Gong KTH Royal Institute of ogy Stockholm Sweden Aleˇs Zamuda University of Maribor Slovenia Manuel Molina Lopez Universidad Carlos III Madrid Spain Amr Abdullatif Alberto Cabri Francesco Masulli and Stefano Rovetta University of Genova and Vega Research Laboratories Genova Italy for their contributions to this chapter Why HPC Modelling and Simulation for Big Data Applications Matters References Bracciali et PWHATSHAP eﬃcient haplotyping for future generation sequencing BMC Bioinform Misale Ferrero Torquati Aldinucci Sequence alignment tools one parallel pattern to rule them all BioMed Res Int Article ID https Aldinucci Ruggieri Torquati Porting decision tree algorithms to ticore using FastFlow In Bonchi Gionis Sebag M eds ECML PKDD LNCS LNAI vol pp Springer Heidelberg https Buono Danelutto Lametti Torquati Parallel patterns for eral purpose In Euromicro International Conference on Parallel Distributed and Processing pp Aldinucci et Parallel stochastic systems biology in the cloud Brief form Aldinucci Drocco Misale Tremblay Languages for big data ysis In Sakr Zomaya eds Encyclopedia of Big Data Technologies pp Springer Cham https Tordini Aldinucci Viviani Merelli Li Scientiﬁc workﬂows on clouds with heterogeneous and preemptible instances In Proceedings of the International Conference on Parallel Computing ParCo September Bologna Italy Advances in Parallel Computing IOS Press Akhter Othman Energy aware resource allocation of cloud data center review and open issues Cluster Comput Mastelic Oleksiak Claussen Brandic Pierson Vasilakos Cloud computing survey on energy eﬃciency ACM Comput Surv Sikora Arabas Kamola Mincer Ko lodziej Dynamic power management in computer networks and data intensive systems Future Gener Comput Syst Antal et MoSiCS modeling simulation and optimization of complex systems case study on energy eﬃcient datacenters Simul Model Pract Theory https Karpowicz CPU frequency control for the Linux system Concurrency Comput Pract Exp Karpowicz Arabas tilevel control system for network of Linux software routers design and mentation IEEE Syst J Prado Dynamic voltage frequency scaling simulator for real workﬂows management in green cloud computing PLoS ONE Healy Lynn Barrett Morrison Single system image survey J Parallel Distrib Comput Oussous Benjelloun Lahcen Belfkih Big data technologies survey J King Saud Univ Comput Inf Sci Berman Fox Hey Grid Computing Making the Global Infrastructure Reality Wiley Hoboken Sehgal Bhatt Cloud Computing Concepts and Practices Springer Heidelberg https Grelck et MOSIX OpenSSI Kerrighed UNICORE Globus Toolkit Cannataro Handbook of Research on Computational Grid Technologies for Life Sciences Biomedicine and Healthcare IGI Global Hershey Walters Crouch Bennett Building computational grids using uitous web technologies In Xu Afsarmanesh H eds IAICT vol pp Springer Heidelberg https Hwu ed GPU Computing Gems Emerald edn Morgan Kaufman Waltham Singh Bhat Raju D Souza Survey on various load balancing techniques in cloud computing Adv Comput Zhang Yu Wang Huang Liu Liu Load balancing in data center networks survey IEEE Commun Surv Tutor Staples Torque resource manager In Proceedings of the Conference on Supercomputing ACM New York Slurm Workload Manager Mohamed Hong job scheduling algorithms survey In International Conference on Cloud Computing and Big Data CCBD pp White Hadoop The Deﬁnitive Guide Reilly Media Newton Apache Spark Apache Storm Apache Flink RapidMiner Studio Orange Frank Hall Witten Data Mining Practical Machine Learning Tools and Techniques Morgan Kaufmann Burlington Milne et generation sequence assembly visualization formatics Carver Otto Parkhill Berriman BamView viewing mapped read alignment data in the context of the reference sequence matics Rutherford et Artemis sequence visualization and annotation matics Desale Top tools for social network analysis and visualisation www Sikora federated approach to parallel and distributed simulation of complex systems Int Appl Math Comput Sci Marin Wainer approximate parallel DEVS simulation of web search engines Concurrency and Computation Pract Exp Miller Cotterell Buckley Supporting modeling continuum in tion from predictive analytics to simulation modeling In Proceedings of Winter Simulation Conference Simulation Making Decisions in Complex World pp IEEE Press Why HPC Modelling and Simulation for Big Data Applications Matters Sikora software tool for federated tion of wireless sensor networks and mobile ad hoc networks In K ed LNCS vol pp Springer Heidelberg https Song Wu Ma Ciu Gong Military simulation big data ground state of the art and challenges Math Probl Eng https Fidjeland Roesch Shanahan Luk NeMo platform for neural modelling of spiking neurons using GPUS In IEEE tional Conference on Systems Architectures and Processors pp Szynkiewicz novel simulator for large scale spiking neural networks Telecommun Inf Technol Accelerated simulation of P systems on the GPU survey In Pan Song T eds CCIS vol pp Springer Heidelberg https Beyer Hadwiger Pﬁster survey of volume visualization In Proceedings of the Eurographics Conference on Visualization Eurovis pp Maeda et Jade heterogeneous multiprocessor system simulation platform using recorded and statistical application models In Proceedings of HiPEAC Workshop on Advanced Interconnect Solutions and Technologies for Emerging Computing Systems pp Casanova Giersch Legrand Quinson Suter Versatile scalable and accurate simulation of distributed applications and platforms J Parallel Distrib Comput Calheiros Ranjan Beloglazov Rose Buyya CloudSim toolkit for modeling and simulation of cloud computing environments and ation of resource provisioning algorithms Softw Pract Exp Workload Manager Nasir Morales Kourtellis Seraﬁni The power of both choices practical load balancing for distributed stream cessing engines CoRR Zaharia Chowdhury Franklin Shenker Stoica Spark cluster computing with working sets In Proceedings of the USENIX Conference on Hot Topics in Cloud Computing HotCloud USENIX Association Berkeley Dean Ghemawat MapReduce simpliﬁed data processing on large clusters CACM Apache Software Foundation Hadoop http Accessed Intel Threading Building Blocks Accessed Park Voss Kim Eigenmann Parallel programming ment for OpenMP Sci Program Pacheco Parallel Programming with MPI Morgan Kaufmann Publishers San Francisco Khronos Compute Working Group OpenACC Directives for Accelerators http Accessed Grelck et Aldinucci Danelutto Meneghin Torquati Kilpatrick Eﬃcient streaming applications on with FastFlow the biosequence alignment In Advances in Parallel Computing vol Elsevier Amsterdam Aldinucci Danelutto Kilpatrick Torquati FastFlow and eﬃcient streaming on In Pllana Xhafa F eds ming and Computing Systems Parallel and Distributed Computing Wiley New York Enmyren Kessler SkePU skeleton programming library for systems In Proceedings of the Fourth International shop on Parallel Programming and Applications HLPP pp ACM New York Grelck Scholz SAC functional array language for eﬃcient multithreaded execution Int J Parallel Program Grelck Scholz Shafarenko Asynchronous stream processing with net Int J Parallel Program Lee Parks Dataﬂow process networks Proc IEEE Misale Drocco Aldinucci Tremblay comparison of big data frameworks on layered dataﬂow model Parallel Process Lett Aldinucci Danelutto Anardu Torquati Kilpatrick lel patterns macro data ﬂow for programming In Proceedings of International Euromicro PDP Parallel Distributed and cessing pp IEEE Garching Dean Ghemawat MapReduce simpliﬁed data processing on large clusters In Proceedings of USENIX Symposium on Operating Systems Design Implementation pp Cole Algorithmic Skeletons Structured Management of Parallel tions Research Monographs in Parallel and Distributed Computing Pitman don Chu et for machine learning on multicore In Proceedings of the International Conference on Neural Information Processing Systems pp Zaharia et Resilient distributed datasets abstraction for cluster computing In Proceedings of the USENIX Conference on Networked Systems Design and Implementation Carbone Ewen Haridi Tzoumas Lightweight chronous snapshots for distributed dataﬂows CoRR Toshniwal et Storm twitter In Proceedings of the ACM SIGMOD national Conference on Management of Data pp Akidau et The dataﬂow model practical approach to balancing ness latency and cost in unbounded data processing Proc VLDB Endowment Kitchenham Brereton Budgen Turner Bailey Linkman Systematic literature reviews in software engineering systematic literature review Inf Softw Technol Calzone Fages Soliman BIOCHAM an environment for modeling biological systems and formalizing experimental knowledge Bioinformatics Zechner Seelig Rullan Khammash Molecular circuits for dynamic noise ﬁltering Proc Natl Acad Sci Why HPC Modelling and Simulation for Big Data Applications Matters Fages Soliman On robustness computation and optimization in In Proceedings of Computational Methods in Systems Biology International Conference CMSB Brno Czech Republic ber pp Nasti Gori Milazzo Formalizing notion of concentration robustness for biochemical networks In Mazzara Ober G eds STAF Software Technologies Applications and Foundations LNCS vol pp Springer Cham https Proceedings of the International Joint Conference on Biomedical Engineering Systems and Technologies In BIOINFORMATICS in Press Sansom Castiglione Lio Metabolic disorders how can systems elling help Lancet Diabetes Endocrinol Bartocci Li Computational modeling formal analysis and tools for systems biology PLOS Comput Biol Capobianco Li Comorbidity networks beyond disease correlations J Complex Netw Capobianco Li Comorbidity multidimensional approach Trends Mol Med Bartocci Li Merelli Paoletti Multiple veriﬁcation in complex biological systems the bone remodelling case study In Priami Petre Vink eds Transactions on Computational Systems Biology XIV LNCS vol pp Springer Heidelberg https Li Paoletti Moni Atwell Merelli Viceconti Modelling osteomyelitis BMC Bioinform Suppl Paoletti Li Merelli Viceconti Multilevel computational modeling and quantitative analysis of bone remodeling Trans Comput Biol Bioinform Li Merelli Paoletti Viceconti combined process algebraic and stochastic approach to bone remodeling Electron Notes Theor Comput Sci The Second International Workshop on Interactions Between Computer Science and Biology Luisi Ferri Stano Approaches to minimal cells review Naturwissenschaften Kuruma Stano Ueda Luisi synthetic biology approach to the construction of membrane proteins in minimal cells Biochimica et Biophysica Acta BBA Lorenzo Ospri Marangoni On ﬁne stochastic simulations of pure systems Commun Comput Inf Sci Stano Luisi Marangoni Characterization of the emergent properties of synthetic system BMC Bioinform Fanti Gammuto Mavelli Stano Marangoni Do protocells erentially retain macromolecular solutes upon study based on the extrusion of POPC giant vesicles Integr Biol Calviello Stano Mavelli Luisi Marangoni systems stochastic simulation analysis at nanoscale range BMC Bioinform The Genomes Project Consortium global reference for human genetic variation Nature Grelck et The Computational Consortium Computational status promises and challenges Brief Bioinform Huang Popic Batzoglou Short read alignment with populations of genomes Bioinformatics Bille Landau Raman Sadakane Satti Weimann Random access to strings In Annual Symposium on Discrete Algorithms SODA pp Navarro Indexing highly repetitive collections In International shop on Combinatorial Algorithms IWOCA pp Gagie Gawrychowski Puglisi Faster approximate pattern matching in compressed repetitive texts In International Symposium on Algorithms and Computation ISAAC pp Iliopoulos Kundu Pissis Eﬃcient pattern matching in degenerate texts In Drewes Truthe B eds LATA LNCS vol pp Springer Cham https Danecek et The variant call format and VCFtools Bioinformatics Holub Smyth Wang Fast on indeterminate strings Discret Algorithms Grossi et pattern matching on set of similar texts In CPM LIPIcs Bernardini Pisanti Pissis Rosone Pattern matching on degenerate text with errors In Fici Sciortino Venturini R eds SPIRE LNCS vol pp Springer Cham https Pissis Retha Dictionary matching in texts with cations in searching VCF ﬁles In International Symposium on Experimental Algorithms pp Aoyama Nakashima Inenaga Bannai Takeda Faster online elastic degenerate string matching In Annual Symposium on Combinatorial Pattern Matching CPM LIPIcs pp Alzamel et Degenerate string comparison and applications In national Workshop on Algorithms in Bioinformatics WABI LIPIcs pp Patterson et WhatsHap weighted haplotype assembly for generation sequencing reads Comput Biol PMID Aldinucci Bracciali Marschall Patterson Pisanti Torquati haplotype assembly In di Serio Li Nonis Tagliaferri R eds CIBB LNCS vol pp Springer Cham https Vitabile Conti Lanza Cusumano Sorbello Metabolic networks robustness theory simulations and results Interconnection Netw Vitabile Conti Lanza Cusumano Sorbello Topological mation ﬂux balance analysis and extreme pathways extraction for metabolic networks behaviour investigation IOSPress pp Vitello Alongi Conti Vitabile cognitive agent for autonomous urban vehicles routing optimization IEEE Trans Cogn Dev Syst Why HPC Modelling and Simulation for Big Data Applications Matters Conti Ruﬀo Vitabile Barolli BIAM new sis methodology for digital ecosystems based on architecture Soft Comput Beccuti et GPU accelerated analysis of cross regulation in multiple sclerosis In Mencagli et eds LNCS vol pp Springer Cham https Totis et Overcoming the lack of kinetic information in biochemical tions networks ACM SIGMETRICS Perform Eval Rev Bardozzo Tagliaferri study on oscillations in Escherichia coli metabolic networks BMC Bioinform Suravajhala Kogelman Kadarmideen data tion and analysis using systems genomics approaches methods and applications in animal production health and welfare Genet Sel Evol Serra Galdi Tagliaferri Machine learning for bioinformatics and roimaging Wiley Interdisc Rev Data Min Knowl Discov McClean Gillespie Garg Barton Scotney Kullerton Using models to cost stroke patient care across health social and community services Eur Oper Res WHO World Health Statistics Monitoring the SDGs Technical report Garg McClean Barton Can management science methods do more to improve healthcare Jahangirian et rapid review method for extremely large corpora of literature applications to the domains of modelling simulation and management Int Inf Manag Garg Barton Meenan Fullerton Intelligent patient management and resource planning for complex heterogeneous and stochastic healthcare tems IEEE Trans Syst Man Cybern Part Syst Hum Garnett Cousens Hallett Steketee Walker Mathematical models in the evaluation of health programmes Lancet Shanmugam Garg Model employee appraisal system with artiﬁcial ligence capabilities J Cases Inf Technol Aleem Translating lessons from lean six sigma project in training site to electronic health primary care practice challenges and opportunities Qual Manag Health Care Kannan Fish Willett Agile model driven development of tronic health specialty population registries Heidari Gorji Farooquie comparative study of total quality management of health care system in India and Iran BMC Res Notes Garg Dauwels Earnest Leong methods for dling missing data in questionnaires IEEE Biomed Heal Inform McClean Barton Garg Fullerton modeling framework that combines Markov models and simulation for stroke patient care ACM Trans Model Comput Simul Herlihy Luchangco Distributed computing and the multicore revolution SIGACT News Smith Uncertainty Quantiﬁcation Theory Implementation and tions vol SIAM Grelck et John Walker Big Data Revolution that will Transform How We Live Work and Think Houghton Miﬄin Harcourt Boston Guizani Mohammadi Aledhari Ayyash Internet of Things survey on enabling technologies protocols and applications IEEE Commun Surv Tutor Kennedy Post Mine Repeat Social Media Data Mining Becomes Ordinary Springer London https Zhang Melin Gong Barth Axner Mixed ﬁdelity aerodynamic and optimization for wings In International Conference on High Performance Computing and Simulation pp QC Zafari et Task parallel implementation of solver for electromagnetic scattering problems https Viktorin Senkerik Pluhacek Kadavy Zamuda Distance based parameter adaptation for diﬀerential evolution In IEEE Symposium Series on Computational Intelligence SSCI pp IEEE Viktorin Senkerik Pluhacek Kadavy Zamuda Distance based parameter adaptation for based diﬀerential evolution Swarm Evol Comput Zamuda Sosa Success history applied to expert system for underwater glider path planning using diﬀerential evolution Expert Syst Appl Zamuda Zarges Stiglic Hrovat Stability selection using genetic algorithm and logistic linear regression on healthcare records In Proceedings of the Genetic and Evolutionary Computation Conference Companion GECCO pp Zamuda Nicolau Zarges discrete optimization marking pipeline survey taxonomy evaluation and ranking In ceedings of the Genetic and Evolutionary Computation Conference Companion GECCO pp Zamuda combined economic and emission mal optimization by surrogate diﬀerential evolution Appl Energy Zamuda Sosa Adler Constrained diﬀerential evolution tion for underwater glider path planning in eddy sampling Appl Soft Comput Zamuda Brest Environmental framework to visualize emergent artiﬁcial forest ecosystems Inf Sci Zamuda Brest Vectorized procedural models for animated trees struction using diﬀerential evolution Inf Sci Zamuda Mlakar Diﬀerential evolution control parameters study for adaptive triangular brushstrokes Informatica Int Comput Inform Zamuda Brest On tenfold execution time in real world optimization lems with diﬀerential evolution in perspective of algorithm design In International Conference on Systems Signals and Image Processing IWSSIP pp IEEE Zamuda Brest control parameters randomization frequency and propagations in diﬀerential evolution Swarm Evol Comput Gil Berlanga Molina InContexto multisensor ture to obtain people context from smartphones Int Distrib Sens Netw Why HPC Modelling and Simulation for Big Data Applications Matters Vivacqua Molina ambient assisted living to monitor elderly health in outdoor tings IEEE Softw Griol Molina Sanchis Integration of conversational interfaces to develop practical applications for mobile devices Ambient Intell Smart Environ JAISE Marti Molina Bicharra Anomaly detection based on sensor data in petroleum industry applications Sensors Vlahogianni Karlaftis Golias traﬃc forecasting where we are and where we re going Transp Res Part C Emerg Technol Abdullatif Masulli Rovetta Tracking time evolving data streams for traﬃc forecasting Data Sci Eng Open Access This chapter is licensed under the terms of the Creative Commons Attribution International License http which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give appropriate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this chapter are included in the chapter s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the chapter s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder Parallelization of Hierarchical Matrix Algorithms for Electromagnetic Scattering Problems Elisabeth B Afshin Marco Alessandro Giorgio Francesca Giuseppe Christoph Corinne and Clemens Scientiﬁc Computing Department of Information Technology Uppsala University Uppsala Sweden Antenna and EMC Lab LACE LINKS Foundation Turin Italy ASML Netherlands BV Veldhoven Netherlands Department of Electronics and Telecommunications Politecnico di Torino Turin Italy Department of Computer and Information Science University Sweden MINES ParisTech PSL University CRI Paris France Informatics Institute University of Amsterdam Amsterdam Netherlands Abstract Numerical solution methods for electromagnetic scattering problems lead to large systems of equations with millions or even billions of unknown variables The coeﬃcient matrices are dense leading to large computational costs and storage requirements if direct methods are used commonly used technique is to instead form hierarchical tion for the parts of the matrix that corresponds to interactions The overall computational cost and storage requirements can then be reduced to N log N This still corresponds to simulation that requires parallel implementation The hierarchical algorithms are rather complex both regarding data dependencies and communication patterns making parallelization In this chapter we describe two classes of algorithms in some detail we provide survey of existing solutions we show results for implementation and we provide various perspectives on diﬀerent aspects of the problem The list of authors is organized into three subgroups Larsson and Zafari coordination and implementation Righero Francavilla Giordanengo Vipiana and Vecchi deﬁnition of and expertise relating to the application Kessler Ancourt and Grelck perspectives and parallel expertise c The Author s Ko lodziej and Eds cHiPSet LNCS pp https Parallelization of Hierarchical Matrix Algorithms Keywords Electromagnetic scattering Hierarchical matrix Task parallel Fast multipole method Nested equivalent source approximation Introduction In this chapter we consider how eﬃcient solution algorithms for netic scattering problems can be implemented for current and heterogeneous cluster architectures Simulation of electromagnetic ﬁelds is an important industrial problem with several application areas One of the most well known is antenna design for aircraft but electromagnetic behavior is tant also for other types of vehicles for satellites and for medical ment common way to reduce the cost of the numerical simulation is to assume solutions and to reformulate the Maxwell equations describing the electromagnetic waves in terms of surface currents That is the resulting numerical problem is and is solved on the surface of the body being studied see Fig for an example of realistic aircraft surface model Fig Surface currents on an aircraft model from boundary element simulation with around million unknowns The size N of the discretized problem which for boundary element cretization takes the form of system of equations with dense coeﬃcient matrix can still be very large on the order of millions of unknowns going up to billions and this size increases with the wave frequency If an iterative solution method is applied to the full dense matrix the cost for each multiplication is N and direct storage of the matrix also requires ory resources of N Diﬀerent approximate factorizations of the matrices that can reduce the costs to N log N or even N have been proposed in the literature such as the MultiLevel Fast Multipole Algorithm MLFMA see factorization see factorizations based on the Adaptive Cross Approximation ACA see or based on matrices as the Nested Equivalent Source Approximation NESA All these approximations can be seen as decomposing the original dense matrix into sparse matrix accounting for near ﬁeld interactions and archical matrix structure with low storage requirements accounting for far ﬁeld interactions Larsson et large investment in terms of research and development been made in constructing and implementing these rather complex algorithms eﬃciently large part of this eﬀort was made before the advent of multicore architectures Then the focus was much more on minimizing the amount of computations optimizing for powerful server potentially with few processors cores Now clusters are built from regular computers with large numbers of cores and times the additional complexity of accelerators Memory bandwidth is often the limiting factor in this case The question for companies as well as researchers with advanced application codes is how to make these codes run on their own or their customers clusters with the least eﬀort in terms of changes to the code and with the maximum output in terms of utilization of the cluster and computational eﬃciency Some of the properties of the hierarchical matrix algorithms that make allel implementation challenging are ﬁrst the general form of the algorithm with interleaved stages of interactions in the vertical direction between parents and children in tree structure and horizontally between the diﬀerent branches at each level of the tree The stages typically have diﬀerent levels of parallelism and work loads and there is bottleneck when the algorithm reaches the coarsest level of the tree structure and the amount of parallelism is the smallest That is the algorithm itself is generally heterogeneous Furthermore the tree can be unbalanced in diﬀerent ways due to the geometry of the underlying structure and the groups at the ﬁnest level can contain diﬀerent number of unknowns depending on how the scattering surface cuts through space An overview of the challenges inherent in the implementation of the fast multipole method FMM which is one of the algorithms in this class on modern computer architectures can be found in In the following sections we ﬁrst provide description of two types of hierarchical algorithms for electromagnetic scattering Then in Sect we provide survey of literature and software in this area Section discusses two implementations of simpliﬁed algorithm for shared memory Then in Sect diﬀerent perspectives regarding the question of how to eventually port the software to clusters and heterogeneous architecture are given Two Classes of Algorithms and Their Properties In this section we will go deeper into the MLFMA and NESA algorithm classes and describe their properties from the parallelization perspective For the ematical details of the algorithms see for MLFMA and for NESA Interaction Matrices Solving system of equations for the unknown surface currents given measured ﬁeld values can be seen as an inverse problem When using an iterative method we transform the inverse problem to repeated solutions of the forward problem which is easier to address Parallelization of Hierarchical Matrix Algorithms The forward problem consists of computing an electromagnetic ﬁeld given distribution of We will use two examples to make it more concrete well known model problem is to compute the electrostatic potential φ x N K x xj qj generated by the point charges qj located at the points xj The kernel K which is logarithmic in two dimensions and proportional to the inverse distance in three dimensions represents the interaction between the ﬁeld points and the charges The corresponding scattering problem that is of real interest in industry and for research similar structure and consists of computing the electric magnetic ﬁelds generated by surface currents on for example metallic object such as an aircraft We write this in simpliﬁed form as r G r r j r G r r j r dr where r is point in space is the surface of the object and G is Green s function To render the problems tractable for computer simulation they are cretized In the ﬁrst case we already have discrete charges In the second case boundary integral formulation of the problem is used where we represent the ﬁelds and surface currents by set of basis functions vj and corresponding ﬁcients which we denote by Ej and qj j N Henceforth we will refer to qj as sources and to individual basis functions as locations The ﬁelds are evaluated in the same discrete locations as where the sources are located This allows us to express the forward problem as multiplication Zq where is the vector of the ﬁeld variables Z is an N N interaction matrix where element zij describes the contribution from unit source at location j to the ﬁeld at location i and q is the vector of source values The Hierarchical Algorithm The basis for the fast algorithms is that interactions between locations near to each other are stronger then distant interactions In the algorithms interactions are computed directly while interactions are approximated in such way that the required storage and the amount of computations is decreased while still respecting given error tolerance The computational domain is hierarchically divided into groups boxes which can be represented as an in two dimensions with levels max Since the charges are located only on the surface of the Larsson et body many of the groups especially on the ﬁner levels are empty of charges and are pruned from the tree When we construct the hierarchical algorithm we consider interactions between groups We let Ei denote the ﬁeld variables in group i at level max and we let Ej i be the contribution from group j at level max to Ei such that Ei Ej i Using the direct multiplication we have that j Ej i Zi jQj where Zi j is matrix block and Qj is the vector of charges in group j at level max In the hierarchical fast multiplication algorithm only the interactions are computed directly These are here deﬁned as tions between groups that are neighbours at the ﬁnest level The actions are instead approximated At each level of the tree structure starting from level the groups are identiﬁed as those that are not neighbours to the target group As much of the interaction as possible is treated at each level to minimize the total number of groups to interact with In Fig we show the layout of the near and for computing the ﬁeld at one location the black box in geometry Fig Illustration of domain that is hierarchically divided into three levels of boxes Charges are located on the wavy curve For the black target box the consists of the six dark gray neighbouring boxes The at each level consists of the four large ﬁve medium and ﬁve small light gray boxes that are not neighbours of the target box The approximation for one interaction the following general form Ej i Zi jQj Ri P max P descending i j P T P max ascending SjQj and can be described in terms of the ﬁve steps described in Algorithm Parallelization of Hierarchical Matrix Algorithms Radiation SjQj Source transfer The sources Qj are converted into an intermediate representation X max j The intermediate representation is propagated up through the parent groups X j until level where the interaction takes place j P X Translation The interaction between groups i and j is computed at level The result is an intermediate ﬁeld representation i T i jX j Field transfer The intermediate ﬁeld representation is propagated down through the child groups i until the ﬁnest level is reached P i Reception The intermediate ﬁeld representation is evaluated at the actual ﬁeld locations Ej i RiY max i Algorithm The algorithm for computing one interaction term If we change the view and instead see the algorithm from the perspective of one particular group at level that takes part in several interactions the work related to that group can be expressed as Algorithm Upward Phase if max then j Compute X max SjQj for local sources else Receive source contributions from all child groups end if then Send accumulated source contribution to parent end Translation Phase Compute i T Downward Phase if then i jX j according to interaction list Receive ﬁeld contribution from parent end if max then Send ﬁeld contribution to all child groups else Compute Ej RjY max j for local target points end Algorithm The algorithm seen from the view of one particular group Larsson et For the parallel implementation there are several relevant aspects to keep in mind For the upward and downward phases communication is performed tically in the tree between parent and child groups The translation operations on the other hand need horizontal communication Due to the hierarchical ture each group an interaction list of limited size The three phases of the algorithm can be overlapped since diﬀerent groups complete the phases at ferent times Even more important is that the interactions for disjunct groups are independent and can be interspersed with the computations The memory savings that the fast algorithms provide stem from the fact that the part of the interaction matrix is replaced with the operators in These are the same for groups that have the same position relative to each other That is only limited number of operators are needed at each level Speciﬁc Properties of the NESA Algorithm In the NESA algorithm all of the operations consist in expressing sources and ﬁeld in terms of equivalent charges The actual sources in group at level max can through low rank approximation be represented by set of equivalent sources that generate matching ﬁeld at some control points located at an exterior test surface In the same way the equivalent sources in child group can be represented by another set of equivalent sources at the parent group This is schematically shown for problem in Fig The number of equivalent charges Q is the same in each group which is why we can save signiﬁcantly in the computation The translation and ﬁeld fers are managed similarly We will not go into all details here instead we refer to Fig parent group and one of its children are illustrated The points on the solid circles are where the equivalent sources are located and the points on the dashed circle are where the ﬁelds are matched To understand the computational properties of the NESA algorithm we acterize each operation in terms of how much memory it needs to load counted in double precision numbers and how many ﬂoating point operations ﬂop are formed We also provide the computational intensity in The results Parallelization of Hierarchical Matrix Algorithms Table Characterization of the products in the NESA algorithm The number of sources in group j is denoted by nj Operator Data size Compute size Intensity Near ﬁeld Zi j Far ﬁeld Sj P T i j P Ri nj Q nj Q Q Q Q Q Q Q are listed in Table All of the operations in the NESA algorithm are dense products with the same computational intensity of ble For modern multicore architectures computational intensity of is needed in order to balance bandwidth capacity and ﬂoating point performance see for example the for the Tintin and Rackham systems at UPPMAX Uppsala University calculated in This means that we need to exploit data locality work on data that is cached locally in order to overcome bandwidth limitations and scale to the full number of available cores Speciﬁc Properties of the MLFMA Algorithm In the MLFMA algorithm the intermediate representation of sources and ﬁelds is given in terms of plane wave directions ˆκ θ φ where θ is the polar angle and φ is the azimuthal angle in spherical coordinate system When computing interactions the Green s function can be represented using an integral over the directions which numerically is done through quadrature method The accuracy of the approximation depends on the number of directions that are used diﬀerence compared with the NESA method is that the number of directions that are needed scale with the box size Table An example of the number of directions N needed at each level in the MLFMA algorithm starting from the ﬁnest level max max L N To compute the number of directions needed for box at level we ﬁrst compute the parameter L from the wave number of the electromagnetic wave Larsson et Fig unit sphere discretized for L with points in θ latitudes and points in φ longitudes the diagonal d of the box and the desired error tolerance τ Sect Then the box is discretized with L points in the and points in Using realistic the giving total number of N tolerance τ and an appropriate box size for the ﬁnest level leads to the sequence of sizes given in Table Figure shows the discretized sphere for L The wide range of sizes for the representations at diﬀerent levels does pose challenge for parallel implementations The interpolation step between parent and child or vice versa can be ized in diﬀerent ways Here we consider the Lagrange interpolation method described in Then the value at one point at the new level is computed using the m nearest neighbours in each coordinate direction The operations of one interpolation step are shown schematically in Fig Fig To interpolate the data middle from child L to parent L sparse interpolation matrix left right is applied to each of the data dimensions The matrix sizes are here and The data matrix is extended with columns to each side to manage the periodicity at the poles Similarly as for the NESA algorithm in the previous subsection we terize the work performed during the algorithm and evaluate its computational Parallelization of Hierarchical Matrix Algorithms Table Characterization of the steps in the MLFMA algorithm The number of sources in group j is denoted by nj Operator Data size Compute size Intensity Near ﬁeld Zi j Far ﬁeld Sj P T i j P Ri nj N nj L L L L L N L L L L L L L L intensity The results are given in Table The radiation and reception steps are products also in this case The interpolation steps have higher computational intensity For m and the child to parent operation we get while for the parent to child operations we get The translation step is often bottleneck in parallel implementation It is an elementwise multiplication with an intensity less than one State of the Art There is rich literature on parallel implementation of hierarchical matrix rithms Many of the implementations are aimed at volume formulations are located in volume as opposed to surface formulations as for the scattering problem The volume formulation is more likely to have large number of particles in group and more tree structure The most common parallelization approach targeting distributed memory systems is to partition the tree data structure over the computational nodes and use an parallelization The resulting performance is typically bit better for volume formulations then for boundary formulations since the computational density is higher in the former case particular issue for the MLFMA formulation of electromagnetic scattering problems is that the work per element group in the tree data structure increases with the level and additional partitioning strategies are needed for the coarser part of the structure The ongoing trend in cluster hardware is an increasing number of cores per computational node When scaling to large numbers of cores it is hard to fully exploit the computational resources using pure MPI implementation due to the rapid increase in the number of messages with the number of MPI processes for communication heavy algorithms As is pointed out in hybrid parallelization with MPI at the distributed level and threads within the computational nodes is more likely to perform well That is need for Larsson et eﬃcient shared memory parallelizations of hierarchical algorithms to be used in combination with the distributed MPI level arises The emerging method of choice for implementing complex algorithms on ticore architectures is parallel programming which is available through the StarPU OmpSs and SuperGlue works but also in OpenMP since version Starting with where StarPU is used for task parallel implementation of an FMM algorithm several authors have taken an interest in the problem In SuperGlue is used for multicore implementation of an adaptive FMM The Quark tem is used for developing an FMM solver in Since tasks were introduced in OpenMP recurring question is if the OpenMP implementations can reach the same performance as the speciﬁc discussed above An early OpenMP task FMM implementation is found in This was before the depend clause was introduced allowing dependencies between sibling tasks OpenMP Cilk and other models are compared for FMM in OpenMP and are compared in and diﬀerent OpenMP implementations and task parallel times are compared with special focus on locking and synchronization in common conclusion from these comparisons is that the commutative clause vided by most task parallel systems is quite important for performance and that this would be useful upgrade of OpenMP tasks for the future An alternative track is to develop special purpose software components for the class of algorithms see PetFMM and Tapas An open source implementation of MLFMA is available through the EM software parallelized with MPI An example of commercial MLFMA software is Eﬁeld provided by ESI Group parallelized for shared memory Proposed Solution and Proof of Concept During the last decade programming emerged as the main gramming paradigm to run scientiﬁc applications on modern and heterogeneous computer architectures recent and fairly complete overview of the current state of the art can be found in The key idea is that the programmer provides the sequential of an algorithm in terms of tasks These are then submitted to system which analyses the data dependencies of the tasks and schedules them onto available hardware resources to be executed in parallel It can in some cases be possible to obtain higher performance by code but the cost in programming eﬀort and the renewed cost if the system conﬁguration changes are usually considered too high There are several arguments for using task parallel programming for the hierarchical products considered here The work ﬂow is already described in terms of tasks operating on data ciated with the individual groups Therefore the overhead of converting the algorithm into suitable form can be largely avoided Parallelization of Hierarchical Matrix Algorithms The data size varies between groups the number of child groups and tions across the tree structure The amount of work varies with the level and the phase of the algorithm All of this indicates that the asynchronous task execution provided by system is more likely to be eﬃcient than statically determined schedule The dependencies between tasks are complex problem dependent and hard to analyze manually With the scheduling dependencies are matically managed and tasks can run as soon as their dependencies have been met Furthermore the guarantees correctness in the sense that if the sequential task ﬂow of the application code is described correctly the parallel execution guarantees to respect the order up to admissible interleavings As proof of concept we have implemented the NESA algorithm for the electrostatic potential problem using the SuperGlue framework detailed description of the implementation details and the results can be found in beneﬁt of using the NESA algorithm is that the tasks are more similar both in size and type than for the MLFMA algorithm The main arguments for ing the SuperGlue framework are i that it very low scheduling overhead and can therefore handle small task sizes well and ii that commutative data accesses are naturally included in the dependency management based on versioning Commutative accesses relate to tasks that touch the same data and that can therefore not run concurrently but that can otherwise run in any order We have also implemented the NESA algorithm using OpenMP tasks and provide some results and comments on how the two implementations compare In the following subsections we provide brief survey of task parallel gramming frameworks we discuss the SuperGlue and OpenMP implementations and we provide some illustrative performance results Programming Overview One of the key features of task parallel programming is that it makes it relatively easy for the programmer to produce parallel application code that performs well However it is still important for the programmer to understand how to write task parallel program and how various aspects of the algorithm are likely to impact performance The granularity of the tasks determines the number of tasks which direct eﬀect on the potential parallelism As an application programmer it is beneﬁcial to be aware of how tasks interact with each other and with the data That is to understand the character of the data dependencies There may be diﬀerent ways of splitting the work that lead to diﬀerent degrees of parallelism In the NESA and MLFMA cases basic task size is given by the algorithm through the division of the domain into groups The discussion to have is whether some groups need splitting the coarse levels in MLFMA or merging the leaf groups The granularity of tasks also an eﬀect on how the tasks interact with the memory hierarchy If the tasks are small enough data may ﬁt into the cache If the system is such that tasks are scheduled at the Larsson et cores where the data they need is cached signiﬁcant performance gains may be secured As was discussed in Sects and the computational intensity provided by the tasks of the NESA and MLFMA algorithms is not enough to scale well if all of the data is read from the main memory In is investigated It is shown that the eﬀect of bandwidth contention between tasks can be reduced by scheduling mix of diverse tasks However in the NESA case all of the tasks have similar computational intensity so that approach is not applicable From the user perspective it would be ideal if there were only one work for task parallelism or at least one common standard for task parallel programming implemented by diﬀerent frameworks Steps are being taken in this direction see also the implementation in but it will take some time until it is in place Meanwhile we provide an overview of some of the more relevant initiatives The StarPU framework was initially developed to manage scheduling between the CPU and GPU resources in one computational node It over time been developed in diﬀerent ways and become one of the most widely adopted general purpose systems In StarPU an important component is the management of data transfers and data prefetching Advanced performance prediction based on performance measurements is used in the diﬀerent scheduling algorithms StarPU very good performance for large scale problems with relatively large task sizes When task sizes become too small the overhead of the advanced scheduling is too large and performance goes down Another important system is OmpSs which is the current resentative of the StarSs family In OmpSs the tasks are deﬁned through compiler directives in the same way as in OpenMP In fact the development of the OpenMP standard in terms of tasks and task dependencies is driven by the development of OmpSs In this way the constructs and implementations are well tested before being adopted by the standard The use of directives can be seen as less intrusive when transforming legacy code into task parallel code compared with the use of speciﬁc APIs for task submission LAPACK which implements large selection of linear algebra tions is one of the most widely used libraries in scientiﬁc computing With the advent of multicore architectures number of projects were started to provide multicore and GPU support These have now converged into using the PaRSEC system which excellent performance both for large and small task sizes PaRSEC can be used for all types of algorithms but it requires the task dependencies to be expressed in speciﬁc data ﬂow language This allows to build parametrized task graph that can be used eﬃciently by the system but it can be an obstacle for the application programmer The SuperGlue framework was developed mainly for research purposes with focus on performance It is task parallel framework for multicore architectures It is very lightweight it uses an eﬃcient representation of dependencies through data versions and very low overhead such that comparatively small tasks can be used without loosing performance Parallelization of Hierarchical Matrix Algorithms Tasks with dependencies were introduced in OpenMP The dependencies are only between sibling tasks submitted in the same parallel region and there is not yet support for commutative tasks which are relevant for the NESA and MLFMA types of algorithms The main reason for using OpenMP is that it is standard and is likely to remain making it relatively secure investment in coding The SuperGlue Task Parallel Implementation SuperGlue is implemented in as headers only library In order to write version of the NESA algorithm for SuperGlue we need to deﬁne SuperGlue task class for the product that is the computational kernel used in all the tasks In Program we show slightly simpliﬁed code that emphasizes the most relevant parts The task class contains the data that is touched by the task constructor and run method In the constructor the types of data accesses are registered In this case it is commutative add access to the output vector The read accesses to the input data are not registered as that data is not modiﬁed during execution The access information is used for tracking dependencies and extra dependencies increase the overhead cost The constructor is called at task submission while the run method is called at task execution time class SGTaskGemv public Task Options private SGMatrix x public SGTaskGemv SGMatrix SGMatrix SGMatrix x Handle Options hy ReadWriteAdd add hy void run double Mat x double X double Mat X Program The MVP task class Larsson et In the application code all former calls to the product routine should be replaced by the corresponding task submission If we hide the task submission statement in subroutine the syntax of the application code does not need to change at all The new subroutine that replaces the original product by the task submission is provided as Program void gemv SGMatrix SGMatrix x SGMatrix SGTaskGemv t new SGTaskGemv x sgEngine submit t Program The subroutine that submits an MVP task There are also other small changes such as starting up the SuperGlue time and the SGMatrix data type which equips the ordinary matrix type with the data handle that is used when registering accesses longer description of the implementation can be found in and the full implementation is available at The OpenMP Implementation An implementation with similar functionality as the tion described above some created with OpenMP as well simple task construct was introduced in OpenMP and depend clause was added in OpenMP to allow dependencies between sibling tasks tasks created within the same parallel region This means that if we create several allel regions for diﬀerent parts of the algorithm there will eﬀectively be barriers in between and the tasks from diﬀerent regions can not mix pragma omp parallel pragma omp single Submit tasks for near field multiplication FMM OT C Q Submit tasks for far field multiplication FMM OT C Q pragma omp taskwait pragma omp barrier Program The structure of the OpenMP implementation There is one global parallel region lines and within this region only one thread can submit tasks lines https Parallelization of Hierarchical Matrix Algorithms The proper way to do it is to create one parallel region that covers the whole computation and then make sure that only one thread generates tasks such that the sequential order is not compromised An excerpt from the OpenMP main gram that illustrates this is shown in Program The tasks are implicitly mitted from the and subroutines whenever the cblas dgemv subroutine is invoked The tasks are deﬁned using the task pragma with the depend clause see Program Only the necessary inout dependence for the output data vector is included Adding the nonessential read dependencies on the matrix and input data vector was shown in the experiments to degrade performance pragma omp task depend inout N Mat X Program The OpenMP task pragma that deﬁnes gemv task As can be seen the implementation is not so diﬃcult but there are several ways to make mistakes that lead to suboptimal performance The programmer needs to understand how the task generation the task scheduling and the allel regions interact Performance Results In this section we summarize the experimental results from and relate these to the arguments we gave for using parallel implementation The ease of implementation was discussed in the previous two subsections The next two arguments concerned the beneﬁts of asynchronous task execution dynamic and automatic scheduling and mixing of computational phases Execution traces for the SuperGlue implementation when running on one shared memory node of the Tintin cluster at the Uppsala Multidisciplinary ter for Advanced Computational Science UPPMAX are shown in Fig The simulation parameters P and Q are the average number of sources in one group at the ﬁnest level the average of nj and the number of auxiliary sources in each group respectively The trace top nicely illustrates how tasks of diﬀerent sizes are scheduled asynchronously onto worker threads with ible idle time between the tasks The trace furthermore illustrates that the diﬀerent computational phases can be interleaved to large extent using schedule that it would be diﬃcult to construct statically Finally the last trace shows that the tasks can be embedded in the computation As will be discussed below this is beneﬁcial since the tasks have lower computational intensity and in this case are also smaller The idle time that can be seen in the beginning for thread in the middle and bottom panels is the time for task submission Another question that was investigated using the proof of concept tation was how the task size impacts scalability and how small tasks can be used without loosing performance The same problem with N source points is solved in all experiments but the method parameters P the average number Larsson et Fig Execution traces for the near ﬁeld computation top the far ﬁeld computation middle and the combined execution bottom for Q and P Each task is shown as triangle and the color indicates which phase of the algorithm it belongs to dark gray radiation medium gray source transfer light gray translation black ﬁeld transfer light gray and reception medium gray of source points at the ﬁnest level and Q the number of auxiliary points used for each group are varied between the experiments We compute the speedup Sp using p cores as Sp Each node of the Tintin cluster consists of two AMD Opteron Bulldozer processors peculiarity of the Bulldozer architecture is that each ﬂoating point unit FPU is shared between two cores This means that the theoretical speedup when using threads cores is only p and the highest theoretical speedup on one node with threads is Figure shows the results for diﬀerent task sizes The computation scales relatively well for all of the task sizes but the performance improves with size P For the there is scalability when both P and Q are small The situation improves when the sizes increase but the scalability is signiﬁcantly worse than for the For the combined computation the results are Parallelization of Hierarchical Matrix Algorithms better than the results with the same sizes and for the larger tasks even better than the results That is the mixing of the two phases allows the limited scalability of the computation to be hidden behind the better performance of the computations We can however conclude that Q and P which are reasonable numbers for the case results in tasks that are too small for scalability Using Q which is suitable for the problem is however enough for shared memory scalability This is an indication that the proof of concept approach can be used for the real problem Fig Speedup for the computation left the computation dle for P solid lines and for P dashed lines and the combined tation right In Table we compare the execution times the speedup and the utilization for execution with small tasks and with larger tasks The utilization is deﬁned as the fraction of the total execution time that is spent in executing tasks That Table The parallel execution time Tp the speedup Sp the speedup in relation to the theoretical speedup p and the utilization Up computed as the fraction of time spent executing tasks for two problem settings Tp ms Sp p Up p Q P Q P Larsson et is the lack of utilization reveals overhead idle time and load imbalance For the problem with larger tasks both utilization and speed are close to optimal For the problem with small tasks the utilization goes down to for threads Then one might expect that the execution time leading to speedup but this is not at all the case Figure shows the slowdown of individual task execution as function of the number of threads factor of is expected for threads due to the Bulldozer architecture This is also the case for the larger tasks For the smaller tasks the computations exhibit slowdown of which limits the potential scalability to maximum at threads The computational intensity does not change with the task size but potential explanation can be found when looking at the scheduling in the system For large enough tasks the system time to use the knowledge of which data is needed by task to place it in the work queue of the thread where that data is cached thereby ensuring data locality However for too small task sizes task execution becomes faster than task submission and the opportunity to ﬁnd the next task in time is lost Then the threads try to steal work from each other This results in contention on the work queues as well as loss of data locality n w d w S l Q x Q P x Q P x P Threads n w d w S l Q x Q P x Q P x P Threads Fig Increase in individual task execution times for the complete execution for P Q left and for P Q right The ﬁnal question we ask in this section is whether OpenMP is eﬃcient enough to use for this problem We already mentioned the fact that OpenMP currently does not support commutative tasks The performance of the OpenMP implementations increased over time and will most likely continue to do so The experiments were carried out both on node of the Tintin cluster described in the previous section and on local shared memory system with sockets of Intel Xeon Sandy Bridge processors yielding total of cores On Tintin the codes were compiled with gcc version and OpenMP while on Sandy Bridge the compiler was gcc combined with OpenMP Parallelization of Hierarchical Matrix Algorithms We compare the execution times using SuperGlue SG and using OpenMP OMP for the full execution and for two diﬀerent task sizes The results given in Tables and show that OpenMP is slower for small task sizes and especially when small sizes are combined with large numbers of threads However for the larger problem sizes the diﬀerences are small and the results vary between the two hardware systems We do not see the eﬀect of the missing commutative clause As long as tasks are large enough These results indicate that OpenMP can be used for this type of problem Table Execution times in ms for the SuperGlue SG and OpenMP OMP mentations executed on Tintin node p P Q P Q SG OMP SG OMP Table Execution times in ms for the SuperGlue SG and OpenMP OMP mentations executed on the Sandy Bridge system p P Q P Q SG OMP SG OMP Perspectives and Future Directions Our implementation demonstrates that task parallel mentation provides the expected beneﬁts As long as the task granularity is not too small relative to the overhead of the system the proposed solution performs well Thus we can recommend this general direction of parallelization but there are many further aspects to consider we discuss some of them in the following subsections Larsson et Recommendations for Task Parallel Implementation When performing large scale simulations it becomes sary to use distributed computer systems and hence distributed parallel gramming or partitioned global address space PGAS model In it was shown that hierarchical programming model was beneﬁcial for the distributed implementation Larger tasks are communicated between tional nodes and then split into subtasks that are executed in parallel within each node For the upward and downward phases it seems natural to let larger task resent operations within subtree For the translation phase it is less clear what the best type of task is Perhaps translations between subtrees can be performed as one larger task but this reduces the opportunities to interleave the translation stage with the other stages The partitioning of the global tree structure into subtrees would be formed at level where the number of groups is at least equal to the number of computational nodes Then the question is how to share the work and the data for the levels above the splitting point For the NESA algorithm this is not such big problem as the amount of work at these lower levels is small ever for the MLFMA algorithm the work increases signiﬁcantly for the lower levels as can be seen in Table In this case the work for these levels needs to be divided between the computational nodes while the data could potentially be shared by all drawback of such an approach could be that this kind of splitting becomes more intrusive from the programming perspective than just making each subroutine call into one task As we saw in the implementation small task sizes can also become problem but from the programming perspective we do not want to explicitly merge work into larger tasks In preliminary implementation which is not yet ﬁnished we performed experiments with batching of small tasks When tasks are submitted to the system they are saved in buﬀer until there is enough work to actually start batched task which then executes all of them at once The question of which system or programming model to use is diﬃcult one Especially for company it is important to know what kind of support of programming model can be expected and whether permissions and licenses for using it remain stable This would be an argument for using OpenMP for the shared memory part For distributed programming however there is similarly established standard as of yet The choice is then to either develop custom which is unlikely to be as good as the already existing ones or to trust an existing one which may at some point longer be supported Automatically Mapping Workloads to Accelerators Applications that perform regular computations on large number of data are often good candidates for eﬃcient execution on accelerators such as GPUs ever mapping some parts of the applications onto GPU is not easy especially Parallelization of Hierarchical Matrix Algorithms when the application is written in Indeed in references to array elements or certain values such as loop bounds can be hidden in function calls Automatic tools that detect data dependencies statically and generate lel code and GPU kernels need this information explicitly Otherwise dynamic analysis and code instrumentation are required Initially the mapping consists in detecting loops that meet the criteria of the accelerator These criteria express the adequacy between the loop nest patterns and the target accelerator hierarchy external parallel loops will be mapped directly to streaming cores and sequential internal loops in threads The loop nest sizes must be large enough to compensate for communication time and less than the number of possible accelerator threads Finally an estimation of the kernel memory footprint is required to ﬁt the overall memory of the GPU If we take into account only the pieces of application that naturally respect these constraints we miss many pieces of code that can beneﬁt from tion Gouin presents methodology to increase the number of application pieces that can beneﬁt from accelerator optimization and describes all necessary ping stages The actual programming of GPU kernels preferably speciﬁed within the same source ﬁle as the calling CPU code and of the necessary device memory management and data transfers GPU device memory can be made ier for the programmer by adopting parallel programming model supporting GPU execution For example OpenACC allows to write kernels by annotating sequential code in style similar to OpenMP parallel loop annotations The OpenMP task model supports code generation for GPU tion of tasks since OpenMP with the introduction of the target directive for oﬄoading computations to accelerator devices SYCL https is programming layer atop OpenCL that provides source abstraction for OpenCL based accelerator programming For improved programmability runtime systems for heterogeneous programming such as StarPU can also be coupled with programming abstraction layers such as SkePU which from constructs such as skeleton function calls automatically generate the API calls for the management of tasks data buﬀers and their dependencies by the runtime system Optimizing the Task Sizes In the application the basic task size is given by the algorithm through the division of the domain into groups As the tiling transformation makes it possible to optimize task granularity at the loop level adjusting task and group sizes can improve data locality improve cache reuse and reduce communication overhead The new decomposition must be performed in order to balance tions and communications Considering the OpenMP implementation and large Larsson et number of small tasks merging could also reduce the global thread creation heads and thread scheduling Task and group sizes are spaces and the optimal position parameters depend on the target architecture constraints memory size number of cores Finding these optimal parameters is complex since they are dynamic variables Autotuner techniques combining proﬁling information might be used to develop heuristics and to limit the maximum task sizes at each level of the application Limiting Recursive Task Creation on CPU computations over recursively deﬁned sparse hierarchical domains such as could if applicable for the underlying computational problem choose to stop the recursive subdivision at certain depth limit and for example switch to computations over dense representations below this limit or sequentialize the independent subcomputations instead of creating smaller task for each of them For example OpenMP provides the if clause to the task construct for conditional task creation Such as well as the degree of task unrolling in general can be used as tuning eter to balance the between computational work to perform degree of data parallelism in tasks and tasking and synchronization overhead For ple Thoman et describe combined compiler and runtime approach for adaptive granularity control in recursive CPU programs Techniques for Task and Data Granularity Adaptation on GPU The task granularity in dynamically scheduled computations on heterogeneous system can have major impact on overall performance Each task executing on an accelerator typically contains just one kernel call or bly several kernel calls that execute in sequence on the same accelerator unit For the application considered in this chapter of size turn out to be too for GPU execution in practice as most of the GPU s putation capacity remains unused and the task management overhead which is for StarPU in the order of several dozen microseconds becomes large in relation to the task s work number of programming environments allow to control task granularity already at task creation in particular for tasks by ducing conditions for recursive task creation as described in Sect Moreover number of static and dynamic techniques exist for adapting task granularity in GPU execution context In the remainder of this section we review number of such granularity adaptation techniques speciﬁcally for GPU task execution which could be leveraged in future extensions of this work Overpartitioning of computation into more than one nel call leads to ﬁner granularity which can enable automated hybrid GPU computing but also incurs increased runtime overhead for the ment of the additional tasks Parallelization of Hierarchical Matrix Algorithms Kernel fusion is an optimization for accelerator computations that tries to merge calls into fewer ones Persistent kernels on GPU are applicable to scenarios with many subsequent kernel calls of one or few statically known types and can signiﬁcantly reduce the accumulated kernel latencies for small GPU tasks Operand transfer fusion is granularity coarsening optimization for the munication of kernel operand data between main memory and accelerator memory Overpartitioning computations can be generated from parallel programming models As an example we could consider the skeleton programming framework SkePU for based systems SkePU provides for each supported skeleton map reduce stencil etc multiple implementations for threaded CPU execution multithreaded CPU execution using OpenMP and GPU execution in CUDA or OpenCL Moreover SkePU also provides that generates tasks for the StarPU runtime system From single skeleton call number of asynchronously executed tasks can be generated by partitioning the work and thus converting some of the skeleton call s data parallelism into task parallelism Such overpartitioning automatically exploits hybrid computing via StarPU s dynamic heterogeneous task uler at the expense of increased runtime overhead for the management of the additional tasks Kernel Fusion Kernel fusion is an agglomeration optimization for accelerator computations that merges multiple kernels resp kernel calls into single one The purpose of this coarsening of the granularity of accelerator usage is to either improve data locality or to reduce kernel startup overhead or to improve the overall throughput by combining with nels Kernel fusion is special case of the classical loop fusion transformation namely for the case of parallel loops executing on an accelerator with many parallel hardware threads such as GPU Kernel fusion can be done in two diﬀerent ways parallel fusion by scheduling of independent kernels or serial fusion by serialization of possibly dependent kernels see also Fig for illustration Serial fusion is particularly eﬀective if it can internalize ﬂow of bulk operand data intermediate vectors or between ducer and consumer kernels and moves the time points of production and sumption of each such data element much closer to each other Hence these data elements can now be stored and reused in registers or fast memory which reduces the amount of slow memory accesses and thus increases the arithmetic intensity of the code In contrast parallel fusion does not change the arithmetic intensity of the code but eliminates kernel startup time overhead improves thread occupancy and thus utilization of the accelerator especially for kernels with relatively small operands Moreover it can lead to overall improved throughput by Larsson et Fig Left Serial kernel fusion by sequencing code from calls to diﬀerent kernels in the same parallel loop preserving data ﬂow dependencies between nels in the fused Parallel kernel fusion by two previously independent kernel executions within the same superkernel Adapted from with kernels For GPUs parallel fusion can be done at the granularity of individual threads or of thread blocks the latter of which should give better performance number of static kernel fusion techniques especially for compilers targeting GPUs have been presented in the literature by Wang et Wahib and Maruyama and Filipovic et Filipovic and Benkner evaluate the eﬀectiveness of parallel kernel fusion on GPU Xeon Phi and CPU Wen et apply parallel kernel fusion in compiler that tries to pair bound with kernels Qiao et study serial kernel fusion for image processing DSLs Persistent Kernel For scenarios with many small tasks that all execute the same or just few diﬀerent statically known code using persistent kernel is another technique to reduce the GPU kernel overhead time which is for current CUDA GPUs in the order of several microseconds thus signiﬁcant for small tasks In contrast to starting new kernel execution for each GPU task that is supplied with all its input data at its start and that delivers all output data on exit persistent kernel is started just once in the beginning and continuously runs on the GPU until it is eventually terminated by the CPU When idle the persistent kernel performs busy waiting on its input data buﬀers until it ﬁnds new data to work on after it was written transferred there by the CPU It then performs the corresponding operation and writes the data to the corresponding output buﬀer The CPU can ﬁnally terminate the kernel by writing special poison pill value into an input ﬁeld that the GPU kernel polls regularly during busy waiting For example Maghazeh et describe how the technique was used in packet processing application in telecommunications Parallelization of Hierarchical Matrix Algorithms Operand Transfer Fusion Heterogeneous systems that expose physically distributed memory architecture to the programmer require the explicit memory allocation and transfer of not yet uploaded kernel operand data from main memory to accelerator memory before kernel execution and the transfer of the kernel s output operands back to main memory or possibly to other accelerator memories if needed there for subsequent computations Accelerator APIs provide functions for memory allocation and transfer of operands such as cudaMalloc and cudaMemcpy respectively The data transfer time for bulk operand vector or of N elements can generally be modeled by linear cost function tcomm α βN which is characterized by the transfer startup time α and the word transfer time β On PCIe GPUs the startup time α can be in an order of about with ﬂoats For tasks with small operands the transfer startup time is thus overhead Likewise there is signiﬁcant overhead for device memory allocation where required key observation is that multiple operands that can be stored adjacently in both main memory and accelerator memory can be transferred in single larger message thus saving transfer startups compared to separate transfers for each operand Likewise device memory can be allocated for such operands by single call to cudaMalloc Li and Kessler present dynamic optimization based on lazy allocation They replace the standard API functions for lazy execution operand memory allocation and operand transfer by variants that defer their eﬀect until kernel call execution time At the kernel call the operands and their availability in accelerator memory hence the need for allocation and transfer are deﬁnitely known even in cases where static analysis could not resolve this information due to variable aliasing or statically unknown task mapping Then operands to be transferred together will be allocated consecutively in memory if possible This greedy optimization applies to one kernel call at time Coordination common characteristic of the programming frameworks discussed so far is that they despite all abstractions from concrete hardware do require considerable expertise in parallel programming to get things right and even more such expertise to get things eﬃcient One reason is that they intertwine two diﬀerent aspects of program execution algorithmic behaviour what is to be computed and organization of execution how tation is performed on multiple execution units including the necessary problem decomposition communication and synchronization requirements The aim of coordination programming is precisely to separate centric code from code The term goes back to the inal work of Gelernter and Carriero but seen many variations since For example is declarative coordination language whose design thoroughly avoids the intertwining of computational and organizational aspects Larsson et achieves near complete separation of the concern of writing sequential application building blocks application engineering from the concern of composing these building blocks to form parallel application concurrency engineering deﬁnes the coordination behaviour of networks of asynchronous less components and their orderly interconnection via typed streams We erately restrict to coordination aspects and leave the speciﬁcation of the concrete operational behaviour of basic components named boxes to tional programming languages An box is connected to the outside world by two typed streams single input stream and single output stream The operational behaviour of box is characterized by stream transformer function that maps single data item from the input stream to possibly empty stream of data items on the output stream eﬀectively promotes functions implemented in standard programming language into asynchronously executed nents In order to facilitate dynamic reconﬁguration of networks box nal state and any access to external state ﬁle system environment ables etc is conﬁned to using the streaming network This allows us to cheaply migrate boxes between computing resources and even having individual boxes process multiple data items concurrently Boxes execute fully asynchronously as soon as data is available on the input stream box may start computing and producing data on the output stream Boxes usually represent units of computation instead of basic operations as in the original approach Hence eﬀectively implements macro data ﬂow model It is distinguishing feature of that it neither introduces streams as explicit objects nor that it deﬁnes network connectivity through explicit wiring Instead it uses algebraic formulae to describe streaming networks The tion of boxes to single input and single output stream SISO is essential for this provides ﬁve network combinators serial and parallel composition serial and parallel replication as well as feedback Any combinator preserves the SISO property any network regardless of its complexity again is SISO entity To summarize is an abstract notation to express concurrency in cation programs in an abstract and intuitive way It avoids the typical ances of concurrent programming Instead borrows the idea of streaming networks of asynchronous stateless components which gates applications into their natural building blocks and exposes the data ﬂow between them We have developed highly tuned system customized to the speciﬁc needs of In addition we have developed Distributed for cluster architectures is not at all conﬁned to classical streaming applications as we have demonstrated through number of numerical application case studies We have not yet implemented any of the methods for electromagnetic scattering problems described earlier in this paper and unfortunately for the time being we lack the resources to do so However the closest matching algorithm we do Parallelization of Hierarchical Matrix Algorithms have implemented with is Tiled Cholesky Factorization another chical matrix algorithm Here compared very favourably against yet another established approach Intel s Concurrent Collections CnC In fact outperformed CnC both with respect to code size and ease of programming as well as performance and scalability An interesting question for future work is whether or better to what may be able to these positive results for the not dissimilar algorithms discussed in this paper Summary and Conclusions In this chapter we have discussed the properties of hierarchical matrix rithms arising in electromagnetic scattering problems and how to parallelize these problems on multicore heterogeneous and distributed hardware tures Two diﬀerent classes of algorithms were discussed in more detail MLFMA and NESA algorithms The main diﬀerence between these from parallelization perspective is that in the former the work performed for groups at diﬀerent levels varies signiﬁcantly while in the latter the work size per group is form Because of this parallelization of MLFMA needs to be more intrusive since the work in coarse level groups needs to be split over processes Both the data structures and the interaction patterns in the hierarchical matrix algorithms are irregular which is why we suggest to use parallel gramming model that supports asynchronous execution pilot tion using task parallel programming model for shared memory tures showed promising results regarding the potential to mix the computational phases during the execution and regarding the resulting utilization of the ware challenging aspect was the relatively small work sizes for individual groups We discuss diﬀerent approaches to managing task granularity that could be implemented in future projects When working with industrial or academic legacy codes several potentially conﬂicting interests inﬂuence the choices To change which algorithm is used is typically major investment since it is unlikely that this part is well rated from the rest of the code If the software was started from scratch today perhaps other algorithmic choices would be made in light of the current ing hardware architectures To achieve the best possible performance probably requires some refactoring of the code while minimizing the changes to the ing code is relevant both from cost perspective and maintainability tive Finally when using programming models which build on some particular implementation of system external dependencies are duced that complicate the administration of the software and introduce risk of future incompatibility or discontinuation In this chapter we have tried to shed light on some of these choices to support further work in the area Larsson et References Agullo Aumage Bramas Coulaud Pitoiset Bridging the gap between OpenMP and runtime systems for the fast multipole method IEEE Trans Parallel Distrib Syst https Agullo Bramas Coulaud Darve Messner Takahashi based FMM for multicore architectures SIAM Sci Comput https Anderson et LAPACK Users Guide edn Society for Industrial and Applied Mathematics Philadelphia Atkinson On the performance of parallel tasking runtimes for an irregular fast multipole method application In Supinski Olivier Terboven Chapman eds IWOMP LNCS vol pp Springer Cham https Augonnet Thibault Namyst Wacrenier StarPU uniﬁed platform for task scheduling on heterogeneous multicore architectures Concurr Comput Pract Exper https Benson Poulson Tran Engquist Ying parallel directional fast multipole method SIAM Sci Comput https Bordage Parallelization on heterogeneous multicore and systems of the fast multipole method for the Helmholtz equation using runtime system In Omatu Nguyen T eds Proceedings of the Sixth International ence on Advanced Engineering Computing and Applications in Sciences pp International Academy Research and Industry Association IARIA Curran Associates Red Hook Bosilca Bouteiller Danalis Faverge Dongarra PaRSEC exploiting heterogeneity to enhance scalability Comput Sci Eng Chandramowlishwaran Knobe Lowney Sarkar giari Multicore implementations of the Concurrent Collections programming model In Workshop on Compilers for Parallel Computing land Buttari Langou Kurzak Dongarra class of parallel tiled ear algebra algorithms for multicore architectures Parallel Comput Chandramowlishwaran Knobe Vuduc Performance evaluation of current Collections on multicore computing systems In IEEE International Parallel and Distributed Processing Symposium IPDPS Atlanta USA pp IEEE April Cruz Knepley Barba dynamically parallel fast multipole library Int Numer Methods Eng https Darve Cecka Takahashi The fast multipole method on parallel clusters multicore processors and graphics processing units Comptes Rendus https Parallelization of Hierarchical Matrix Algorithms Dastgeer Kessler Thibault Flexible runtime support for eﬃcient ton programming on hybrid systems In Proceedings of the national Conference on Parallel Computing Ghent Belgium September Advances in Parallel Computing vol pp IOS press https Duran et OmpSs proposal for programming heterogeneous architectures Parallel Proces Lett http Enmyren Kessler SkePU skeleton programming library for systems In Proceedings of the Internatioanl Workshop on Parallel Programming and Applications ACM ber https Ernstsson Li Kessler SkePU ﬂexible and skeleton ming for heterogeneous parallel systems Int J Parallel Program https Filipovic Benkner OpenCL kernel fusion for GPU Xeon Phi and CPU In Proceedings of the International Symposium on Computer Architecture and Computing pp IEEE https Filipovic Madzin Fousek Matyska Optimizing CUDA code by nel fusion application on BLAS Supercomput https Fukuda Matsuda Maruyama Yokota Taura Matsuoka Tapas an implicitly parallel programming framework for hierarchical algorithms In IEEE International Conference on Parallel and tributed Systems ICPADS pp December https Gelernter Carriero Coordination languages and their signiﬁcance mun ACM Gijsbers Grelck An eﬃcient scalable runtime system for macro data ﬂow processing using Int J Parallel Program https Gouin Methodology for image processing algorithms mapping on massively parallel architectures Technical report MINES ParisTech Gouin Ancourt Guettier An up to date mapping methodology for GPUs In Workshop on Compilers for Parallel Computing CPC Dublin Ireland April https Grelck Julku Penczek Distributed cluster and grid ing without the hassle In International Conference on Cluster Cloud and Grid Computing CCGrid Ottawa Canada IEEE Computer Society https Grelck Scholz Shafarenko Asynchronous stream processing with Int J Parallel Program https Grelck Scholz Shafarenko Coordinating data parallel SAC programs with In Proceedings of the IEEE International Parallel and Distributed Processing Symposium IPDPS Long Beach California USA IEEE puter Society Press Alamitos https Larsson et Gupta Stuart Owens study of persistent threads style GPU programming for GPGPU workloads In Innovative Parallel Computing dations and Applications of GPU Manycore and Heterogeneous Systems INPAR pp IEEE May https Hierarchical parallelization of the multilevel fast multipole algorithm MLFMA Proc IEEE https Holm Engblom Goude Holmgren Dynamic autotuning of adaptive fast multipole methods on hybrid multicore CPU and GPU systems SIAM Sci Comput https Kessler et Programmability and performance portability aspects of geneous systems In Proceedings of the Conference on Design Automation and Test in Europe pp IEEE March https Knobe Ease of use with Concurrent Collections CnC In USENIX Workshop on Hot Topics in Parallelism HotPar Berkeley USA Kurzak Pettitt Massively parallel implementation of fast multipole method for distributed memory machines J Parallel Distrib Comput https Lashuk et massively parallel adaptive fast multipole method on geneous architectures Commun ACM https Li Kessler Lazy allocation and transfer fusion optimization for heterogeneous systems In Proceedings of the Euromicro International Conference on Parallel Distributed and Processing pp IEEE March https Li Francavilla Vipiana Vecchi Chen Nested equivalent source approximation for the modeling of multiscale structures IEEE Trans Antennas Propag Li Francavilla Vipiana Vecchi Fan Chen doubly chical MoM for modeling of multiscale structures IEEE Trans tromagn Compat Li Francavilla Chen Vecchi Wideband fast modeling of large multiscale structures via nested equivalent source approximation IEEE Trans Antennas Propag https Ltaief Yokota execution of fast multipole methods Concurr Comput Pract Exp https Maghazeh Bordoloi Dastgeer Andrei Eles Peng aware packet processing on heterogeneous systems In Proceedings of the Design Automation Conference DAC pp ACM https Mautz Harrington Electromagnetic scattering from homogeneous material body of revolution Arch Electron Nilsson Fast numerical techniques for electromagnetic problems in frequency domain thesis Division of Scientiﬁc Computing Department of Information Technology Uppsala University Penczek Cheng Grelck Kirner Scheuermann Shafarenko based coordination approach to concurrent software engineering In Workshop on Execution Models for Extreme Scale Computing DFM Minneapolis USA IEEE https Parallelization of Hierarchical Matrix Algorithms Penczek et Parallel signal processing with Procedia Comput Sci https http iCCS Badia Labarta ming environment for architectures In Proceedings of the IEEE International Conference on Cluster Computing Tsukuba Japan October pp https https Qiao Reiche Hannig Teich Automatic kernel fusion for image processing DSLs In Proceedings of the International Workshop on Software and Compilers for Embedded Systems SCOPES ACM May https Rao Wilton Glisson Electromagnetic scattering by surfaces of arbitrary shape IEEE Trans Antennas Propag Seo Lee fast algorithm for solving PEC scattering problems IEEE Trans Magn Song Lu Chew Multilevel fast multipole algorithm for netic scattering by large complex objects IEEE Trans Antennas Propag Thibault On Runtime Systems for Programming on Heterogeneous Platforms Habilitation diriger des recherches L Bordeaux Thoman Jordan Fahringer Adaptive granularity control in task parallel programs using multiversioning In Wolf Mohr an Mey D eds LNCS vol pp Springer Heidelberg https Tillenius SuperGlue shared memory framework using data versioning for parallelization SIAM Sci Comput https Tillenius Larsson Badia Martorell task ing ACM Trans Embedded Comput Syst https Velamparambil Chew Analysis and performance of distributed ory multilevel fast multipole algorithm IEEE Trans Antennas Propag https Vipiana Francavilla Vecchi EFIE modeling of scale structures IEEE Trans Antennas Propag Wahib Maruyama Scalable kernel fusion for GPU tions In Proceedings of the International Conference for puting Networking Storage and Analysis SC pp IEEE https Wang Lin Yi Kernel fusion an eﬀective method for better power eﬃciency on multithreaded GPU In Proceedings of the International Conference on Green Computing and Communications and International ence on Cyber Physical and Social Computing pp https Wen Boyle Fensch MaxPair enhance OpenCL concurrent kernel execution by weighted maximum matching In Proceedings of the ACM https Larsson et YarKhan Kurzak Dongarra Quark users guide queueing and runtime for kernels Technical report Zafari TaskUniVerse uniﬁed interface for versatile parallel cution In Wyrzykowski Dongarra Deelman Karczewski K eds PPAM LNCS vol pp Springer Cham https Zafari et Task parallel implementation of solver for electromagnetic tering problems CoRR http Zafari Larsson Tillenius DuctTeip an eﬃcient programming model for distributed parallel computing submitted Zaichenkov Gijsbers Grelck Tveretina Shafarenko The cost and beneﬁts of coordination programming two case studies in Concurrent tions CnC and Parallel Process Lett https Zhang Asynchronous task scheduling of the fast multipole method using ious runtime systems In Fourth Workshop on Execution els for Extreme Scale Computing pp https Zhao Vouvakis Lee The adaptive cross approximation algorithm for accelerated method of moments computations of EMC problems IEEE Trans Electromagn Compat Open Access This chapter is licensed under the terms of the Creative Commons Attribution International License http which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give appropriate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this chapter are included in the chapter s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the chapter s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder Tail Distribution and Extreme Quantile Estimation Using Approaches Imen B and Elisabeth Allianstic Research Laboratory EFREI Paris Ecole d Informatique et Technologies du Avenue Villejuif France Scientiﬁc Computing Department of Information Technology Uppsala University Box Uppsala Sweden Abstract Estimation of tail distributions and extreme quantiles is important in areas such as risk management in ﬁnance and insurance in relation to extreme or catastrophic events The main diﬃculty from the statistical perspective is that the available data to base the mates on is very sparse which calls for tailored estimation methods In this chapter we provide survey of currently used parametric and parametric methods and provide some perspectives on how to move forward with estimation Keywords Risk measures Extreme value theory Kernel estimation Bandwidth selection Introduction This chapter presents position survey on the overall objectives and speciﬁc challenges encompassing the state of the art in tail distribution and extreme quantile estimation of currently used parametric and approaches and their application to Financial Risk Measurement What is envisioned is an enhanced estimation method based on the Extreme Value Theory approach The compounding perspectives of current challenges are addressed like the threshold level of excess data to be chosen for extreme values and the bandwidth selection from bias reduction perspective The application of the kernel estimation approach and the use of Expected Shortfall as ent risk measure instead of the Value at Risk are presented The extension to multivariate data is addressed and its challenges identiﬁed Overview of the Following Sections In the following sections Financial risk sures are presented in Sect Section covers Extreme Value Theory Sect c The Author s Ko lodziej and Eds cHiPSet LNCS pp https Rached and Larsson Parametric estimation and estimation methods Sect Parametric estimation methods and Sect the perspectives identiﬁed by the addressed challenges when estimating the presented ﬁnancial risk measures Financial Risk Measures The Long Term Capital Management collapse and the Russian debt crisis the Latin American and Asian currency crises and more recently the gage credit market turmoil followed by the bankruptcy of Lehman Brothers and the world s trading loss at are some examples of ﬁnancial disasters during the last twenty years In response to the serious cial crises like the recent global ﬁnancial crisis regulators have become more concerned about the protection of ﬁnancial institutions against catastrophic market risks We recall that market risk is the risk that the value of an investment will decrease due to movements in market factors The diﬃculty of modelling these rare but extreme events been greatly reduced by recent advances in Extreme Value Theory EVT Value at Risk VaR and the related concept of Expected Shortfall have been the primary tools for measuring risk exposure in the ﬁnancial services industry for over two decades Additional literature can be found in for Quantitative Risk Management and in or in for the application of EVT in insurance ﬁnance and other ﬁelds Value at Risk Consider the loss X of portfolio over given time period δ then VaR is risk statistic that measures the risk of holding the portfolio for the time period δ Assume that X cumulative distribution function cdf FX then we deﬁne VaR at level α as δ VaR α X inf x R P X x α inf x R FX x α F X α F X is the generalized inverse of the cdf FX Typical values of α are and while δ usually is day or days VaR become standard measure for risk management and is also recommended in the Basel II accord For an overview on VaR in more economic setting we refer to and Despite its widespread use VaR received criticism for failing to distinguish between light and heavy losses beyond the VaR Additionally the traditional VaR method been criticized for violating the requirement of Artzner et analysed risk measures and stated set of that should be desirable for any risk measure The four axioms they stated are Monotonicity Higher losses mean higher risk Translation Equivariance Increasing or decreasing the loss increases decreases the risk by the same amount Subadditivity Diversiﬁcation decreases risk Positive Homogeneity Doubling the portfolio size doubles the risk Tail Distribution and Extreme Quantile Estimation Any risk measure which satisﬁes these axioms is said to be coherent related concept to VaR which accounts for the tail mass is the conditional tail tion CVaR or Expected Shortfall is the average loss conditional on the VaR being exceeded and gives risk managers additional valuable information about the tail risk of the distribution Due to its usefulness as risk measure in the Basel Committee on Bank Supervision even proposed replacing VaR with to measure market risk exposure Conditional Value at Risk or Expected Shortfall CVaRα X VaRα X Acerbi and Tasche proved in that CVaR satisﬁes the above axioms and is therefore coherent risk measure Conditional can be derived from VaR in the case of ous random variable and another possibility to calculate CVaR is to use Acerbi s Integral Formula CVaRα X α α VaRβ X dβ Estimating from the empirical distribution is generally more diﬃcult than estimating VaR due to the scarcity of observations in the tail As in most risk applications we do not need to focus on the entire distribution Extreme value theory is then practical and useful tool for modeling and quantifying risk Value at Risk and Extreme value theory is covered well in most books on risk management and VaR in particular also with much less extent see for example and Vice versa VaR is treated in some Extreme value theory literature such as and Extreme Value Theory Two Main Approaches Extreme value theory EVT is the theory of modelling and measuring events which occur with very small probability More precisely having an Xn sample of n random variables independently and identically following bution function F we want to estimate the real xpn deﬁned by xpn pn with pn where pn is known sequence and u inf x R x u is the generalized inverse of the survival function F Note that xpn is the order quantile pn of the cumulative distribution function F similar problem to the estimate of xpn is the estimate of small ties pn or the estimation of the tail distribution In other words for series of ﬁxed cn reals we want to estimate the probability pn deﬁned by pn P X cn with cn xn n Rached and Larsson The main result of Extreme Value Theory states that the tails of all butions fall into one of three categories regardless of the overall shape of the distribution Two main approaches are used for implementing EVT in practice Block maxima approach and Peaks Over Thresholds POT Block Maxima Approach The Fisher and Tippett and Gnedenko theorems are the fundamental results in EVT The theorems state that the maximum of sample of properly normalized independent and identically distributed random variables converges in distribution to one of the three possible distributions the Weibull Gumbel or the Theorem Fisher Tippett Gnedenko Let Xn F and n Xn n If there exist two sequences an and bn and real γ such that P Xn n an bn x Hγ x when n then Hγ x γx exp exp exp if γ if γ γx x We say that F is in the domain of attraction of Hγ and denote this by F DA Hγ The distribution function Hγ is called the Generalized Extreme Value distribution GEV This law depends only on the parameter called the tail index The density associated is shown in Fig for diﬀerent values of γ According to the sign of γ we deﬁne three areas of attraction x H γ x Fig The GEV distribution for γ solid line γ dashed line and γ line Tail Distribution and Extreme Quantile Estimation If γ F DA This domain contains the laws for which the survival function decreases as power function Such tails are know as fat tails or heavy tails In this area of attraction we ﬁnd the laws of Pareto Student Cauchy etc If γ F DA Gumbel This domain groups laws for which the vival function declines exponentially This is the case of normal gamma exponential etc if γ F DA Weibull This domain corresponds to thin tails where the distribution ﬁnite endpoint Examples in this class are the uniform and reverse Burr distributions The Weibull distribution clearly ﬁnite endpoint F sup x F x This is usually the case of the distribution of mortality and insurance claims for example see The tail is thicker than the bel s Yet it is well known that the distributions of the return series in most ﬁnancial markets are heavy tailed fat tails The term fat tails can have eral meanings the most common being extreme outcomes occur more frequently than predicted by the normal distribution The block Maxima approach is based on the utilization of maximum or imum values of these observations within certain sequence of constant length For suﬃciently large number k of established blocks the resulting peak ues of these k blocks of equal length can be used for estimation The procedure is rather wasteful of data and relatively large sample is needed for accurate estimate Peaks Over Threshold POT Approach The POT approach consists of using the generalized Pareto distribution GPD to approximate the distribution of excesses over threshold This approach been suggested originally by hydrologists This approach is generally preferred and forms the basis of our approach below Both EVT approaches are equivalent by the Haan theorem sented in Theorem Haan For large class of underlying distribution functions F F DA Hγ sup F x Gγ σ u x F where F sup x F x is the end point of the distribution Fu x P X u u is the distribution of excess and Gγ σ is the Generalized Pareto Distribution GPD deﬁned as Gγ σ x x exp γ σ if γ σ if γ σ Rached and Larsson This means that the conditional excess distribution function Fu for u large is well approximated by Generalized Pareto Distribution Note that the tail index γ is the same for both the GPD and GEV distributions The tail shape parameter σ and the tail index are the fundamental parameters governing the extreme behavior of the distribution and the eﬀectiveness of EVT in forecasting depends upon their reliable and accurate estimation By incorporating information about the tail through our estimates of γ and σ we can obtain VaR and estimates even beyond the reach of the empirical distribution Parametric and Estimation Methods The problem of estimating the tail index γ been widely studied in the erature The most standard methods are of course the method of moments and maximum likelihood Unfortunately there is explicit form for the parameters but numerical methods provide good estimates More generally the two common approaches to estimate the tail index are models the Hill estimator Fully parametric models the Generalized Pareto distribution or GPD Estimation The most known estimator for the tail index γ of fat tails distribution is without contest the Hill estimator The formal deﬁnition of fat tail tions comes from regular variation The cumulative distribution is in the domain if and only if as x the tails are asymptotically F x where and τ Based on this approximation the Hill estimator is written according to the statistics order n Xn n as follows Hkn n kn ln n ln n where kn is sequence so that kn Other estimators of this index have have been proposed by Beirlant et using regression exponential model to reduce the Hill estimator bias and by that introduce least squares estimator The use of kernel in the Hill estimator been studied by et An eﬀective estimator of the extreme value index been proposed by Falk and Marohn in more detailed list of the diﬀerent works on the estimation of the index of extreme values is found in Note that the Hill estimator is sensitive to the choice of threshold u n or the number of excess kn and is only valid for data Tail Distribution and Extreme Quantile Estimation Parametric Estimation The principle of POT is to approximate the survival function of the excess distribution by GPD after estimating its parameters from the distribution of excess over threshold u as explained in the following two steps First distribution estimation Let Xn follow distribution F and let YNn Yi Xi be the exceedances over chosen threshold The distribution of excess Fun is given by Fun P X X and then the distribution F of the extreme observations is given by F F The distribution of excess Fun is approximated by Gγ σ and the ﬁrst step consists in estimating the parameters of this last distribution using the sample YNn The parameter estimations can be done using MLE Diﬀerent methods have been proposed to estimate the parameters of the GPD Other estimation methods are presented in The Probability Weighted Moments PWM method proposed by Hosking and Wallis for γ was extended by Diebolt et by generalization of PWM estimators for γ as for many applications in insurance distributions are known to have tail index larger than Second estimation In order to estimate the extreme quantile xp deﬁned as xpn xpn F xpn pn npn We estimate F u by its empirical counterpart and we approximate Fun by the approximate Generalized Pareto Distribution GP D ˆγn ˆσn in the Eq Then for the threshold u n the extreme quantile is estimated by ˆxpn k n ˆσn ˆγn k npn ˆγn The application of POT involves number of challenges The early stage of data analysis is very important in determining whether the data the fat tail needed to apply the EVT results Also the parameter estimates of the limit GPD distributions depend on the number of extreme observations used The choice of threshold should be large enough to satisfy the conditions to permit its application u tends towards inﬁnity while at the same time leaving suﬃcient observations for the estimation high threshold would generate few excesses thereby inﬂating the variance of our parameter estimates Lowering the threshold would necessitate using samples that are longer considered as being in the tails which would entail an increase in the bias Rached and Larsson Estimation Methods main argument for using estimation methods is that ciﬁc assumptions on the distribution of the data is made priori That is model speciﬁcation bias can be avoided This is relevant when there is limited mation about the theoretical data distribution when the data can potentially contain mix of variables with diﬀerent underlying distributions or when suitable parametric model is available In the context of extreme value butions the GPD and GEV distributions discussed in Sect are appropriate parametric models for the univariate case However for the multivariate case there is general parametric form We restrict the discussion here to one particular form of estimation kernel density estimation Classical kernel estimation performs well when the data is symmetric but problems when there is signiﬁcant skewness common way to deal with skewness is transformation kernel estimation which we will discuss with some details below The idea is to transform the skew data set into another variable that more symmetric distribution and allows for eﬃcient classical kernel estimation Another issue for kernel density estimation is boundary bias This arises because standard kernel estimates do not take knowledge of the domain of the data into account and therefore the estimate does not reﬂect the actual behaviour close to the boundaries of the domain We will also review few bias correction techniques Even though kernel estimation is with respect to the lying distribution there is parameter that needs to be decided This is the bandwidth scale of the kernel function which determines the smoothness of the density estimate We consider techniques intended for constant bandwidth and also take brief look at variable bandwidth kernel estimation In the latter case the bandwidth and the location is allowed to vary such that bias can be reduced compared with using ﬁxed parameters Kernel density estimation can be applied to any type of application and data but some examples where it is used for extreme value distributions are given in non parametric method to estimate the VaR in extreme quantiles based on transformed kernel estimation TKE of the cdf of losses was proposed in kernel estimator of conditional is proposed in In the following subsections we start by deﬁning the classical kernel tor then we describe selection of measures that are used for evaluating the quality of an estimate and are needed in the algorithms for bandwidth tion Finally we go into the diﬀerent subareas of kernel estimation mentioned above in more detail Classical Kernel Estimation Expressed in words classical kernel estimator approximates the probability density function associated with data set through sum of identical symmetric Tail Distribution and Extreme Quantile Estimation kernel density functions that are centered at each data point Then the sum is normalized to have total probability mass one We formalize this in the following way Let k be bounded and symmetric probability distribution function pdf such as the normal distribution pdf or the Epanechnikov pdf which we refer to as the kernel function Given sample of n independent and identically distributed observations Xn of random variable X with pdf fX x the classical kernel estimator is given by ˆfX x n n kb x Xi where kb mator for the cumulative distribution function cdf is given by b and b is the bandwidth Similarly the classical kernel b k ˆFX x n n Kb x Xi x kb t dt That is K is the cdf corresponding to the pdf where Kb x k Selected Measures to Evaluate Kernel Estimates measure that we would like to minimize for the kernel estimate is the mean integrated square error MISE This is expressed as ˆfX x fX x dx MISE b Ω where Ω is the domain of support for fX x and the argument b is included to show that minimizing MISE is one criterion for bandwidth selection However MISE can only be computed when the true density fX x is known MISE can be decomposed into two terms The integrated square bias and the integrated variance Ω ˆfX x fX x ˆfX x dx Var Ω dx To understand these expressions we ﬁrst need to understand that ˆfX is random variable that changes with each sample realization To illustrate what it means we work through an example Example Let X be uniformly distributed on Ω and let k be the Gaussian kernel Then fX x x otherwise Rached and Larsson For each kernel function centered at some data point we have that EX kb x kb x Kb x Kb x If we apply that to the kernel estimator we get the integrated square bias Kb x Kb x dx By ﬁrst using that Var X X X we get the following expression for the integrated variance kb x Kb x Kb x dx The integrals are evaluated for the Gaussian kernel and the results are shown in Fig The bias which is largest at the boundaries is minimized when the bandwidth is very large but large bandwidth also leads to large variance Hence MISE is minimized by bandwidth that provides between bias and variance s b i r u q S x c n i r V S M I x b Fig The square bias left and the variance middle as function of x for b The curve for b is shown with dashed line in both cases MISE right is shown as function of b solid line together with the integrated square bias dashed line and the integrated variance line To simplify the analysis MISE is often replaced with the asymptotic MISE approximation AMISE This holds under certain conditions involving the ple size and the bandwidth The bandwidth depends on the sample size and we can write b b n We require b n as n while nb n as n Furthermore we need fX x to be twice continuously diﬀerentiable We then have the asymptotic approximation AMISE b nb R k k f Tail Distribution and Extreme Quantile Estimation where R g mizes AMISE can be analytically derived to be g x dx and mp k xpk x dx The bandwidth that bopt R k k f n leading to AMISE bopt k k f The optimal bandwidth can then be calculate for diﬀerent kernel functions We have for the Gaussian kernel bG opt πR f n The diﬃculty in using AMISE is that the norm of second derivative of the unknown density needs to be estimated This will be further discussed under the subsection on bandwidth selectors We also mention the skewness γX of the data which is measure that can be used to see if the data is suitable for classical kernel estimation We estimate it as ˆγX n n n n Xi Xi It was shown in see also that minimizing the square integrated error ISE for speciﬁc sample is equivalent to minimizing the function ˆfX x CV b Ω dx n n ˆfi xi where ˆfi is the kernel estimator obtained when leaving the observation xi out Other useful measures of the goodness of ﬁt are also discussed in Kernel Estimation As was illustrated in Example boundaries where the density does not go to zero generate bias This happens because the kernel functions cross the boundary and some of the mass ends up outside the domain We want from the kernel method that ˆfX x fX x in all of the support Ω of the density function but this condition does not hold at boundaries unless we also have that the density is zero there An overview of the topic and of simple boundary correction methods is given in By employing linear bias correction method we can make the moments of order and satisfy the consistency requirements total probability mass and such that the expectation is consistent to order Rached and Larsson at the boundary general linear correction method for density supported on Ω that is shown to perform well in the form kb for kernel function centered at the data location x The coeﬃcients aj aj b x are computed as z aj b x yjkb x dy where z is the end point of the support of the kernel function An example with modiﬁed Gaussian kernels close to boundary is shown in Fig At the boundary the amplitude of the kernels becomes higher to compensate for the mass loss while away from the boundary they resume the normal shape and size The kernel functions closest to the boundary become negative in small region but this does not aﬀect the consistency of the estimate Fig Bias corrected kernel functions using the linear correction approach near boundary dashed line more recent bias correction method is derived in based on ideas from This type of correction is applied to the kernel estimator for the cdf and can be seen as Taylor expansion It also improves the capturing of valleys and peaks in the distribution function compared with the classical kernel estimator It requires that the density is four times diﬀerentiable that the kernel is metric and at least for the theoretical derivations that the kernel is compactly supported on Ω The overall bias of the estimator is as pared with for the linear correction method while the variance is similar to what is achieved with the uncorrected estimator This boundary correction approach is used for estimating extreme value distributions in x ˆFX x λ x x Tail Distribution and Extreme Quantile Estimation where x ˆFX x b b ˆfX x b and λ k The parameter λ is kernel dependent and should be chosen such that AMISE is minimized but according to the estimator is not that sensitive to the choice An explicit expression for AMISE with this correction is derived in and is also cited in where the value λ is also given as an approximate minimizer of the variance for the Epanechnikov kernel Transformation Kernel Estimation The objective in transformation kernel estimation is to ﬁnd transformation of the random variable X which for example distribution into symmetric random variable Then classical kernel estimation can be successfully applied to The transformation function T should be monotonic and increasing For true density it should also be concave It also needs to have at least one continuous derivative The transformation is applied to the original data to generate transformed data sample yi T xi i For the pdfs of the two random variables it holds that fX x T x fY and for the cdfs we have that FX x FY We apply the kernel density estimator to the transformed data leading to the following estimator for the original density ˆfX x T x ˆfY T x n n kb T x T xi Several diﬀerent transformation classes have been proposed for heavy tailed data The shifted power transformation family was proposed in T x sign x ln x where min xi and An algorithm for choosing the transformation parameters is given in First restriction is made to parameters that give close to zero skewness for the transformed data Then AMISE of the classical kernel estimation for the density fY is minimized assuming an asymptotically optimal bandwidth This is equivalent to minimizing R f As Rached and Larsson we do not know the true density an estimator is needed The estimator suggested in is ˆR f n n n k k yi yj k u s k s ds and c is the bandwidth used where the convolution k k u in this estimate The mapping introduced in takes data in ΩX and maps it to ΩY xα M α xα M α The scale M is determined by minimizing R ˆf Given scale M α is mined such that probability mass spills over at the right boundary That is the resulting density does not have mass at or beyond inﬁnity T x modiﬁed Champernowne distribution transformation is derived in with transformation function T x α x c M c α cα x c α M c α where M can be chosen as the median of the data and α and c are found by maximizing log likelihood function see So far we have only considered the possibility of performing one mation but one can also transform the data iteratively or perform two speciﬁc consecutive transformations Doubly transformed kernel estimation is discussed in The idea is to ﬁrst transform the data to something close to uniform and then to apply an inverse beta transformation This makes the ﬁnal bution close to beta distribution and the optimal bandwidth can then easily be computed Bandwidth Selection As brieﬂy mentioned in Sects and the choice of bandwidth b in kernel estimation signiﬁcant impact on the quality of the estimator but choosing the appropriate bandwidth requires the use of one estimator or another The bandwidth estimator of Silverman b ˆσX is often cited but it assumes that the underlying density can be approximated by normal density This is hence not appropriate for distributions Many bandwidth selection methods use normal reference at some step in the process but this introduces parametric step in the estimation An interesting alternative the Improved bandwidth Tail Distribution and Extreme Quantile Estimation selection algorithm is also described in where the normal reference is inated by formulating equation for the optimal bandwidth We start from the point of how to estimate R f least two possible estimators based on the equality X f x There are at f j X jE f The two types of estimators are f j X n n R b x xk k j k j b x xm dx and j f j n n k b xk xm By requiring the two estimators to have the same asymptotic mean square error we get condition on the bandwidth derivation for the Gaussian kernel is provided in and we summarize these result here to illustrate the bandwidth selection algorithm Requiring and to be asymptotically equivalent gives the following relation between the bandwidths for two consecutive tives j f π N γj and for the Gaussian kernel for j becomes f X j n n k xk xm The two relations and together deﬁne the function γj b We also have Eq for the bandwidth of the Gaussian kernel Combining this with for j allows us to eliminate f to get Now we can apply recursively to get relation ξγ where γ b γ γ b for Here it would be possible to assume normal distribution to estimate and then all the other bandwidths However this does not work well if the true distribution is far from normal In the improved algorithm f in order to compute Rached and Larsson we instead assume that b for some large enough The experiments in indicate that should be enough We then get equation to solve for b b ξγ b Using this relation assumptions on the true distribution are made and this bandwidth selector is shown to perform well also for distributions More Challenges in Estimating the Risk Time Series and Multivariate Case Dynamic Approach Highlighting the underlying assumptions is relevant for understanding model uncertainty when estimating rare or extreme events The VaR and are estimated given that the distribution of asset returns does not change over time In the last two sections when applying the POT approach to the returns in order to calculate these risk measures their distribution was assumed to be stationary dynamic model which captures current risk is then more realistic EVT can also be used based on stochastic time series model These dynamic models use an type process along with the POT to model VaR and which depend on and change due to the ﬂuctuations of the market This approach studied in reﬂects two stylized facts exhibited by most ﬁnancial return series namely stochastic volatility and the of conditional return distributions over short time horizons The Multivariate Case for EVT When estimating the VaR of portfolio under ﬁnancial crises correlations between assets often become more positive and stronger Assuming that the variables are independent and cally distributed is strong hypothesis Portfolio losses are the result not only of the individual asset s performance but also and very importantly the result of the interaction between assets Hence from the accuracy point of view ideally we would prefer the multivariate approach An extension of the univariate EVT models using dependence structure leads to parametric model and is then expected to be less eﬃcient for scarce data approach should be preferred to estimate portfolio tail risk Transformation kernel density estimation is used in for studying tivariate extreme value distributions in temperature measurement data Future directions involve to apply this type of methodology to real and simulated folio data Tail Distribution and Extreme Quantile Estimation References Acerbi Tasche On the coherence of expected shortfall J Bank Finance McNeil Frey Estimation of risk measures for heteroscedastic ﬁnancial time series an extreme value approach Empir Finance Alemany nonparametric approach to calculating Insur Math Econ Artzner Delbaen Eber Heath Coherent measures of risk Math Finance Balkema Haan Residual life time at great age Ann Probab Beirlant Dierckx Goegebeur Matthys Tail index estimation and an exponential regression model Extremes Beirlant Dierckx Guillou On exponential representations of of extreme order statistics Extremes Beranger Duong Sisson Exploratory data analysis for moderate extreme values using Kernel methods Bahraoui Alemany Estimating extreme value cumulative tribution functions using Kernel approaches Perch Nielsen Kernel density estimation of actuarial loss functions Insur Math Econ Botev Grotowski Kroese Kernel density estimation via diﬀusion Ann Stat Nielsen Kernel density estimation for distributions using the Champernowne transformation Statistics Cai Wang Nonparametric estimation of conditional VaR and expected shortfall Econ Chen estimation of expected shortfall Financ Econ Choi Hall On bias reduction in local linear smoothing Biometrika Clements Hurn Lindsay mappings and their use in Kernel density estimation J Am Stat Assoc Coles An Introduction to Statistical Modeling of Extreme Values Springer Series in Statistics Springer London https Deheuvels Mason Kernel estimates of the tail index of tribution Ann Stat Viharos Estimating the tail index In Szyszkowicz B ed totic Methods in Probability and Statistics pp dam Danielsson Financial Risk Forecasting Wiley Hoboken Diebolt Guillou Rached Approximation of the distribution of excesses through generalized moments method Stat Plan Infer Dowd Measuring Market Risk Wiley Hoboken Rached and Larsson Duﬃe Pan An overview of value at risk Deriv Eling Fitting insurance claims to skewed distributions are the and good models Insur Math Econ Embrechts P ed Extremes and Integrated Risk Management Risk Books don Embrechts Mikosch Modelling Extremal Events for Insurance and Finance Springer Heidelberg https Falk Marohn Eﬃcient estimation of the shape parameter in Pareto models with partially known scale Stat Decis Feuerverger Hall Estimating tail exponent by modelling departure from Pareto distribution Ann Stat Fisher Tippett Limiting forms of the frequency distribution of the largest or smallest member of sample Math Proc Camb Philos Soc Gnedenko Sur distribution limite du terme maximum d une Ann Math Hill simple general approach to inference about the tail of distribution Ann Stat Hosking Wallis Parameter and quantile estimation for the generalized Pareto distribution Technometrics Hull Risk Management and Financial Institutions Prentice Hall Upper dle River Jones Simple boundary correction for Kernel density estimation Stat put Jones Marron Sheather brief survey of bandwidth selection for density estimation J Am Stat Assoc Jones McKay Hu Variable location and scale Kernel density estimation Ann Inst Stat Math Jorion Value at Risk The New Benchmark for Managing Financial Risk New York Kim Kim Park Lee bias reducing technique in Kernel tion function estimation Comput Stat McNeil Frey Embrechts Quantitative Risk Management Concepts Techniques and Tools Princeton Series in Finance Princeton University Press Princeton Pickands Statistical inference using extreme order statistics Ann Stat Pitt Guillen Estimation of parametric and ric models for univariate claim severity distributions an approach using Reiss Thomas Statistical Analysis of Extreme Values with Applications to Insurance Finance Hydrology and Other Fields Basel Scaillet Nonparametric estimation of conditional expected shortfall Insur Risk Manag J Silverman Density Estimation for Statistics and Data Analysis Monographs on Statistics and Applied Probability vol Chapman London Wand Marron Ruppert Transformations in density estimation J Am Stat Assoc Tail Distribution and Extreme Quantile Estimation Open Access This chapter is licensed under the terms of the Creative Commons Attribution International License http which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give appropriate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this chapter are included in the chapter s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the chapter s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder Towards Eﬃcient and Scalable Content Delivery Issues and Challenges Irene B Alejandro Anthony Christos Valentina Nikolas Rabih and George Department of Computer Science University of Cyprus Nicosia Cyprus irenekilanioti mettour george Departamento Lenguajes Sistemas Universidad Sevilla Seville Spain afdez damiancerero Faculty of Electronic Engineering University of Nis Niˇs Serbia valentina TEI of Thessaly Karditsa Greece karageorgos nikosbit University of East London London UK Abstract This chapter presents the authors work for the Case Study entitled Delivering Social Media with Scalability within the framework of Modelling and Simulation for Big Data tions cHiPSet COST Action We identify some core research areas and give an outline of the publications we came up within the framework of the aforementioned action The ease of user content generation within social media platforms information multimedia data along with the proliferation of Global Positioning System GPS capture devices lead to data streams of unprecedented amount and radical change in information sharing Social data streams raise variety of practical challenges derivation of meaningful insights from eﬀectively gathered social information paradigm shift for content distribution with the leverage of contextual data associated with user preferences geographical characteristics and devices in general etc In this article we present the methodology we followed the results of our work and the outline of comprehensive survey that depicts the situation and organizes challenges concerning social media streams and the infrastructure of the data centers supporting the eﬃcient access to data streams in terms of content distribution data diﬀusion data replication energy eﬃciency and network infrastructure The challenges of enabling better provisioning of social media data have been identiﬁed and they were based on the context of users accessing these resources The existing literature been systematized and the main research points and industrial eﬀorts in the area were identiﬁed and analyzed In our works in the framework of the Action we came up with c The Author s Ko lodziej and Eds cHiPSet LNCS pp https Towards Eﬃcient and Scalable Content Delivery potential solutions addressing the problems of the area and described how these ﬁt in the general ecosystem Introduction Social Data Streams Features Herein some basic terminology for the topic of our Case Study entitled ering Social Media with Scalability within the framework of Modelling and Simulation for Big Data Applications cHiPSet COST Action is introduced The terminology appears in published works and as well Social networks media and platforms enable communication exchange ness and knowledge acquisition as well as social network users connection with each other with the purpose of sharing content Social data is the information that social media users share information multimedia data tags annotations and likes and may include metadata such as the user s location native language biographical data and shared links whereas streams denotes various approaches have been performed that we do not refer to static datasets but rather to dynamic information generated and transmitted over the Online Social Network OSN Formally an OSN is depicted by directed graph G V where V is the set of the vertices of the graph representing the nodes of the network and are the edges between them denoting various relationships among the edges of the graph The semantics of these edges vary and their interpretation is expanded for various OSNs from personal acquaintance to common interests microblogging services or business contact As far as the directionality of the edges of the social graph is concerned it is associated with the concept of the OSN for Facebook an edge denotes mutual friendship between the endpoints of link for Twitter if the edge between and B points at B s posts tweets appear in B s main Twitter page and so on social node centrality is indicative of the importance of node within social network It is given in terms of function on the vertices of graph where the values produced are expected to provide ranking which identiﬁes the most important nodes In Rogers classic work the author deﬁnes information diﬀusion as the process in which an innovation is communicated through certain channels over time among the members of social system In this context the innovation is deﬁned as the ﬁrst spread of information from an originator social cascade is speciﬁc case of information diﬀusion and practically occurs within an OSN when piece of information is extensively retransmitted after its initial publication from user Cascades can be represented as rooted directed trees where the initiator of the cascade is the root of the tree and the length of the cascade is the height of the resulting tree Each vertex in the cascade tree can have the information of the user and the identity of the item replicated in the cascade Figure depicts an example of the evolution of social cascade in directed Kilanioti et Social Cascade B C D F H G I J Fig The evolution of social cascade in Twitter graph The cascade follows the arrows direction For example in Twitter B C D are followers of whereas the adopters of new information piece could be the nodes that after having been exposed in video link they retransmit it contributing remarkably to Internet traﬃc Challenges for Distribution of Social Data Streams In the survey we wrote in the framework of cHiPSet COST Action we identiﬁed the challenges of enabling better provisioning of social media data based on the context of users accessing these resources In our works that we produced in the framework of the Action we came up with potential solutions addressing the problems of the area and described how these ﬁt in the general ecosystem Distributing social data streams largely depends on the exploitation of usage patterns found in OSNs and can be improved either through the selective prefetching of content or through the strategic of the employed infrastructure The cost of scaling such content might be the number of replicas needed for speciﬁc source or it may take into account the optimal use of memory and processing time of built system Optimization of energy eﬃciency for data centers that support social data interaction and analysis includes tasks such as data growth data center federation and Content Delivery Network CDN at data center level In our taxonomy Fig pillars associated with include Computing Diﬀusion Models and Content Distribution challenges whereas Software for Towards Eﬃcient and Scalable Content Delivery Challenges for Distribution of Social Data Streams Computing Software for Infrastructure Efficiency Content Diffusion Models Content Distribution Environments Social Data Streams Architectures Systems Techniques Solutions Workload Consolidation Techniques VM Consolidation Migration Data Replication Placement Context Representation Modelling Context Sensing Ubiquitous Computing Scheduling Algorithms Data Center Scheduling Frameworks User Privacy Limited Storage Space Power Issues Heterogeneous and Limited Screen size Connectivity Communication Data Exchange Fig Taxonomy of challenges for distribution of social data streams Infrastructure Eﬃciency is associated with This taxonomy includes solutions or approaches to the Challenges for Distribution of Social Data Streams These solutions or approaches require enough eﬀort hence they can also be considered as challenge for the research community Computing Application of social contextual information such as proﬁles images videos biometrical geolocation data and local data in situations where conventional content scaling is infeasible could largely facilitate the spreading of information the identiﬁcation of tial information sources as well as paradigm shift in the way users access and control their personal data multimedia content is especially ﬁcult due to its long tail nature with each item probably not popular enough to be replicated in global scale but with the altogether getting suﬃcient accesses Social analysis tasks interweaved with computing could pave the ground for preactive caching mechanisms in the framework of content delivery infrastructure of streaming providers Software for Infrastructure Eﬃciency The industry made several eﬀorts to address challenges associated with optimization of energy eﬃciency for data centers that support social data interaction and analysis such as data growth isolation interactions data center federation and at data center level but usually lacks from focusing on energy consumption of the employed infrastructures The challenges in the area of include workload consolidation and techniques Virtual Machines VMs consolidation and migration data tion and placement and scheduling algorithms Kilanioti et Diﬀusion Models Prevalence of OSNs formed the landscape of content exchange Popularity of relatively data heavy multimedia user generated content UGC also risen resulting in data deluge across all media platforms Measurement studies such as attribute the recent increases in traﬃc to the extended use of OSNs Elaborate data manipulation presupposes coping with the size of social graphs with billions of nodes and edges Facebook for ple reported that had billion daily active users on average for June and billion monthly active users as of June Its custom data warehouse and analytics infrastructure to apply queries and custom MapReduce jobs in continuous basis on over half petabyte of new data every h for the creation of meaningful aggregations and analysis It is also acknowledged that large proportion of media is distributed via reposted OSN links contributing signiﬁcantly to Internet ﬁc These challenges are closely associated with the Diﬀusion Models used to represent the diﬀusion of information over OSNs and facilitate relevant algorithmic solutions Fig Content Distribution The delivery infrastructure of video operators is made up of scattered servers which with speciﬁc cache selection anisms direct users to the closest servers hosting the requested data sion Control Protocol TCP however is subject to delay jitter and throughput variations and clients are required to preload playout buﬀer before starting the video playback Thus the quality of experience QoE of media platform users is primarily determined by stalling eﬀects on the application layer For the YouTube case cache server selection is also highly Internet Service Provider ISP with geographical proximity not being the primary criterion and DNS level redirections for purposes occurring quite frequently and substantially contributing to the initial startup delay of the playback eral and approaches are focused on the detection of such interruptions that negatively aﬀect the user experience With the growing popularity of OSNs and the increased traﬃc due to outspread of information via the latter the improvement of user experience through scaling demanding content largely depends on the exploitation of usage patterns and geolocation data associated with OSNs These challenges are closely associated with the Architectures Systems and Techniques within the infrastructure Some key factors contributing to the problem of diﬀusion of intensive media content over OSNs are discussed below Datasets In order to harness the power of social networks sion over CDN infrastructure the key areas of interest that need to be explored include the large size of the graphs and also the fact that diﬀusion of links is multiplied through dissemination over sites like YouTube and ampliﬁed by the proliferation of smartphones and cheap broadband connections The amount of information in OSNs is an obstacle since elaborate manipulation of the data Towards Eﬃcient and Scalable Content Delivery may be needed An open problem is the eﬃcient handling of graphs with billions of nodes and edges The desired scaling property refers to the fact that the throughput of the proposed approaches should remain unchanged with the increase in the data input size such as the large datasets that social graphs comprise and the social cascades phenomena that amplify the situation Cost of scaling such content can be expressed in diﬀerent ways For instance it may be matched with the number of replicas needed for speciﬁc source Future experimentations may take into account the optimal use of memory and processing time of an built system Internet of Things IoT is global infrastructure that interconnects things based on interoperable information and communication technologies and through identiﬁcation data capture processing and communication capabilities enables advanced services Things are objects of the physical world cal things such as devices vehicles buildings living or inanimate objects mented with sensors or the information world virtual things capable of being identiﬁed and integrated into communication networks It is estimated that the number of devices surpassed the human population in and that there will be about million devices by Thus the still ongoing signiﬁcant IoT innovation is expected to generate massive amounts of data from diverse locations that will need to be collected indexed stored and analyzed OSN Evolution Existent works examine valuable insights into the dynamic world by posing queries on an evolving sequence of social graphs Time evolving graphs are increasingly used as paradigm for the emerging area of OSNs However the ability to scalably process queries concerning the information diﬀusion remains to great extent unstudied With the exception of sporadic works on specialized problems such as that of inference of dynamic networks based on information diﬀusion data at the time of writing the authors are not aware of relative studies on the information diﬀusion through OSNs under the prism of graphs dynamicity Approaches The demand for data applications that risen in recent decade lead to development of Fifth Generation Wireless cations Development of eﬃcient mechanisms for supporting mobile multimedia and data services is prerequisite for networks Real bottleneck of todays mobile networks is the radio access network and the backhaul Caching in the intermediate nodes servers gateways routers and mobile users devices can reduce doubled transmission from content providers and core mobile networks Known caching techniques that can be used within are content bution network networks networking http web caching evolved packet core caching radio access network caching device to device caching proactive caching predictive caching cooperative caching Those techniques are using diﬀerent algorithms and models Analysis presented Kilanioti et in shown that the deployment of those caching techniques in mobile work can reduce redundant traﬃc in backhaul minimize the traﬃc load increase the transfer rate in mobile network and reduce the latency Correlation of several caching methods and procedures could result in improving network performance and obtaining better results On the other hand well known bottleneck that brings is the complex erogeneity of the network Particularly network consists of diﬀerent technologies that coexist where some technologies could potentially disable the transmission of data of equipment that use other technologies Thus we need solution that eﬃciently handles resources in space frequency and device dimensions Semantic coordination could alternatively be used in such networks The nodes in the system can communicate and share knowledge in terms of the spectrum utilization in the network In the authors proposed to model the spectrum usage coordination as an interactive process between number of distributed communicating agents where agents share their speciﬁc information and knowledge The information includes the current spectrum usage state tial coordinates of the device available communication protocols usage policy spectrum sensing capabilities of the device spectrum needs etc An approach for such coordination is presented in and it is based on semantic gies and communication between heterogeneous agents with potentially diﬀerent capabilities and minimal common compliance The core knowledge is sented by ontologies whose representation and usage is speciﬁed in standardized way The approach is used as dynamic spectrum coordination algorithms used for coordination among diﬀerent wireless technologies in networking This semantic technologies based approach can be used for wide diapason of problems within heterogeneous networks such as network states predictions network analysis minimizing traﬃc load content distribution coordination etc This approach could be used in combination with caching techniques in order to improve content distribution in but further research should be done in this area Mobile CDNs and the Cloud Mobile computing MC created mous demand for online experience that CDNs are required to isfy coverage and rapid extension of provide undisrupted connectivity for mobile devices whereas devices that hop seamlessly from WiFi to cellular networks and technologies such as will be optimised for uses that put premium on continuous connectivity regardless of the user location optimizations for applications along with drastically simpliﬁed and more intuitive use of devices with interactions instead of physical keyboards contribute to mobile applications becoming the premium mode of accessing the Internet at least in the US Cellular networks have become the main way citizens connect to the net worldwide specially in developing countries Thanks to the development of mobile devices and their networking capacities as well as the arrival of fast and reliable networks such as high quality connectivity is ensured everywhere Towards Eﬃcient and Scalable Content Delivery and any time The irruption of new paradigms such as IoT increased the number of connected devices sensors actuators etc which requires tures that provide higher throughput networking specially in use cases where high deﬁnition videos are involved and even new scenarios are yet to emerge Mobile Computing entails the processing and transmission of data over medium that does not constraint the interaction to speciﬁc location or ﬁxed physical link Figure depicts general overview of the MC paradigm in its current form It is the present decade that signiﬁes the eration of MC around the world although handheld devices have been widely used for around two decades in the form of Personal Digital Assistants PDAs and early smartphones Almost ubiquitous coverage and rapid extension of around active subscriptions per inhabitants in Europe and America provide undisrupted connectivity for mobile devices whereas of the world s population is reported to own cellular subscription in Moreover the MC paradigm is nowadays further combined with other nant technology schemes leading to the paradigms of Mobile Cloud Computing Mobile Edge Computing Anticipatory Mobile Computing etc Today s mobile devices include smartphones wearables carputers tablet PCs and They are not considered as mere communication devices as they are in their majority equipped with sensors that can monitor user s location activity and social context Thus they foster the collection of Big Data by allowing the recording and extension of the human senses Mobile social networking involves the interactions between users with similar interests or objectives through their mobile devices within virtual social works Recommendation of interesting groups based on common patterns display of multimedia content associated to nearby places as well as automatic exchange of data among mobile devices by inferring trust from social relationships are among the possible mobile social applications eﬁting from location and place information Industrial Applications Maintenance service optimization of distributed plant operations is achieved through several distributed control points so that risk is reduced and the reliability of massive industrial systems is improved Automotive Applications Automotive applications capture data from sensors embedded in the road that cooperate with sensors They aim at weather adaptive lighting in street lights monitoring of parking spaces ability promotion of driving as well as accident avoidance through warning messages and diversions according to climate conditions and traﬃc congestion Applications can promote massive vehicle data recording stolen vehicle recovery automatic crash notiﬁcation etc Retail Applications Retail applications include among many others the monitoring of storage conditions along the supply chain the automation of restocking process as well as advising according to customer habits and erences Kilanioti et g n i t n u c c n i t z i r h t u n i t c i t n h t u n i t t S r v i c s n r T s B t n g m H S T B H d u l C B d u l C t l d u l C s b t D H s b t D H s r v r S t n i P s s c c t i l l t S n i t c i l p p m t s u C S T B k r w t N l i b M r c h t l H s r v r S t n i P s s c c t i l l t S c r m m C S T B B k r w t N l i b M s i t i C t r m S t l d u l C t n i P s s c c i F i W g n i m G c i v d l i b M n h p t r m S c i v D s s l r i W n h p t r m S n i t c ﬁ i t N h s u P s n i t c i l p p s c i v D s c i t l n r v r S s s l r i W t n i P s s c c i F i W g n i t u p m c l i b m r v s m r t s t d l i c S g i F Towards Eﬃcient and Scalable Content Delivery Healthcare Telemedicine Applications Physical condition monitoring for patients and the elderly control of conditions inside freezers storing vaccines medicines and organic elements as well as more convenient access for people in remote locations with usage of telemedicine stations Building Management Applications Video surveillance monitoring of energy usage and building security optimization of space in conference rooms and workdesks Energy Applications Applications that utilize assets optimize processes and reduce risks in the energy supply chain Energy consumption monitoring and management monitoring and optimization of performance in solar energy plants Smart homes Cities Applications Monitoring of vibrations and material conditions in buildings bridges and historical monuments urban noise toring measuring of electromagnetic ﬁelds monitoring of vehicles and trian numbers to optimize driving and walking routes waste management Embedded Mobile Applications Applications for recommendation of ing groups based on common patterns infotainment and matic exchange of data among mobile devices by inferring trust from social relationships Visual eﬀects streaming workﬂow will give users access to visual eﬀects tools that can be accessed via web given enough bandwidth to maintain connection for streaming the User Interface from the cloud Video Game streaming workﬂow will give ers the option of streaming content that requires interaction between the game controller and the graphics on the TV screen Technology Applications Hardware manufacture among many others is improved by applications measuring peformance and predicting maintenance needs of the hardware production chain Roadmap Our chapter is organized as follows Sect discusses existent surveys concerning modelling simulation and performance evaluation in the examined bibliographical ﬁeld The association of computing with social networks is given in Sect Infrastructure eﬃciency of deployed data centers for the distribution of social content is analyzed in Sect in terms of software solutions as well as data center scheduling frameworks Section presets egorization of most predominant models for the depiction of the information diﬀusion process in social network Section discusses various architectures systems and techniques for eﬃcient content distribution based on social data streams along with diverse studies that corroborate them as well as the way network infrastructure aﬀects the social data streams Section concludes and ﬁnally gives the outline of future research directions Kilanioti et Related Work In manner that resembles the utilization of social data streams Anjum et review the deployment of content delivery solutions They present challenges caused due to heterogeneity in user access patterns and the variety of contextual information such as interests and incentives of Internet Service Providers and Content Providers Furthermore Perera et survey context awareness from an IoT perspective They indicate that the technology in the IoT is expected to enable expansion of conventional content delivery systems to broader network of connected devices They systematize the collection modeling reasoning and distribution of context in relation to sensor data in work that resembles the social data harvesting in terms of volume variety and velocity The survey also addresses broad range of methods models systems applications and middleware solutions related to context awareness in the realm of IoT that could be potentially applicable to social data streams too In Kilanioti et study various experiments on modiﬁed content delivery simulation framework and compare miscellaneous policies for dynamic content delivery based on analysis of social data streams The incorporation of an dynamic mechanism becomes indispensable for content delivery services since i signiﬁcantly large proportion of Internet traﬃc results from multimedia content that is produced via online media vices and transmitted over OSNs and ii multimedia content providers such as YouTube often rely on ubiquitous content distribution infrastructures The policies presented take patterns of user activity over OSNs and exploit properties of users participating in extensive retransmissions of items over OSNs The authors proceed to incorporate diverse caching schemes of the underlying infrastructure miscellaneous policies for the handling of OSN data and various approaches that take into account the most eﬃcient timing for content ment The simulation framework introduced in serves in this study as the basis of further parameterized content delivery experimentation that exploits information transmission over OSNs and decreases replication costs by tively copying items to locations where items are bound to be consumed Downloads of large size multimedia contents are explored through several studies together with techniques that try to reduce doubled content sions using intelligent caching strategies in mobile networking The main idea is redistribution of mobile multimedia traﬃc in order to eliminate duplicated downloads of popular contents Intelligent caching strategies would enable access to popular contents from caches of nearby nodes of mobile work operator Those strategies allow content providers to reduce access delays to the requested content Many caching algorithms for content distribution already exist Eﬃcient caching strategy could enhance the energy eﬃciency of networks thus the cooperative caching architecture is presented in This strategy addressed the increasing demand for mobile multimedia and data vices in energy eﬃciency in emerging systems using content caching and distribution Towards Eﬃcient and Scalable Content Delivery We are not aware of surveys in the bibliography suggesting an holistic roach for the utilization of social data streams towards facilitation of content distribution decisions and social analysis tasks other than The diverse parameters we review in this work modelling simulation performance tion take into account decisions and considerations ing energy eﬃciency of employed data centers keeping solutions and various network approaches for applications We review combined aspects such as optimal route selection data redundancy data localization and data center optimizations Social Networks and Computing social network is network of social bindings between people Supported Cooperative Work CSCW contributed much in oﬀering advanced collaborative systems for leveraging human connections and ing human interactions in workspace environments but these systems mostly focus on interactions where connections among people tend to be formal and structured Recently however social and computing disciplines focused speciﬁcally on the design of services applications that support human social interactions and can be more informal The advancement of wireless networks as well as mobile and ubiquitous computing enabled the improvement of services by enabling social encounters between proximate users with common interests in an anywhere and anytime fashion as in Ubiquitous Computing systems Thus there been shift of the application focus from virtual to physical social spaces using ubiquitous technologies This shift introduces great number of possibilities however it also introduces number of challenges that are related to ubiquitous computing While systems for ubiquitous computing environments are an emerging trend in social computing due to the fact that environments are more dynamic and heterogeneous than Internet based environments appropriate solutions and design guidelines are required to facilitate their ubiquitous aspect Ubiquitous Computing ﬁrst introduced in the nineties refers to the ing of the computing paradigm from the desktop Personal Computer PC to more distributed and embedded form of computing Together with sive Computing for many these terms are synonymous Ubiquitous Computing introduced the concept of anywhere anytime computing allowing users to interact with computers embedded in objects in an anywhere and anytime manner Ubiquitous Computing speciﬁes also that the interaction of users with such devices must be straightforward to the degree that the user would not be aware of such an interaction Thus in order for ubiquitous and pervasiveness to be achieved computers must disappear from the be embedded to common objects that humans use daily and provide computational and informational services without expecting from users to explicitly and sciously interact with them Kilanioti et Challenges in Ubiquitous Computing can be categorized to Want and ing i power management issues refers to how mobile devices deal with processing power and storage space and the kind of wireless technology to use in every given situation ii limitations in connecting devices this issue to do with how all these small devices will be connected and managed iii user face issues since Ubiquitous Computing demands for many diﬀerent devices of various types of interfaces and displays of various sizes the challenge in user interfaces lies in developing user friendly and interactive interfaces to the level where users will be motivated in using them iv issues related to Location Aware Computing Henricksen et add to the above list the challenge of managing heterogeneous devices of diﬀerent hardware and software cations such as sensors and actuators embedded devices in objects such as shoes home and oﬃce appliances such as videos mobile devices and traditional desktop computers in order for these devices to interact seamlessly Another challenge they mention to do with maintaining network connections while devices move between networks of diﬀerent nature and characteristics In uitous environments people tend to use many devices simultaneously therefore there is need for these devices to communicate and exchange data Another challenge Satyanarayanan notes is tracking user intentions This is tant in Pervasive Computing in order for the system to understand what system actions could help the user and not hinder An important challenge on is to build tems that detect and manipulate the context in manner making decisions proactively based on the context and provoke actions based on those decisions that assist the user through task the aforementioned should be done without any user participation or disturbance except maybe in case of emergency Another important issue is obtaining contextual information textual information can be any information related to the user the computing system the environment of the user and any other relevant information regarding the interaction of the user and the system User s personal computing space can be used as the user s context any information regarding the user taken from her personal proﬁle calendars lists etc various types of context can be sensed in real time like location people and objects nearby while tual parameters could also include the current emotional and physiological state of the user Contextual challenges also include the way context is represented ontologies can be used or other context modeling techniques the way this mation is to be combined with the system information as well as how frequently should context information be considered Hinze and Buchanan tiate the static context from the ﬂuent context An example of static context is users proﬁle information while ﬂuent context is dynamic context time The authors propose that context model should be deﬁned for each important entity such as the user the locations etc The authors mention as challenges the capturing of the context whether it should be done automatically at particular times or manually by the user and the process of storing the text whether it should be stored on the client on the server or both On the Towards Eﬃcient and Scalable Content Delivery process of accessing contextual information Hinze and Buchanan propose that can help in reducing the amount of data to be accessed in real time by any relevant data the static context to increase eﬃciency User modelling in another challenge in developing ubiquitous systems User modeling in ubiquitous environments is challenging user often changes roles depending on the context and the current environment acts into the big challenge is how to capture these changes and how to react on them Perhaps one of the most important contextual parameters is location as it plays an important role in systems and ubiquitous systems An issue with location as contextual parameter is the type of location sensing technology to be used while privacy is another issue The issue with privacy is whether user privacy should be sacriﬁced for location awareness and to what extent third issue is the semantic and contextual representation of the tion in order to utilize more contextual parameters than just the location itself For example by semantically representing locations one can attach to them various information resources such as webpage user proﬁle various objects with semantic representation etc Schilit et proposed the movement from the simpliﬁed concept of location to more contextually rich notions of place where people and activities should also be considered Possible problems towards this concept include the diﬃculty in managing large scale positioning data privacy concerns regarding and the challenge of how to associate information objects such as web page with location vacy issues regarding are related to human psychology users often consider privacy issues when their location is to be known by system but at the same time they provide private information such as credit card numbers and addresses to online systems without hesitation This happens because in the ﬁrst case they simply do not see the beneﬁt of providing their location to be used by simple application ﬁnding friends in the proximity while at the latter case they clearly see the beneﬁt of buying goods online The authors also argue that the centralized nature of the most location tracking applications having central server on which all user personal data are stored discourages users from providing any personalized information because centralized data can be accessed by anyone not only illegally hackers but also the government corporations with interest in user data advertisers etc solution can be the use of decentralized schema where any personal data is stored and lated on the client side the user s device An example of such technology is the well known Global Positioning System GPS the client device uses satellite links to calculate locally the user s current position and Adaptation related challenges and issues include Modelling the context which method is more appropriate to use Observing the context automatically or manually Context sensing how are contextual parameters retrieved sensors user proﬁles In retrieving context data from various sources sensors how are inconsistencies between these data resolved Kilanioti et Accuracy of contextual information should be well known during the design of ubiquitous systems Storing the context on server privacy issues on client or on both Accessing the context Using the context How are the user and the environment connected and interact How will the application modify its behaviour be adapted based on the context Systems should be more than just the location place is more than location also Location related challenge Devices should not operate based only on their own context but based on the context of the whole system Contextual information should be used to reduce the amount of input that is needed from users also Interaction related challenge How to capture changes in the user s role deals with capturing the current context the environment and the various circumstances and user modelling what possible role could person play according to context Context should be processed and various components should adapt to it without interfering with user s task user explicit interaction should be necessary Adaptation in ubiquitous environments may need to adopt various devices separately and at the same time while the user maintains tent view for the computing evolved over time from desktop applications web applications mobile computing computing to IoT over the last decade computing became more popular with the introduction of the term ubiquitous computing by Mark Weiser while the term was ﬁrst used by Schilit and Theimer in computing proven to be successful in understanding sensor data Advances in sensor technology led to more powerful cheaper and smaller sensors The number of employed sensors is expected to grow over the next decade generating ultimately big data In settings where social communities become mobile users not only act meet and communicate via social networks but are mobile as well move into the environment interact with others etc the concept of group awareness is met where context related to the group is exploited to enable ubiquitous applications and services to function and serve people s concerns and needs in pervasive manner There is need thus for formulating dynamic communities aiming to facilitate people in performing common tasks It is often the case that such dynamic communities are resolved after the current goals have been achieved It is evident thus that the context within which such dynamic communities are created act achieve goals and are then resolved is important and that through this context we can understand the groups ests and thus personalize the applications and services oﬀered Towards Eﬃcient and Scalable Content Delivery bibliography study on mobile social network applications and forms states that the context features that these applications and platforms use can be summarized as follows Location Interest Time Personal Activity and Social Interaction Here context is any information that can be used to acterize the situation of an entity and social context is the information relevant to the characterization of situation that inﬂuences the interactions of one user with one or more other users Moreover in Mobile Social Network model is proposed aiming to facilitate the creation of dynamic social networks based on combination of multiple contexts including location users proﬁle domain speciﬁc data and OSN data along with services for fostering the interaction among users Infrastructure Eﬃciency Software Solutions for Infrastructure Eﬃciency Regarding infrastructure eﬃciency various models have been proposed for the optimization of such infrastructures that support social networks data centers These approaches have also been proposed by industry partners addressing ious challenges Among these challenges the following have been identiﬁed data volume increase b conﬁnement c interactions made in or near real time d federation of data center infrastructures and between data centers but usually not focused on cost tiveness One of the main data center costs is energy consumption of both the IT equipment as well as the supporting Mechanical and Electrical M tructure widely used indicator that measures the energy eﬀectiveness of the M infrastructure overhead is Power Usage Eﬀectiveness PUE which is calculated as the Total facility IT energy and theoretical minimum of Figure shows the categories on which the research community and other stakeholders have developed solutions for the improvement of costs and ciency Workload Consolidation and Techniques VM Consolidation and Migration Data Replication and Placement and Scheduling Algorithms The main objective based on these solutions is to reduce the idleness of computing and storage nodes throttle resources while switching oﬀ unused machines without jeopardizing Service Level Agreements Some representative examples from each category are shown in Table Regarding Workload Consolidation and Techniques heuristics for consolidation of jobs and maximization of resource utilization are presented in These approaches estimate resource tion in terms of CPU utilized by tasks and encourage resources to execute tiple tasks in parallel The proposal from is an algorithm that search for Kilanioti et minimum function taking into account and running time by combining resource allocation and heuristic rules and simulating Directed Acyclic Graph DAG based workloads One of the most popular approaches to increase eﬃciency is to switch oﬀ idle servers which is usually tested in dedicated simulation tools Several models including games theory models are used to balance opposite requirements in data centers such as performance and energy consumption Even models utilized for economic environments such as Data Envelopment Analysis are employed to analyze the eﬃciency in various realistic data centers and propose corrections to improve eﬃciency In addition techniques for energy conservation like Virtual Machine VM Migration and Consolidation are widely studied and already employed In resource manager solution focused on virtualized data centers which enables lower energy consumption by applying VM migrations and allocations based on current CPU usage is proposed An extension of VM migration is sented in where Service Level Agreement SLA restrictions are considered Allocation and migration of VMs is also the target in where Bayesian Belief network algorithm is presented Moreover pattern is taken into account for an energy manager in based on the aggregation of traﬃc during low usage periods and shutting down idle machines Solutions for improving energy proportionality through Data Replication and Placement are also available distributed ﬁle system approach that tries to store data on subsets of machines is presented in Such subsets of machines contain only one copy of each ﬁle and administrators can decide how many subsets will be turned on to serve incoming requests On the other hand division of the cluster in zones is proposed in enabling operators to shut down zones In similar way in the authors present variation of the Hadoop File System HDFS that divides the cluster in Hot Zones that store recent data and Cold Zones where low spatial or temporal popularity ﬁles are stored Then power oﬀ policy is applied to the Cold Zones replica placement on data popularity is also presented in Scheduling Algorithms is the last family of solutions green scheduling algorithm based on neural networks is proposed by focusing on the prediction of workload demand with the purpose of applying policies to idle servers Experiments presented simulate medium sized data center that runs homogeneous workload that is intended to respond to user requests scheduling policies combined with Dynamic Voltage and Frequency Scaling DVFS is presented in In scheduling algorithm is proposed based on genetic algorithms which takes into account energy eﬃciency performance and security constraints We have classiﬁed the related word under consideration in terms of their ﬁnal objective including modelling b simulation c performance Such classiﬁcation is shown in Table Towards Eﬃcient and Scalable Content Delivery Table Related work summary Ref Title Performance evaluation of green scheduling algorithm for energy savings in cloud computing Savings Category Workload consolidation and power oﬀ policies power oﬀ policy based on neural network predictor nodes cluster simulation End user homogeneous requests that follow pattern Evaluation Workload Ref Title Energy eﬃcient utilization of resources in cloud computing systems Savings Category Workload consolidation and power oﬀ policies task consolidation heuristic based on diﬀerent cost functions Evaluation Simulation of not stated size cluster Workload Synthetic workload in terms of number of tasks inter arrival time and resource usage Ref Title Saving energy in data center infrastructures Savings Category Workload consolidation and techniques safety Evaluation Workload margin policy and nodes cluster simulation Synthetic workload that follows pattern Ref Title Energy eﬃcient resource management in virtualized cloud data centers Savings Category Evaluation Workload VM consolidation and migration nodes cluster simulation using CloudSim Synthetic workload that simulates services that fulﬁll the capacity of the cluster Ref Title dynamic scheduling for parallel Savings application in cloud computing Category scheduling algorithms and scheduling algorithm for DAG jobs Evaluation Experimentation on nodes cluster Workload Synthetic directed acyclic workload Data Center Scheduling Frameworks Resource managers have direct impact on the eﬃciency of the infrastructure since they are responsible for the application of scheduling models The responsibility for actually deciding resource negotiation and tasks deployment have range from traditional approaches to fully managed solutions such as data centers which are used by many entities with multiple users and various kind of applications and requirements Kilanioti et Table Classiﬁcation of approaches according to their objective The green color represents that the work focuses strongly on that objective and the red color represents opposite The constraints imposed by diverse applications in terms of size and duration may lead to various resource eﬃciency latency rates and security levels Current trends aim to utilize the same hardware resources to deploy various kind of applications and frameworks with diverse requirements which increases the complexity since diverse data are to be processed We present the main categories of the scheduling els following several approaches and we show their limitations summarized in Table Monolithic models where centralized manager is responsible for all ing and resource managing decisions came ﬁrst Such models are good choice when the workload is composed of relative low number of Batch jobs due to these schedulers being omniscient since such kind of workload does not usually have strict latency requirements Monolithic resource managers perform scheduling operations as they are able to pletely examine the data center This detailed inspection allows the nation in terms of performance implications and impact on shared resources Due to this detailed cluster inspection monolithic ized schedulers usually utilize resources at higher level than other approaches Monolithic centralized schedulers also achieve decisions which result in shorter makespans load balancing and predictable formance and availability Towards Eﬃcient and Scalable Content Delivery Table Cluster scheduling approaches Frameworks Strategy Optimal environment Paragon Centralized Low number of and sensitive jobs Quasar Monolithic environments Mid and high number of jobs Mixed workloads Borg YARN Mesos Omega Centralized Mid number of diverse non latency sensitive jobs workloads Centralized Mid number of heterogeneous workloads High number of short and jobs Canary Distributed High number of short non jobs Tarcil Sparrow Mercury Hybrid Hawk Eagle Mixed workloads composed of of short jobs and of jobs Long jobs Mixed workloads Homogeneous workloads Other workloads patterns Evolving patterns With the arrival of new computation paradigms such as microservices rent trends tend to divide jobs into smaller parts which usually are more sensitive This new scenario with huge amounts of small jobs overcome the ity of Monolithic models Two new centralized resource managing models were proposed to overcome this limitation by dividing the responsibility of resource managing and scheduling resource managers such as Mesos and YARN employ central resource manager which coordinates set of independent ulers The parallel schedulers pessimistically block the data center in order to make scheduling decision Such manager oﬀers resource schedulers and as response the set of schedulers perform scheduling decisions for deciding which machines will execute particular task The down side of this model is that opposed to Monolithic models the schedulers are not omniscient In this model state and tasks requirements are not always available to make optimal scheduling decisions resource managers such as Omega employ centralized manager which orchestrates set of parallel scheduling agents In contrast to resource managers each scheduling agent makes scheduling sions based on partially copy of the whole state Kilanioti et Instead of blocking the data center to apply their decisions they follow transactional approach If transaction ends up in conﬂict the state is requested and the scheduling restarts However all the aforementioned proposals suﬀer from performance tlenecks when huge workloads composed of millions of tasks are under consideration as they employ centralized coordinators for resource managing or even for resource managing as well as scheduling Distributed schedulers such as Sparrow and Canary are built to work optimally when the aforementioned scenarios are considered Distributed models employ faster and simpler algorithms in order to analyze smaller areas of the data center which leads to decisions with higher throughput and lower latency rates The frameworks and applications served by data centers are constantly ing Current trends show that in most cases heterogeneous workloads are being deployed in realistic clusters Such workloads are composed by two main kinds of jobs Jobs such as web servers and works which represent of jobs These jobs consume however more than of computing resources because they run for long periods and b Jobs such as MapReduce tasks which represent of jobs These jobs run for shorter periods and consume less than of computing resources In such environment scheduling operations may severely impact on the aforementioned large jobs Hence distributed models may achieve worse results in terms of performance compared to those achieved by centralized models Finally hybrid models such as Hawk and Mercury were oped to work well under the aforementioned scenario These models employ centralized and distributed approaches in order to overcome the limitations cussed Hybrid models use centralized scheduler for jobs to vide scheduling and on the other hand they employ distributed approach for those short jobs which need quick scheduling to achieve latency goals Beyond workload consolidating and resource throttling other research explored the impact IT hardware refresh and optimization could have on data centre energy consumption Additionally it was shown that addressing energy eﬃciency at the design stage of software systems presents signiﬁcant opportunity to reduce infrastructure energy consumption Content Diﬀusion Models for Social Data Streams This section outlines the most predominant models for the depiction of the diﬀusion process in social network described in Most of the existent algorithmic solutions for content distribution are built on them thus the assumption that content circulation over social data streams is depicted by one of them is of crucial importance for the suggested solutions The main algorithmic problems studied in the bibliography are related with the discovery of nodes that are most prone to diﬀuse content to the greatest extent and the Towards Eﬃcient and Scalable Content Delivery categorization of nodes according to their inﬂuence degree The categorization of the models is depicted in Fig The models presented are the most recent in the bibliography and there are prior recent models to the best of our edge The discrimination of models is based on whether they take the structure of the network into consideration or not tic In other words the discrimination criterion is if they incorporate knowledge about underlying associations of the nodes edges or to the contrary follow an approach Information Diﬀusion Models Classiﬁcation Li et in classify tion diﬀusion issues as issue that is with regard to What Why and Where They consider What to refer to the question what latent tion is there to be found in social networks and they provide as an example the ﬁndings such as the way that an individual s shopping habits relate to profession that can be included in large volume of consumer data more their proposal considers Why to refer to the question why the information propagated in this way This question refers to the factors that have aﬀected the diﬀusion result for example the factors that have produced particular social cascade Finally in their view Where refers to the question where will the information be diﬀused to in the future This question refers to the future diﬀusion path that will be followed For example if two inﬂuential users receive the same information from common contact in social network but have diﬀerent perspective on the information then it is important to mate how they will respond and whether they will propagate the information through the network Based on the issue they classify information diﬀusion models as tive and explanatory Explanatory models aim to discover answers to important questions concerning the information diﬀusion process such the determination of the main factors that aﬀect information diﬀusion and the most inﬂuential nodes in the network Predictive models on the other hand are used to predict the future information diﬀusion process in social networks based on certain factors for example the quality of information diﬀused In similar manner Luu et in classify information diﬀusion models in and network diﬀusion models The former refers to user nities without any knowledge about the user relationship network and the latter is more applicable to the social networks where user relationships networks are given Facebook blog networks For each model category Luu et describe representative models notable model extensions as well as model applications The surveyed applications include Inﬂuence Maximization and Contamination Minimization Model extensions are asynchronous models that incorporate time delay factors into the basic models An important class of information diﬀusion models are inspired from natural and biological systems typical example is provided by Dewi and Kim who propose model for information diﬀusion in complex networks using ant colony optimization The model introduces selﬁshness in forwarder nodes and unacquainted nodes and employs ant colony optimization to ﬁnd shortest path Kilanioti et and manage the selﬁsh nodes and disjoined nodes The authors provide tion results in two types of networks lattice networks and scale free networks and the results show that the model higher performance and higher reachability than selected baseline epidemic model Inﬂuence Maximization Inﬂuence maximization is an important issue in social network analysis domain which concerns ﬁnding the most inﬂuential nodes in social network Determining the inﬂuential nodes is made with respect to information diﬀusion models and is based on the observation that most of the existing models only contain trust relationships In this respect et in classify inﬂuence maximization models in two classes based and They evaluate all models in comparison with selected benchmark models through two real data sets the Epinions and Bitcoin OTC Based on the evaluation results main conclusion is drawn when distrusted user performs an action or adopts an opinion the target users may tend not to do it The eﬃciency of inﬂuence maximization algorithms is subject to active research since the problem is known to be In this respect Kempe et proposed greedy algorithm referred to as SimpleGreedy that guarantees inﬂuence spread of its optimal solution Along this line Ko et in propose an improved algorithm termed which by combines PBIM Path Based Inﬂuence Maximization and Community Based Inﬂuence Maximization Ko et further provide evaluation results from extensive iments with four datasets They show that achieves great improvement up to times in performance over methods and ﬁnds the seed set that provides the inﬂuence spread very close to that of the methods Holistic View Models Rogers theory is quantiﬁed by the Bass model The Bass model is based on the notion that the probability of adopting by those who have not yet adopted is linear function of those who had previously adopted It predicts the number of adopters n t N of an innovation at time t in the information diﬀusion scenario the number of retransmitters of an information piece n t pM q p N t N t where N t is the cumulative number of adopters by time t M is the potential market the ultimate number of adopters p is the coeﬃcient of tion the external inﬂuences expressing the individuals inﬂuenced by the mass media and q is the coeﬃcient of imitation internal inﬂuence expressing the individuals inﬂuenced by the early adopters This approach however largely ignores the underlying network structure Towards Eﬃcient and Scalable Content Delivery Models under the same concept of holistic view of the social behaviour make use of diﬀerential equations and include among others the ﬂow model by Katz and Lazarsfeld the rumours model and also more recent ones such as the Van den Bulte and Joshi model of inﬂuentials and imitators Fig diﬀusion models Models These include completely novel models but also variations of the mentioned holistic models such as the Nekovee variation of the Kendall model and are separated in following categories based on whether they are mathematically formulated Analytical models and then applied or are the outcome of empirical methods such as regression regression trees etc Empirical models Analytical Models The ﬁrst mathematical models based on nodes thresholds for the depiction of information diﬀusion were developed by Schelling and Granovetter categorization of the most predominant models is presented Models In Kleinberg proposes simple networked dination games model The author assumes that there are two behaviours node v V in the graph G V can follow and B The model is based on the notion that for each individual the beneﬁts of adopting new behaviour increase as more of its neighbours adopt the new behaviour At discrete time steps each node updates its choice of or B according to the behaviour of its neighbours The objective of the nodes is to switch each time to the behaviour that reaps the maximum beneﬁt for them For the nodes v and w there is motivation for Kilanioti et behaviour matching expressed in the following way where parameter q is real number q if v and w both choose behaviour they both receive q payoﬀ if v and w both choose behaviour B they both receive q payoﬀ if v and w choose diﬀerent behaviours they both receive payoﬀ v s payoﬀ for choosing is qdA v and for choosing B is q dB v The overall payoﬀ for v playing the game with its neighbours in G is the sum of the individual pairwise payoﬀs q is actually the threshold expressing the fraction of adopting neighbours since it easily results that v should adopt behaviour B if dB v qdv and if dB v qdv where dv is the degree of the node dA v the number of its neighbours with behaviour and dB v the number of its neighbours with behaviour B Initially there is set S of nodes adopting behaviour B and hq S is the set of nodes adopting B after one round of updating with threshold hk q S is the set of nodes adopting B after k successive rounds set S is contagious with respect to hq if new behaviour originating at S eventually spreads to the full set of nodes and the contagion threshold of social network G is the maximum q for which there exists ﬁnite contagious set The technical issue of progressive or processes monotonous or as referred to later on in the present study refers to the fact that when node v following till then the behaviour updates to behaviour B in time step t it will be following B in all subsequent time steps Although intuitively we would expect progressive processes to give ﬁnite contagious sets more easily because of lack of early adopters setbacks that would hinder the cascade Kleinberg points out that both the progressive and models have the same contagion thresholds which in both cases is at most behaviour can t spread very far if it requires strict majority of your friends to adopt it More models can be found in the work of Arthur who proposes simple cascade model of sequential decisions with positive ties manifested by term that adds to the payoﬀ of decision Namely in the scenario of two competing products the latter become more valuable as they are used by more users for social media site or smartphone for example it will aquire better applications and support as its users grow Also models are introduced by Banerjee and Bikhchandani et that are based on inﬂuence not due to positive externalities but because of information conveyed from earlier decisions The proposed els however have the drawback of not taking heterogeneity into consideration in the notion that all nodes have the same threshold and all their neighbours contribute the same in making node change its behaviour Models Combining nodes private information and their tions of earlier adoptions in Kleinberg and Easley present Bayes based model to formulate information cascades answering questions such as What is Towards Eﬃcient and Scalable Content Delivery the probability this is the best restaurant given the reviews I have read and the crowds I see there P r P r P r P r B Three factors are taken into consideration The states of the world Payoﬀs and Signals The ﬁrst factor expresses whether an option is good or bad if new restaurant is good or bad choice Supposing that the two options of the world are K the option is good idea and B the option is bad idea the world is placed in K with probability p and in B with probability p P r K p P r B P r K p Payoﬀs for node v are deﬁned as follows If v rejects the option the payoﬀ is If v adopts good idea it receives positive vg payoﬀ If v adopts bad idea it receives negative vb payoﬀ If v adopts without any prior knowledge the payoﬀ is and P r q The signals refer to private information each individual gets about the beneﬁt or not of decision high signal H suggests that adoption is good idea whereas low signal L suggests that it is bad idea If accepting is indeed good idea then P r q In the restaurant example the private information could be review that an individual reads about the ﬁrst restaurant with high signal corresponding to review comparing it favorably to restaurant B If choosing the ﬁrst restaurant is indeed good there should be higher number of such reviews so P r q Kleinberg and Easley consider how individual decisions are made using Eq when they get sequence of independently generated signals consisting of number of high signals and number of low signals thus making interesting observations about situations where individuals can observe others earlier decision but do not have access to their knowledge The basic propagation models on which most generalizations for information diﬀusion are based are the Linear Threshold Model LTM and the Independent Cascade Model ICM with many proposed extensions LTM ICM and also proposed uniﬁcation Linear Threshold Model Based on the assumption that some node can be either active adopts new piece of information or inactive and taking into account the monotonicity assumption namely that nodes can turn from inactive to active with the pass of time but not the opposite we can say that the LTM is based on the following notion Each node v predeﬁned activation threshold θv which expresses how diﬃcult it is for the node to be inﬂuenced when its neighbors are active the weighted fraction of the Kilanioti et neighbors of node that must become active in order for node to become active and is inﬂuenced by each one of its neighbors w according to weight bvw v bvw The thresholds can be produced randomly with so that uniform distribution but some approaches investigate uniform threshold for all the nodes of the network The process takes place in discrete steps v bvw θv are gradually added and the nodes that satisfy the constraint as active to the initial set of nodes It s worth mentioning that LTM can result as modiﬁcation of the networked coordinations game referred in the previous paragraph with the diﬀerentiation of payoﬀs for diﬀerent pairs of nodes LTM expresses the idea that the inﬂuence of the neighbours of node is additive but when the rule of inﬂuence can not be expressed by simple weighed sum for example node becomes active when one of its acquaintances and two of its do so the arbitrary function gv substitutes the weighed sum In the General Threshold Model for time steps t node v becomes active if the set of active neighbours at t satisfy gv X θv Independent Cascade Model Under the ICM model there is also set of initially active nodes the process takes place in discrete steps but when node v becomes active it only one chance of activating each of its inactive neighbors w until the end of the process with probability pvw independent of the activations history and with an arbitrary order Exact evaluation of activation probabilities is exponential to the number of edges of the graph Improving the performance of the works in and there are works studying the calculation of these probabilities such as based on General Threshold Model with the assumption that each parent s inﬂuence is ﬁxed or based on the ICM In the latter sampling from the twitter dataset is conducted in an eﬃcient Monte Carlo fashion using the algorithm and the problem is tackled with two diﬀerentiations one of which considering the past paths of data known retweets for the twitter dataset and one considering only the past path endpoints known hashtags and urls and joint probabilities are taken into consideration reﬂecting also model uncertainty Epidemical Models In the case of epidemical models single activated infected node causes the change of state of neighbour susceptible node whereas in the threshold and models node to interact with multiple neighbour nodes to evolve complex contagion Epidemical models were introduced on the assumption that information would propagate like diseases They constitute another category with an almost straightforward pairing with the ICM The ICM captures the notion of contagion more directly and also allows us to incorporate the idea that node s ness to inﬂuence does not depend on the past history of interactions with its neighbors Epidemical models variations include the simple branching processes model where node infects number of nodes and the contagion proceeds in subsequent waves with probability π This model is characterized by the basic reproductive Towards Eﬃcient and Scalable Content Delivery number of the disease kπ where k is the number of new people somebody meets which expresses the anticipated number of new cases of the disease that single node will cause Extensions of the epidemical models are the SIR SIS and SIRS models S stands for susceptible nodes nodes that have not been infected yet and have immunity to the contagion I stands for infected nodes nodes contagious to their susceptible neighbours and R stands for recovered nodes with the recovery considered as permanent in SIR and temporary in the case of SIRS The sequence of the letters in the acronyms of the models explains the ﬂow of the epidemic In SIR model nodes pass from the state of being susceptible to the state of being infected and then recover In SIS model nodes are immediately susceptible once they have recovered like in the case of common cold recovery does not imply immunity that lasts for long In the SIRS model recovered nodes free of infection may rejoin the susceptible nodes Markov Chain Models Markov chains are used to describe transitions from one state of system to another in ﬁnite set of possible states Their oryless nature Markov property to do with the fact that the next state each time is independent of the preceding states More formally With set of states ξr the process moves successively from one state to another in steps and speciﬁcally from state ξi to state ξj with probability pij transition probability independent of the previous states of the chains or remains in the same state with probability pii particular state is picked from Ξ as the initial state Markov chains are usually depicted with directed graph where the edges labels denote the transition probabilities Markov models are widely used for analysing the web navigation of users PageRank is based on Markov model and is used for ranking of mation in the World Wide Web By assigning weights that denote the relative importance of an hyperlinked document in set of documents the likelihood that person will reach speciﬁc page through random clicks is essentially represented In Song et use Markov Chain Model CTMC namely Markov model that describes the transition among states after some time of stay in particular state This time is exponentially distributed and does not aﬀect the transition probability to the next state The information diﬀusion model is introduced on network G V w τ G contains set V of n nodes and edges between nodes representing the information diﬀusion paths w denotes the set of the edges weights amount of information to ﬂow from one node to another and τ the set of the time delay on the information diﬀusion paths Thus the representation of the graph matches the CTMC in the notion that each node represents state each weight transition probability and the delay is represented as the in each state Voter Model The basic voter model introduced by Cliﬀord and Sudbury and Holley and Liggett is deﬁned in an undirected network and allows the spread of two opinions In discrete time steps node adopts the opinion of Kilanioti et randomly chosen neighbour For node v V in graph G V Γ v is the set of neighbors of v in G and initially the nodes are arbitrarily endowed with state At time step t each node adopts the opinion of one uniformly picked neighbour With an initial assignment V inductively we deﬁne v with probability with probability b where v ft u v and b v ft u v and Shapira argue that it is one of the most natural abilistic models to capture the information diﬀusion in social network It is suitable for depicting the spread of technological product as it is proved that under this model consensus is reached with probability and Shapira refer to the almost consensus of products such as Google as search engine YouTube as website etc Models from Physics Models from physics include the Ising model serving for the description of magnetic systems and bootstrap percolation serving for the description of magnetic systems neuronal activity glassy dynamics etc The Ising model was ﬁrst proposed in statistical physics and encompasses the notion of ground state in physics the state with the minimum energy and that of the nature of the network Similarly to the basic voter model there can be two competing opinions in favour of or against subject let s say depicted by and which in physics express the correspondence of an atom forming network to spin variable can be considered as the basic unit of magnetization state σi The total energy of the system under this model Hamiltonian is deﬁned as H H σ Eσiσj Jσi i j i for each conﬁguration σ σN with the parameter J associated with an external magnetic ﬁeld and with the interaction N the number of the atoms The ground state is the lowest energy conﬁguration sg in physics the zero temperature conﬁguration so that sg argminsH s In social network can be seen as the state with the most likely opinion minimizing conﬂicts among its members atoms In the standard bootstrap percolation process node is initially either active with given probability f or inactive It becomes active if k k of its nearest neighbors are active In that notion it resembles the problem of random graphs where is the maximal subgraph within which all vertices have at least k neighbors but whereas bootstrap percolation starts from subset of seed vertices according to the activation rule the of the network can be found by subsequent pruning of vertices which have less than k neighbors Towards Eﬃcient and Scalable Content Delivery Empirical Models Before the advent of traces the potential of networks in the transmission of information and messages was stated already by Milgram in his renowned experiment or Christakis who suggested in study of participants that risks such as the risk of becoming obese or beneﬁts such as stopping of smoking are propagated through social ties However it is large scale and traces that through the track of interactions in OSNs although not compulsorily easily collectible have driven to the formulation of plethora of empirical models Some generic observations concerning the empirical models are the following Many of them lack insight of information content unlike works such as that of Huberman et who formulate model taking into consideration solely the features of an information item news item in Twitter Sometimes the discovered patterns in empirical models are at odds with the predictions based on theoretical analytical models For example in unison with the epidemical model Leskovec et in claim that cascades depicting the blogosphere information diﬀusion are mostly More speciﬁcally they notice that the number of edges in the cascade increases almost linearly with the number of nodes suggesting that the average degree in the cascade remains constant as the cascade grows trees property Moreover Leskovec et claim that these trees are balanced as they notice that the cascade diameter increases logarithmically with the size of the cascade In contradiction to the above the trees derived from the diﬀusion model of and Kleinberg in are inconsistent with the epidemic model as they are very narrow and deep with the majority of their nodes having one child and median distance from their root to the their leaves being of hundreds steps Precisely in the spread of is represented by tree Copies of the represent paths through the tree the root represents the originator and the leaves represent the recipients of message w is child of v if w appends its name in the copy of the letter directly below v In order to produce trees with the characteristics mentioned in the previous paragraph the probabilistic model suggested i incorporates asynchrony after receiving message each recipient waits for time t before acting on it and if it receives more copies of the item in this time interval it acts upon only one of them and ii encompasses β as node can either forward the message to its neighbours with probability β or to his corecipients with probability β In Bakshy et attempt to model the information diﬀusion in Twitter with the use of regression trees Twitter is convenient for information diﬀusion modeling since it is explicitly users subscribe to the content of other users The retweet feature moreover helps in the acknowledgement though does not guarantee it of reposts Seeders are users posting original not retweeted content and reposting instead of the conventional retweeting RT username is taken into account Inﬂuence is measured in terms of the size of the whole diﬀusion tree created and not just the plain number of explicit retweets The three diﬀerent cases studied ascribe the inﬂuence to the ﬁrst one having posted link the most recent one or follow hybrid approach Kilanioti et As far as the seed users are concerned the predictors used include the ber of followers number of friends number of tweets and date of joining and regarding the past inﬂuence of seed users the average minimum and maximum total inﬂuence and average minimum and maximum local inﬂuence local refers to the average number of reposts by user s immediate friends over period of one month and total to the average total cascade size over that period Bakshy et come to the conclusion that although large cascades have in their majority previously successful individuals with many followers as tors individuals with these characteristics are not necessarily bound to start large cascade Thus because of the fact that estimations can not be made at an individual level marketers should rely on the average performance By studying the return on investment on the whole with cost function of the number of followers per individual i ci acf ficf where is acquisition cost cf cost per follower and fi is the number of followers they conclude that relatively ordinary users of average inﬂuence and connectivity are most features are also according to Bakshy et not expected to discriminate initiators of large cascades from ones due to the large number of In order to take content into account the regression analysis is repeated encompassing the following features rated interestingness perceived interestingness to an average person rated positive feeling willingness to share via email IM Twitter Facebook or Digg some indicator variables for type of URL and some indicator variables for category of content Moreover Lerman et claim that exploiting the proximity of users in the social graph can serve as an factor for the prediction of mation diﬀusion They discriminate proximity as coming from conservative or processes denoting that the amount of spread information in the network remains or not constant respectively For the case the ing network is not fully known Najar et focus on predicting the ﬁnal activation state of the network when an initial activation is given They ﬁnd the correspondence between the initial and ﬁnal states of the network without considering the intermediate states Their work is based on the analogy between predictive and generative approaches for discrimination or regression problems predictive models depicting better performance when the real data tion can t be captured In Yang and Leskovec use time series model for modeling the global inﬂuence of node through the whole network For each node u an inﬂuence function Iu l is the number of mentions of an information l time units after the node u adopted the information at and with V t being the number of nodes that mention the information at time t it applies V t t Iu t where t are the nodes that got activated before t For the modeling of the inﬂuence functions formulation followed allows greater accuracy and deviation as assumptions are made Towards Eﬃcient and Scalable Content Delivery study of the social news aggregator Digg crawling data from the site story user and social network perspective suggests the presence of previously unconsidered factors for the steering of information spread in OSNs Doerr et suggest that beyond the bare OSN topology two factors matter the temporal alignment between user activities whether users are visiting the site in the same narrow time window and hidden logical layer of interaction patterns occurring in their majority outside the social graph In the direction of studying the information diﬀusion as social graphs evolve Ren et study the evolution steps for shortest paths between two nodes so that they can ascribe them to disjoint path bridge or new friend between them and furthermore metrics such as closeness ity and global metrics like the graph diameter across snapshots of gradually evolving graphs To this end they adopt an eﬃcient algorithm and an eﬃcient storage scheme Firstly they cluster in an incremental procedure not requiring all snapshots to be present in memory successive graphs exploiting their many resemblances daily snapshots As and essentially bound the graphs in the cluster with being the intersection the largest common subgraph of all snapshots in cluster C and the union the smallest common supergraph of all snapshots in C grouping of snapshots into clusters can be based in the idea of the graph edit similarity between these two graphs The graph edit similarity to capture the similarity requirement of cluster is deﬁned as ges Ga Gb Ga Gb Ga Gb Secondly they exploit the idea that denoting the between the vertices v and u by u v in graph where n the solution can easily be found by the intersection or union two graphs of graphs in the cluster or be ﬁxed using these two graphs and they propose framework As far as the storage schemes variations are concerned for cluster of shots C Gk the deltas Δ Gi i k consist small fraction of the snapshot and their size depends on the threshold value used for ters similarity The penalty of decompression overheads needed is surpassed by savings in Variations of the storage schemes include the following SM C Δ Δ Gi i k SM C Δ Δ D Gi i k SM F V F C D Δ Δ D Gi i k In the authors consider only the edge sets of Δ Gi and to execute their algorithms on snapshot Gi and the snapshots Gi s of the cluster need not be explicitly stored For further compression of data of an evolving graph sequence similarity of successive snapshots is exploited In D Gi Kilanioti et i i where i Gi and i Gi are the changes made to snapshot to obtain the next snapshot Gi Authors observe that the size of the set of edge changes D Gi Gi is on average just the size of Δ Gi Hence representing an EGS in terms of the D s is much more space eﬃcient than in term of the Δ s Further compression can be achieved by exploiting redundancy Distribution of Social Data Streams Content Distribution for Social Data Streams This subsection provides description of architectures systems and techniques for the distribution of social data content Architectures In Jacobson et introduce Content Centric ing CCN noting that network use evolved to be dominated by content distribution and retrieval CCN notion of host at its lowest level packet address names content not location while simultaneously preserving the design decisions that make simple robust and scalable Content is treated as primitive and with new approaches Jacobson et simultaneously achieve scalability and performance To share resources within the context of social network with the use of the cloud business model Chard et in propose the SocialCloud architecture Users register in cloud services computational capacity photo storage etc and their friends can consume and provide these services through Facebook cation The allocation of resources trading or reciprocal use between friends is conducted by an underlying market infrastructure whereas the Social Cloud application passes SLA to the service The advertisement of the service so that it can be included in the market is done with XML based metadata stored in Globus Monitoring and Discovery System MDS An interesting approach applicable to the realm of content delivery is based on an architecture which combines global learning and local caches with small population It is shown that thresholds can timely exploit popularities to improve caching performance Moreover the caching eﬃciency is maximized by combination of global learning and clustering of access locations accompanied by score mechanisms to help with practical issues at local caches Practical considerations include though the size of the content that circulates over OSN and the eﬀect since the goal of the authors is ﬁrst to learn good estimate at the global point and then feed it back to the local caches in the form of content scores thus making the approach possibly prohibitive for content delivery Systems In Buzztraq Sastry et build prototype system that takes advantage of the knowledge of the users friends location and number to ate hints for the placement of replicas closer to future accesses Comparing their Towards Eﬃcient and Scalable Content Delivery strategy with location based placement which instead uses the geographical location of recent users they ﬁnd substantial decrease of cost when requests as part of cascades are more than random accesses of content Furthermore their system reacts faster when there is new region shift since it starts counting friends of previous users in new region even before request comes from that region The key concept of Buzztraq is to place replicas of items already posted by user closer to the locations of friends anticipating future requests The intuition is that social cascades are rapidly spread through populations as social epidemics The experimental results indicated that social cascade prediction can lower the cost of user access compared to simple placement ztrack is simple system that only provides hints as to where to place objects Other more complex constraints that the present work covers such as server bandwidth and storage are not taken into account Moreover social cascade is indirectly analyzed because there to be page where users connect to view the videos and have access to their social proﬁle In the direction of distributing content while lowering bandwidth costs and improving QoS although without considering storage constraints Traverso et in exploit the time diﬀerences between sites and the access patterns that users follow Rather than naively pushing UGC immediately which may not be consumed and contribute unnecessarily to traﬃc spike in the upload link the system can follow approach where the ﬁrst friend of user in Point of Presence PoP asks for the content Moreover rather than ing content as soon as user uploads content can be pushed at the local time that is for the uplink and be downloaded in subsequent time bin also for the downlink The larger the diﬀerence is between the content production bin and the bin in which the content is likely to be read the better is the performance of the system In Scellato et study how Twitter can be used to examine social cades of UGC from YouTube and discover popular objects for replication They improve the temporary caching policy by placing content after accounting for the distance between users For the model CDN system constructed and tested Scellato et used the Limelight network properties with clusters of servers worldwide To test the system two diﬀerent video weights were used geosocial in which node locality values are calculated from all the users that have posted message about the item even without being involved in cascade and cade in which node locality values are calculated from the users participating in the item s social cascade It was shown that the model improved performance against weight policy with geocascade weight performing better Techniques The introduction of concrete uniﬁed metrics for the tion of the extent of the social dissemination local or global cascades ena is an open issue systematic incorporation of this quantiﬁed knowledge into the existent underlying content delivery infrastructure would be salutary for proactive steps towards the improvement of user experience Kilanioti et Furthermore novel techniques aim to incorporate the information extracted from OSNs in the way that users share content and in how the content ultimately reaches the users Some of these works use the information directly from OSNs whereas others use such information indirectly The research goals vary the decision for copying content improvement of policy for temporary caching etc Zhou et leverage the connection between content exchange and graphic locality using Facebook dataset they identify signiﬁcant geographic locality not only concerning the connections in the social graph but also the exchange of content and the observation that an important fraction of content is created at the edge is with web based scheme for caching using the access patterns of friends Content exchange is kept within the same Internet Service Provider ISP with component that can be deployed by existing web browsers and is independent of the type of content exchanged Browsing users online are protected with where k is the number of users connected to the same proxy and are able to view the content In Hoque and Gupta propose technique with logical addressing scheme for putting together in the disk blocks containing data from friends The large scale of OSNs and the predominant tail eﬀect do not allow use of niques such as those used in multimedia ﬁle systems or web servers where items are globally popular and techniques keeping related blocks together tracking the access pattern of blocks respectively To this purpose in the social graph is divided into communities The organization of blocks in the disk is conducted with greedy heuristic that ﬁnds layout for the users within the communities and organizes the diﬀerent communities on the disk by considering tie strength The system is implemented on top of the graph database as layout manager Instead of optimizing the performance of UGC services exploiting spatial and temporal locality in access patterns Huguenin et in show on large more than videos YouTube dataset that content locality induced by the related videos feature and geographic locality are in fact correlated More speciﬁcally they show how the geographic view distribution of video can be inferred to large extent from that of its related videos proposing UGC storage system that proactively places videos close to the expected requests Such an approach could be extended with the leverage of information from OSNs Kilanioti et in propose miscellaneous policies for dynamic content delivery over content delivery simulation framework The authors propose policies that take patterns of user activity over OSNs and exploit properties of users participating in social cascades proceed to rate various caching schemes of the underlying infrastructure diﬀerent policies for the handling of OSN data and various approaches that take into account the eﬃcient timing of prefetching Given an eﬃcient placement of surrogate servers with maximum performance and minimum infrastructure cost they apply textual features of the user as heuristics to ﬁnd the best content diﬀusion ment either in global or in local scale which content will be copied in the surrogate servers and to what extent not overlooking memory time and Towards Eﬃcient and Scalable Content Delivery computational cost Moreover they study temporal aspects of diﬀusion related to the most eﬃcient timing of the content placement The simulation framework they introduce can serve as the basis of further parameterized content delivery experimentation that exploits information transmission over OSNs and decreases replication costs by selectively copying items to locations where items are likely to be consumed In terms of performance Kilanioti et note signiﬁcant improvement over the respective improvement only for the plain Social Prefetcher roach up to for selected caching mechanisms compared to in performing better than existent methods employed by most CDNs even though these methods additionally overlook storage issues of the distributed infrastructure Last but not least of more concurrent cascades happening it would be esting to know which of them will evolve as global and which of them will evolve as local possibly making some associations with their content or text features It is challenging to discover contextual associations among the topics which are by nature implicit in the content exchanged over OSNs and spread via social cascades In other words it would be useful to derive semantic relations This way the identiﬁcation of popular topic can be conducted in higher more abstract level with the augmentation of tic annotation While the topic of single information disseminated through an OSN can be explicitly identiﬁed it is not trivial to identify reliable and eﬀective models for the adoption of topics as time evolves characterized with some useful emergent semantics Therefore eﬃcient semantic annotation can be seen as solution for the challenge of characterization of the extent of the social dissemination Content Distribution in Environments and Technologies Content became the main information item exchanged between diﬀerent actors in the Internet Video and multimedia content counts for of the total global traﬃc Rich multimedia content lead to rapid mobile traﬃc growth that current mobile radio network mobile backhaul the capacity of the wireless link and mobile core network can not support could overcome these bottlenecks introducing high increasing ratio of mobility communications and strong entation towards services and applications for content delivery over wireless technology high throughput low data delivery latency and high scalability enabling huge number of devices Environment represents the generation network of mobile systems which opens new possibilities increase radio link capacity and brings plenty of new trends such as heterogeneous networks HetNets new use cases based on connections and communications between device to device massive Communications and Internet of Things IoT evolution of radio access technologies cloudiﬁcation throughout SDN and network function ization NFV paradigms ﬂexible spectrum management cell densiﬁcation etc Kilanioti et NFV and SDN capabilities in systems are expected to enable network grammability Content delivery could be aﬀected by cloudiﬁcation through diﬀerent paradigms Programmable network control and the tualization of all the RAN elements into virtual appliances by ﬂexible NFV agement are included within networks This enable content focused resources allocation Agile design of new network functions and their control are possible Network providers could extend network with new function that includes custom designed information such as services that can oﬀer to the online media service providers The collaboration between the network provider and the online media service provider by means of the edge cache could be enabled by media delivery solutions designed for The control of the network will be kept by the network provider The network provider would give only the relevant information for the online media service provider while the online media service provider will keep the control of the delivery process and decide whether the cache shall be used what and how information or resources are cached Technologies New technologies such as WiFi and ZigBee SDN and NFV rapidly change networks and services and lead to changes to content delivery For example mobile video will generate more than of mobile data traﬃc by It is expected to witness an increase to by which is much greater from in on the share of smart devices and connections while the amount of traﬃc oﬄoaded from was at the end of and it will be percent by is also expected to witness higher oﬄoad rates when the network arrives The main challenges in wireless or mobile environment that have impact on content delivery services are reﬂected to the limited spectrum and bandwidth in wireless heterogeneous networks wireless link characteristics that are dependent on location and time radio congestion handoﬀ issues etc Use Cases Future developments is dependent on service providers technology enablers and customers All these actors are directly involved in sions which use cases to pursue ﬁrst as well what technology is needed for the use cases The standards development process is also dependent on decisions which use cases ﬁrst to deploy All these ongoing developments will directly aﬀect content delivery mechanisms models and systems architectures The main use cases currently are reﬂected to Gigabit broadband to home related to deliver streams rated from to which are needed to deliver television with higher resolution than virtual and augmented reality Speciﬁc applications require special network conﬁguration for example in order to minimize latency in virtual reality cations Next generation mobile user experience Future corporate networks addressed to better service providing which require operators to dynamically manage network and to use software deﬁned networking and network function virtualization Towards Eﬃcient and Scalable Content Delivery Digital industrial ecosystems include agriculture smart cities and healthcare applications which imply network conﬁgurations that every industry ipant can beneﬁt from Infrastructure as service approach is for service providers that lack the resources to invest in nationwide coverage Solutions and Approaches The demand for data applications been on the rise in the recent decade which led to the development of Development of eﬃcient mechanisms for supporting mobile multimedia and data services is prerequisite for networks The real bottleneck of todays mobile networks is access radio network and the backhaul Caching in the intermediate nodes servers gateways routers and mobile users devices can reduce doubled transmission from content providers and core mobile networks Known caching techniques that can be used within are content bution network networks networking http web caching evolved packet core caching radio access network caching device to device caching proactive caching predictive caching cooperative caching Those techniques are using diﬀerent algorithms and models Analysis presented in showed that the deployment of those caching techniques in mobile network can reduce redundant traﬃc in backhaul minimize the traﬃc load increase the transfer rate in mobile network and reduce the latency Correlation of several caching methods and procedures could result in improving network performance and obtaining better results brings complex heterogeneity of the network with diﬀerent gies that coexist where some technologies could totally disable transmission of data of equipment that use other technologies Solutions that eﬃciently handles resources in space frequency and device dimensions are needed One possible eﬃcient solution is semantic coordination in such networks is given in The nodes in the system can communicate and share knowledge of their spective of the spectrum utilization in the network In authors proposed to model the spectrum usage coordination as an interactive process between number of distributed communicating agents where agents share their ciﬁc information and knowledge The information includes the current spectrum usage state spatial coordinates of the device available communication cols usage policy spectrum sensing capabilities of the device spectrum needs etc Approach for such coordination presented in is based on semantic technologies and harmonize communication between heterogeneous agents with potentially diﬀerent capabilities with minimal common compliance The core knowledge is represented by ontologies whose representation and usage is iﬁed in standardized way This semantic approach can be used for wide spectrum of problems within heterogeneous networks such as network states predictions network analysis minimizing traﬃc load content distribution coordination etc This approach could be used in combination with caching techniques in order to improve content distribution in but further research should be carried out in this area Kilanioti et Conclusions This article describes the results of the collaborative work performed as part of Modelling and Simulation for Big Data Applications cHiPSet COST Action The presented case study focused on dia big data from entertainment and social media medical images consumer images voice and video that drives research and development of related nologies and applications and is steadily becoming valuable source of In fact this work describes the general mation and insights landscape and how our approach ﬁts in the general ecosystem Multimedia tent providers such as YouTube strive to eﬃciently deliver multimedia big data to large amount of users over the Internet with currently more than h of video content being uploaded to the site every minute Traditionally these content providers often rely on social data content distribution infrastructures However some measurement studies depict that signiﬁcantly large proportion of HTTP traﬃc results from multimedia content ing through OSNs Consequently the user activity extracted from OSNs can be exploited to reduce the bandwidth usage By incorporating patterns of tion transmission over OSNs into simulated content distribution ture the performance of content distribution mechanisms can be remarkably improved CDN services are increasingly being used to enable the delivery of demanding large media data to of multimedia content providers and extend the capabilities of the Internet by deploying massively distributed tructures to accelerate content delivery Next generation CDNs are being aged in an array of ways to overcome the challenges of providing seamless customer experience across multiple devices with varying connectivity and responding to the call for enterprise application delivery They have to go beyond eﬃcient resource discovery and retrieval tasks of the established CDNs and port reﬁned mechanisms for data placement replication and distribution for large variety of resource types and media formats OSNs on the other hand ate potentially transformational change in user navigation and from this angle the rapid proliferation of OSNs sites is expected to reshape the architecture and design of CDNs The challenges and opportunities highlighted in the plinary ﬁeld of content delivery are bound to foster some ing future developments including innovative cache replacement strategies as product of the systematic research of temporal structural and geographical properties of social cascades Particularly today that HTTP traﬃc ascribed to media circulating over OSNs grown an mechanism over content distribution schemes become essential This mechanism aims to exploit patterns of social interactions of the users to reduce the load on the origin server the traﬃc on the Internet and ultimately improve the user experience By addressing the issue of which content will be copied in the surrogate servers of CDN it ensures optimal content diﬀusion placement At the same time it moderates the impact on bandwidth that the Big Data transmitted via OSNs oﬀering scalable Towards Eﬃcient and Scalable Content Delivery solutions to existing CDNs or OSNs providers Furthermore it paves the way for experimentation with variations on caching schemes timing parameters of content delivery and context of the OSN and the media platform future target is to potentially leverage the CDN services of cloud vice providers in order to lower costs while increasing simplicity CDNs often operated as Software as Service SaaS in cloud providers Amazon Front Microsoft Azure CDN etc aim at addressing the problem of smooth and transparent content delivery CDN actually drives cloud adoption through enhanced performance scalability and cost reduction With the limitation for both CDNs and cloud services being the geographic distance between user ing for content and the server where the content resides cloud acceleration and CDN networks are both complementary to achieving goal of delivering data in the fastest possible way Cloud mainly handles Utilization of CDNs in cloud computing where content is constantly changing and thus not easily cached is likely to have profound eﬀects on large data download References Alexa http Accessed Dec Cisco visual networking index global mobile data traﬃc forecast update white paper Ericsson research blog media delivery https Accessed Dec Facebook Newsroom http Accessed Dec International Telecommunication Union ICT facts and ﬁgures in https Accessed Dec Internet Society Global Internet Report Mobile Evolution and ment of the Internet https Accessed Dec Management of Networks with Constrained Devices Use Cases IETF Internet Draft https Accessed Mar computing introductory technical white paper https ybrCnq Accessed Mar The Internet of Things how the next evolution of the Internet is changing thing CISCO San Jose CA USA White Paper http Accessed Mar The Smart Grid An Introduction US Department of Energy http jTNgf Accessed Mar Under the Hood Scheduling MapReduce jobs more eﬃciently with Corona book engineering Accessed Dec YouTube Statistics https Accessed Dec cHiPSet Research Work Results Grant Period http Accessed Dec Abedini Shakkottai Content caching and scheduling in wireless networks with elastic and inelastic traﬃc Trans Netw Kilanioti et Abolfazli Sanaei Ahmed Gani Buyya tation for mobile devices motivation taxonomies and open challenges IEEE Commun Surv Tutor https Aizenman Lebowitz Metastability eﬀects in bootstrap percolation Phys Math Akpakwu Silva Hancke survey on networks for the internet of things communication technologies and challenges IEEE Access Amur Cipar Gupta Ganger Kozuch Schwan Robust and ﬂexible storage In Proceedings of the ACM sium on Cloud Computing pp ACM Ananthanarayanan et PACMan coordinated memory caching for lel jobs In Proceedings of the USENIX Conference on Networked Systems Design and Implementation USENIX Association Anderson The Long Tail Why the Future of Business is Selling Less of More Hyperion Santa Clara Revised and Updated Edition Anjum Karamshuk Sastry Survey on content delivery networks Comput Netw Arthur Competing technologies increasing returns and by ical events Econ J Assila Kobbane Koutbi survey on caching in mobile network Asur Bandari Huberman The pulse of news in social media casting popularity In Association for the Advancement of Artiﬁcial Intelligence Avelar Azevedo French Power PUE comprehensive nation of the metric White Paper Bakshy Hofman Mason Watts Everyone s an inﬂuencer quantifying inﬂuence on Twitter In Proceedings of the Fourth ACM tional Conference on Web Search and Data Mining pp ACM Bakshy Hofman Mason Watts Everyone s an encer quantifying inﬂuence on Twitter In Proceedings of the Forth tional Conference on Web Search and Web Data Mining WSDM Hong Kong China February pp http Bakshy Rosenn Marlow Adamic The role of social networks in information diﬀusion In Proceedings of the World Wide Web Conference WWW Lyon France April pp http Banerjee simple model of herd behavior Econ Barroso Clidaras The datacenter as computer an duction to the design of machines Synth Lect Comput Arch Bashroush comprehensive reasoning framework for hardware refresh in data centers IEEE Trans Sustain Comput Bashroush Woods Architectural principles for applications IEEE Softw Bass new product growth for model consumer durables Manag Sci Towards Eﬃcient and Scalable Content Delivery Beloglazov Abawajy Buyya resource allocation tics for eﬃcient management of data centers for cloud computing Futur Gener Comput Syst Beloglazov Buyya Energy eﬃcient resource management in virtualized cloud data centers In Proceedings of the International Conference on Cluster Cloud and Grid Computing pp IEEE Computer Society Beloglazov Buyya Optimal online deterministic algorithms and adaptive heuristics for energy and performance eﬃcient dynamic consolidation of virtual machines in cloud data centers Concurr Comput Pract Exp Berger Dynamic monopolies of constant size Comb Theory Ser B Bhattacharya Culler Friedman Ghodsi Shenker Stoica Hierarchical scheduling for diverse datacenter workloads In Proceedings of the Annual Symposium on Cloud Computing ACM Bikhchandani Hirshleifer Welch theory of fads fashion custom and cultural change as informational cascades Polit Econ Bonacich Power and centrality family of measures Am Sociol Borgatti Centrality and network ﬂow Soc Netw http Borthakur et Apache Hadoop goes realtime at Facebook In Proceedings of the ACM SIGMOD International Conference on Management of Data pp ACM Bottazzi Montanari Toninelli middleware for anytime anywhere social networks IEEE Intell Syst Boyd Ellison Social network sites deﬁnition history and scholarship Commun https Brin Page The anatomy of hypertextual web search engine Comput Netw ISDN Syst Brodersen Scellato Wattenhofer YouTube around the world graphic popularity of videos In Proceedings of the International Conference on World Wide Web pp ACM Van den Bulte Joshi New product diﬀusion with inﬂuentials and tators Mark Sci Cha Kwak Rodriguez Ahn Moon I tube you tube body tubes analyzing the world s largest user generated content video system In Proceedings of the ACM SIGCOMM Conference on Internet Measurement San Diego California USA October pp https Chard Caton Rana Bubendorfer Social cloud cloud computing in social networks In IEEE International Conference on Cloud Computing CLOUD Miami FL USA July pp https Chen Alspaugh Katz Interactive analytical processing in big data systems study of mapreduce workloads Proc VLDB Endow Chib Greenberg Understanding the algorithm Am Stat Kilanioti et Cliﬀord Sudbury model for spatial conﬂict Biometrika Coutand et group management in mobile environments In IST Mobile Summit Daley Kendall Stochastic rumours IMA Appl Math Dean Ghemawat MapReduce simpliﬁed data processing on large clusters Commun ACM Delgado Didona Dinu Zwaenepoel scheduling in eagle divide and stick to your probes In Proceedings of the Seventh ACM Symposium on Cloud Computing Delgado Dinu Kermarrec Zwaenepoel Hawk hybrid ter scheduling In USENIX Annual Technical Conference pp Delimitrou Kozyrakis Paragon scheduling for heterogeneous datacenters ACM SIGPLAN Not Delimitrou Kozyrakis Quasar and cluster management ACM SIGPLAN Not Delimitrou Sanchez Kozyrakis Tarcil reconciling scheduling speed and quality in large shared clusters In Proceedings of the Sixth ACM Symposium on Cloud Computing pp ACM Dewi Kim new approach to modeling of information diﬀusion with ant colony optimization in complex networks In Zelinka anthan Chen Snasel Abraham eds Nostradamus Prediction Modeling and Analysis of Complex Systems AISC vol pp Springer Cham https Dey Understanding and using context Pers Ubiquit Comput Dey Abowd Salber conceptual framework and toolkit for supporting the rapid prototyping of applications Interact Dickens Molloy Lobo Cheng Russo Learning stochastic models of information ﬂow In IEEE International Conference on Data Engineering ICDE pp IEEE Dodge Cox Commenges Davison Solomon Wilson The Oxford Dictionary of Statistical Terms Oxford University Press Oxford Doerr Blenn Tang Van Mieghem Are friends overrated study for the social news aggregator Comput Commun Dogar Karagiannis Ballani Rowstron Decentralized scheduling for data center networks In ACM SIGCOMM Computer cation Review vol pp ACM Duy Sato Inoguchi Performance evaluation of green scheduling algorithm for energy savings in cloud computing In IEEE International Symposium on Parallel Distributed Processing Workshops and PhD Forum IPDPSW pp IEEE Easley Kleinberg Networks Crowds and Markets Reasoning About Highly Connected World Cambridge University Press http site GB Shapira note on maximizing the spread of inﬂuence in social networks Inf Process Lett https Towards Eﬃcient and Scalable Content Delivery Fard Abdolrashidi Ramaswamy Miller Towards eﬃcient query processing on massive graphs In International Conference on Collaborative Computing Networking Applications and Worksharing rateCom Pittsburgh PA USA October pp http Ko lodziej elberg models in cloud scheduling In Nolle L ed Proceedings of European Conference on Modelling and Simulation ECMS ECMS Wilhelmshaven Germany May ECMS pp European Council for Modelling and Simulation Dudweiler Kolodziej score cloud scheduler and simulator for tional clouds Simul Model Pract Theory http Kolodziej Toro Score and energy consumption Simul Model Pract Theory http cloud optimization of simulator resources for Ortega Energy policies for monolithic schedulers Expert Syst Appl http Velasco Productive eﬃciency of data centers Energies http Grzonka Kolodziej Security supportive scheduling and energy policies for cloud environments J Parallel Distrib Comput http Ortega Energy wasting at internet data centers due to fear Pattern Recognit Lett http Cognitive Systems for Knowledge Discovery Fowler Christakis Connected The Surprising Power of Our Social works and How they Shape Our Lives HarperCollins Publishers New York City Kang Lerman Corcho Characterising emergent semantics in Twitter lists In Proceedings of the International Conference on the Semantic Web Research and Applications ESWC Heraklion Greece Gea Paradells Lamarca Roldan Smart cities as an application of Internet of Things experiences and lessons learnt in Barcelona In Seventh International Conference on Innovative Mobile and Internet Services in tous Computing IMIS pp IEEE Goldenberg Libai Muller Talk of the network complex systems look at the underlying process of Mark Lett Govindan Liu Kansal Sivasubramaniam Cuanta quantifying eﬀects of shared resource interference for consolidated virtual machines In Proceedings of the ACM Symposium on Cloud Computing ACM Goyal Bonchi Lakshmanan Learning inﬂuence probabilities in social networks In Proceedings of the third ACM International Conference on Web Search and Data Mining pp ACM Kilanioti et Grandl Ananthanarayanan Kandula Rao Akella resource packing for cluster schedulers ACM SIGCOMM Comput Commun Rev Granovetter Threshold models of collective behavior Am Sociol Gruhl Guha Tomkins Information diﬀusion through Blogspace In Proceedings of the International Conference on World Wide Web pp ACM Henricksen Indulska Rakotonirainy Infrastructure for pervasive puting challenges In GI Jahrestagung pp Hindman et Mesos platform for resource sharing in the data center NSDI Hinze Buchanan in mobile tourist information tems challenges for user interaction In International Workshop on Context in Mobile HCI at the Seventh International Conference on Human Computer action with Mobile Devices and Services Holley Liggett Ergodic theorems for weakly interacting inﬁnite tems and the voter model Ann Probab Hoque Gupta Disk layout techniques for online social network data IEEE Internet Comput Zamanifar Assessing information diﬀusion models for inﬂuence maximization in signed social networks Expert Syst Appl Hoßfeld Schatz Biersack Plissonneau Internet video delivery in YouTube from traﬃc measurements to quality of experience In Biersack Callegari Matijasevic M eds Data Traﬃc Monitoring and Analysis LNCS vol pp Springer Heidelberg https Huguenin Kermarrec Kloudas Content and ical locality in content sharing systems In Proceedings of the international workshop on Network and Operating System Support for ital Audio and Video pp ACM Isard Prabhakaran Currey Wieder Talwar Goldberg Quincy fair scheduling for distributed computing clusters In Proceedings of the ACM SIGOPS Symposium on Operating Systems Principles pp ACM Ising Beitrag zur Theorie des Ferromagnetismus Zeitschrift Physik Hadrons and Nuclei Istepanian Hu Philip Sungoor The potential of Internet of health Things for glucose level sensing In Annual national Conference of the IEEE Engineering in Medicine and Biology Society EMBC pp IEEE Jacobson Smetters Thornton Plass Briggs Braynard Networking named content Commun ACM https Grzonka Ko lodziej Security supportive energy aware ing and scaling for cloud environments Jiang Feng Qin Content distribution for systems based on tributed cloud service network architecture Towards Eﬃcient and Scalable Content Delivery Juarez Ejarque Badia Dynamic scheduling for parallel application in cloud computing Futur Gener Comput Syst Karanasos et Mercury hybrid centralized and distributed scheduling in large shared clusters In USENIX Annual Technical Conference pp Katz Lazarsfeld Personal Inﬂuence the Part Played by People in the Flow of Mass Communications Transaction Publishers Piscataway Kaushik Bhandarkar GreenHDFS towards an hybrid Hadoop compute cluster In Proceedings of the USENIX Annual Technical Conference Kempe Kleinberg Tardos Maximizing the spread of inﬂuence through social network In Proceedings of the Ninth ACM SIGKDD tional Conference on Knowledge Discovery and Data Mining pp ACM Kilanioti Improving multimedia content delivery via augmentation with social information The social prefetcher approach IEEE Trans Multimed https Kilanioti et survey on distribution of social data streams over data centres Simul Model Pract Theory http Kilanioti Papadopoulos Delivering social multimedia content with ability In Pop Ko lodziej Di Martino B eds Resource Management for Big Data Platforms CCN pp Springer Cham https Kilanioti Papadopoulos Predicting video virality on Twitter In Pop Ko lodziej Di Martino B eds Resource Management for Big Data forms CCN pp Springer Cham https Kilanioti Papadopoulos Content delivery simulations supported by social Simul Model Pract Theory Kleinberg Cascading behavior in networks algorithmic and economic issues In Nisan Roughgarden Tardos Vazirani V eds Algorithmic Game Theory pp Cambridge University Press Ko Cho Kim Eﬃcient and eﬀective inﬂuence maximization in social networks Inf Sci Kuperman Abramson Small world eﬀect in an epidemiological model Phys Rev Lett Lane Miluzzo Lu Peebles Choudhury Campbell survey of mobile phone sensing Supercomput https Leconte Paschos Gkatzikis Draief Vassilaras Chouvardas Placing dynamic content in caches with small population In IEEE INFOCOM The Annual IEEE International Conference on Computer cations pp April Lee Zomaya Energy eﬃcient utilization of resources in cloud puting systems Supercomput Lerman Intagorn Kang Ghosh Using proximity to predict activity in social networks In Proceedings of the International Conference on World Wide Web pp ACM Kilanioti et Leskovec McGlohon Faloutsos Glance Hurst Patterns of cascading behavior in large blog graphs In Proceedings of SIAM International Conference on Data Mining SDM SIAM Li Wang Gao Zhang survey on information diﬀusion in online social networks models and methods Information Kleinberg Tracing information ﬂow on global scale using Internet data Proc Natl Acad Sci Liberal Kourtis Fajardo Koumaras Multimedia content delivery in SDN and towards networks IEEE COMSOC MMTC Lin Mei Jiang Qi Inferring the diﬀusion and evolution of topics in social communities Mind Luczak Size and connectivity of the of random graph Discrete Math Luo Wang Zhang Wang Superset replica ment strategy towards and distributed storage vice In International Conference on Advanced Cloud and Big Data CBD pp IEEE Luu Hoang Lim survey of information diﬀusion models and relevant problems Maier Feldmann Paxson Allman On dominant characteristics of residential broadband internet traﬃc In Proceedings of the ACM SIGCOMM Internet Measurement Conference IMC pp ACM Mars Tang heterogeneity in homogeneous computers In ACM SIGARCH Computer Architecture News vol pp ACM Menon Big data Facebook In Proceedings of the Workshop on agement of Big Data Systems pp ACM Milgram The small world problem Psychol Today Morris Contagion Rev Econ Stud Najar Denoyer Gallinari Predicting information diﬀusion on social networks with partial knowledge In Proceedings of the International ference on World Wide Web pp ACM Nathuji Kansal Ghaﬀarkhah managing performance ference eﬀects for clouds In Proceedings of the European ference on Computer Systems pp ACM Navarro Da Costa Barbosa Righi aware spontaneous mobile social network In Ubiquitous Intelligence and puting and IEEE International Conference on Autonomic and Trusted Computing and IEEE International Conference on Scalable ing and Communications and Its Associated Workshops pp IEEE CoordSS an ontology framework for heterogeneous networks experimentation Telfor J Nekovee Moreno Bianconi Marsili Theory of rumour spreading in complex social networks Phys Stat Mech its Appl Ousterhout Wendell Zaharia Stoica Sparrow distributed low latency scheduling In Proceedings of the ACM Symposium on Operating Systems Principles pp ACM Towards Eﬃcient and Scalable Content Delivery Pejovic Musolesi Anticipatory mobile computing survey of the state of the art and research challenges ACM Comput Surv https Perera Zaslavsky Christen Georgakopoulos Context aware puting for the internet of things survey IEEE Commun Surv Tutor Perera Zaslavsky Christen Georgakopoulos Sensing as service model for smart cities supported by internet of things Trans Emerg mun Technol Plissonneau Mobile data traﬃc analysis how do you prefer watching videos In Proceedings of the International Teletraﬃc Congress ITC pp IEEE Qu Mashayekhi Terei Levis Canary scheduling architecture for high performance cloud computing arXiv preprint Rasley Karanasos Kandula Fonseca Vojnovic Rao cient queue management for cluster scheduling In Proceedings of the Eleventh European Conference on Computer Systems ACM Reiss Tumanov Ganger Katz Kozuch Heterogeneity and dynamicity of clouds at scale Google trace analysis In Proceedings of the Third ACM Symposium on Cloud Computing ACM Reiss Wilkes Hellerstein Google traces format schema Technical report Google Mountain View CA USA November http Accessed Mar Ren Kao Zhu Cheng On querying historial evolving graph sequences Proc VLDB Endow Ren Kwon Balazinska Howe Hadoop s adolescence an analysis of hadoop usage in scientiﬁc workloads Proc VLDB Endow Ricciardi Careglio Fiore Palmieri et Saving energy in data center infrastructures In First International Conference on Data Compression Communications and Processing CCP pp IEEE Rodriguez Leskovec Structure and dynamics of tion pathways in online media In Proceedings of ACM International Conference on Web Search and Data Mining WSDM Rome Italy Rogers Diﬀusion of Innovations Simon and Schuster New York Sastry Yoneki Crowcroft Buzztraq predicting geographical access patterns of social cascades using social networks In Proceedings of the Second ACM EuroSys Workshop on Social Network Systems SNS Nuremberg Germany March pp http Satyanarayanan Pervasive computing vision and challenges IEEE Pers Commun Scellato Mascolo Musolesi Crowcroft Track globally deliver locally improving content delivery networks by tracking geographic social cades In Proceedings of the International Conference on World Wide Web WWW Hyderabad India April pp http Schilit et Challenge ubiquitous computing and the place lab initiative In Proceedings of the ACM International Workshop on Wireless Mobile Applications and Services on WLAN Hotspots pp ACM Kilanioti et Schilit Theimer Disseminating active map information to mobile hosts IEEE Netw Schwarzkopf Konwinski Wilkes Omega ﬂexible scalable schedulers for large compute clusters In Proceedings of the ACM European Conference on Computer Systems pp ACM Sendling Micromotives and Macrobehavior Norton New York Shue Freedman Shaikh Performance isolation and fairness for cloud storage OSDI Sohrabi Tang Moser Aleti Adaptive virtual machine migration mechanism for energy eﬃciency In Proceedings of the International shop on Green and Sustainable Software pp ACM Song Chi Hino Tseng Information ﬂow modeling based on diﬀusion rate for prediction and ranking In Proceedings of the International Conference on World Wide Web pp ACM Sundmaeker Guillemin Friess Vision and challenges for realising the internet of things Clust Eur Res Proj Internet Things Eur mision Thereska Donnelly Narayanan Sierra practical for data center storage In Proceedings of the Sixth Conference on Computer systems pp ACM Torres Finamore Kim Mellia Munafo Rao ing video server selection strategies in the YouTube CDN In national Conference on Distributed Computing Systems ICDCS pp IEEE Tosic et Semantic coordination protocol for LTE and coexistence In European Conference on Networks and Communications EuCNC pp IEEE Traverso Huguenin Trestian Erramilli Laoutaris naki TailGate handling content with little help from friends In Proceedings of the World Wide Web Conference WWW Lyon France April pp http Vavilapalli et Apache Hadoop YARN yet another resource negotiator In Proceedings of the Annual Symposium on Cloud Computing ACM Vergara EnergyBox tool for data mission energy consumption studies In Energy Eﬃciency in Large Scale tributed Systems COST European Conference Vienna Austria April Revised Selected Papers pp http Verma Pedrosa Korupolu Oppenheimer Tune Wilkes cluster management at Google with Borg In Proceedings of the Tenth European Conference on Computer Systems ACM Wang Zhou Yu Wang Quantitative evaluation of group user experience in smart spaces Cybern Syst Int J Want Pering System challenges for ubiquitous pervasive computing In Proceedings of the International Conference on Software Engineering pp ACM Watts simple model of global cascades on random networks Proc Natl Acad Sci Towards Eﬃcient and Scalable Content Delivery Yan Qian Sharif Tipper survey on smart grid communication infrastructures motivations requirements and challenges IEEE Commun Surv Tutor Yang Breslow Mars Tang precise online QoS ment for increased utilization in warehouse scale computers In ACM SIGARCH Computer Architecture News vol pp ACM Yang Leskovec Modeling information diﬀusion in implicit networks In Proceedings of the IEEE International Conference on Data Mining ICDM pp IEEE Zaharia Borthakur Sen Sarma Elmeleegy Shenker Stoica Delay scheduling simple technique for achieving locality and fairness in cluster scheduling In Proceedings of the European Conference on Computer Systems pp ACM Zhang Yu Guo Wang Exploiting personal and community context in mobile social networks In Chin Zhang D eds Mobile Social Networking CSS pp Springer New York https Zhang Tune Hagmann Jnagal Gokhale Wilkes CPI CPU performance isolation for shared compute clusters In Proceedings of the ACM European Conference on Computer Systems pp ACM Zhou Zhang Franco Mislove Revis Sundaram WebCloud recruiting social network users to assist in content distribution In Proceedings of the IEEE International Symposium on Network Computing and tions Cambridge MA USA Open Access This chapter is licensed under the terms of the Creative Commons Attribution International License http which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give appropriate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this chapter are included in the chapter s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the chapter s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder Big Data in Distributed Applications Valentina Ari Milorad Nenad Mikko Mike Jukka Svetozar Daniel Jacek Pierre and Francisco Faculty of Electronic Engineering University of Nis Nis Serbia valentina Faculty of Computing and Electrical Engineering Tampere University of Technology Tampere Finland Faculty of Science and Mathematics University of Nis Nis Serbia Cracow University of Technology Cracow Poland University of Applied Sciences of Western Switzerland Fribourg Switzerland Rey Juan Carlos University Madrid Spain Abstract Fifth generation mobile networks will rather supplement than replace current networks by dramatically improving their bandwidth capacity and reliability This way much more demanding use cases that simply are not achievable with today s networks will become reality from home entertainment to product manufacturing and healthcare However many of them rely on Internet of Things IoT devices equipped with mitters and sensors that generate enormous amount of data about their ronment Therefore due to large scale of systems combined with their inherent complexity and heterogeneity Big Data and analysis techniques are considered as one of the main enablers of future mobile networks In this work we recognize use cases from various application domains and list the basic requirements for their development and realization Keywords Big Data Use cases Introduction The vision of is becoming clearer as we move closer to the end of this decade The will feature increased network speed and machines cars city infrastructure beside people will be connected It is expected that networks will have bilities and to be energy efﬁcient which require new protocols and access technologies The network represents highly complex and heterogeneous network that integrates massive amount of sensor nodes and diversity of devices such as macro and small cells with different radio access technologies such as GSM WCDMA LTE and that coexist with one another Such network vision is expected to lead to trafﬁc volume of tens of exabytes per month that further demands networks capacity times higher than now Such trafﬁc volume is not supported with nowadays cellular networks Thus practical deployment of networking systems in addition to traditional technology drivers needs some new critical issues to be resolved on different areas The Author s Kołodziej and Eds cHiPSet LNCS pp https Big Data in Distributed Applications such as coordination mechanism power consumption networking behavior prediction positioning and etc Some operators already start their deployments and the standards process forward will be built upon the existing LTE networks with features available as part of the LTE Advanced standard Some features will include carrier aggregation that enable using of existing spectrum efﬁciently with network capacity increase and higher throughput rates networks will play key role as well as technologies such as coordinated multipoint that will enable operators to simultaneously transmit and process signals from multiple sites networks SDN and network functions virtualization NFV will be very important for operators in order to scale their networks quickly in migration from to SDN will play key role for carving virtual which can be used for huge bandwidth applications which for example include video with requirement in speed of as well as lower bandwidth applications which for example connect different user equipment that are less demanding on the network The architecture and deployment will depend upon how the network is used For example applications such as streaming video video conferencing and virtual reality require high speed with growth in the video trafﬁc In order to achieve this requirement the network needs lot of small cell coverage and higher bandwidth spectrum Further will be the network for Internet of Things IoT with support for lot of devices Such IoT network should be efﬁcient in missions with enhanced coverage Because of the high scale of systems combined with their inherent complexity and heterogeneity Big Data techniques and analysis will be one of the main enablers of the new critical issues Big Data refers to large data sets whose size is growing at enormous speed making it difﬁcult to handle and manage them using the traditional techniques and software tools It is step forward from traditional data analysis considering the following aspects ﬁve ﬁve Vs quantity of data volume different types of structured and unstructured data variety the rate with which data is changing or how often it is created velocity the importance of results extracted from data value data quality including trust credibility and integrity veracity Taking into account the prediction that the number of connected devices will increase by the time when will be commercially used it can be concluded that Big Data techniques will play an important role as all the considered usage scenarios are based on extracting knowledge from the enormous amount of heterogeneous data generated by connected devices in order to support the decisioning and other mechanisms in future networks In this chapter we identify use cases and scenarios that could beneﬁt from new capabilities provided by network in synergy with Big Data technologies list basic requirements for their application development and consider some future challenges addressing positioning challenges and solutions The researcher munity and service providers business stakeholders could beneﬁt from this chapter From one side it provides an insight of recent trends in research and development while from the other side it discusses how the research outcomes could be used for development of future services to satisfy customer demands Nejkovic et Section of this chapter gives list use cases requirements while Sect gives identiﬁed use case with short description of each Section gives future challenges targeting positioning systems semantic based approaches security etc Section concludes the chapter Use Cases Requirements The introduction of big data techniques in distributed applications poses lenge as these techniques usually require huge computational resources In general there is need for high performance computing infrastructure This infrastructure would typically be available as private or public cloud or grid Cloud or grid resources will allow for consuming storing and processing huge amounts of data This data shall be prepared for consumption from the edge network Depending on the nature of the data needed by we can envision two kinds of data processing methods online and ofﬂine Ofﬂine methods are easier to handle as the processing can be performed in the cloud or grid This are supposed to be processes that are not critical Online processing is used when response is needed in given amount of time and therefore both the time required to give response and the latency would have high impact on the set of use cases that will be available for For online processing in those cases where common hardware is available at the edge general Big Data solutions can be run on top of this commodity hardware assuming that the constrained resources available are enough In general we identify following requirements needed for use cases Network requirements Network with capabilities faster and networks which can deliver video and other services massive nectivity of devices based on different technologies etc Application requirements Consistent process mining over Big Data triple store Network capability measurement module Reasoning module Learning and diction module for example Neural Network Optimization module sponding domain and application Ontologies etc Storage requirements Big Data triple store Possibility to handles large amounts petabyte or more of data Distributed redundant data storage Massively parallel processing Provides Semantic Big Data processing capabilities Centrally managed and orchestrated Even though the networks are primarily designed for enhanced communication purposes positioning been considered as one of the key features in Moreover the standardization organization third generation partnership project already published several technical reports and speciﬁcations regarding positioning in future networks However since the speciﬁcations are still under development detailed descriptions of different positioning approaches and related positioning protocols are yet unavailable Despite this in order to facilitate development of various future use cases introduced the ﬁrst set of performance requirements considering different types of use cases presented in For each use case speciﬁc positioning accuracy including both Big Data in Distributed Applications horizontal and vertical directions been given according to the type of the area of interest and its service characteristics For certain use cases when applicable also accuracy requirements for velocity estimation and device bearing estimation are provided Similar to common guidelines besides focusing only on maximizing the positioning accuracy also other important positioning service aspects have been considered in the existing reports given in One of the key performance indicators is positioning availability which deﬁnes in which percent of the time the positioning method provides estimates with the speciﬁed accuracy level Another important tioning performance indicator is latency which indicates the elapsed time between triggering the positioning process and ﬁnally obtaining the position estimates over the latency of the ﬁrst position estimate at the initialization stage of the tioning process referred to as the been separately speciﬁed typically with reduced performance requirements compared to the latency in general In addition to the positioning requirements there are various other aspects which have not yet been appropriately addressed in the reports and speciﬁcations but should be considered according to the needs of users operators and parties Such aspects include for example energy consumption security and privacy estimation reliability and related conﬁdence levels and possible regulatory requirements positioning during emergency calls positioning approaches can be divided into two fundamental categories which are positioning and positioning In positioning the position estimates are obtained at the network side based on the signals transmitted by the user device In this approach all heavy putational load is located at the network side which reduces the power consumption of the user device and thus increases the valuable battery life of the device Moreover when the positioning is done at the network side all positioning related information such as network BS locations are already available for the positioning algorithms without introducing additional overhead from signaling the information over the radio interface The fact that the position information is fundamentally located at the network side is especially useful for achieving the future targets of the networks as it facilitates numerous use cases where the latency and reliability of the position estimates are in crucial role In the positioning approach where the user device performs the positioning based on the signals mitted by the network nodes the position information is not directly available at the network side This approach increases user security and privacy but on the other hand it requires additional signaling overhead in order to utilize the device positions jointly as part of new infrastructures such as trafﬁc control and ITS for instance Use Cases In this section we analyze how the current advances in and related concepts can be leveraged in order to provide novel use cases that were not possible before or improve the existing services and solutions Nejkovic et Coordination Mechanisms The network indicates the need for coexistence of multiple wireless technologies in the same environment The problem that raises in such environments is mutual interference among multiple wireless networks which is consequence of an overlapping in usage of the same set of resources Typically such case happens when same radio frequencies are used for multiple communication channels that are based on different radio technologies Coordination protocols deﬁned by the technology standards traditionally address the problem when networks use same technology New coordination concepts are needed in the case of networks based on heterogeneous technologies We identify the following possible scenario In Home Network setting typical home can have several rooms each equipped with WiFi enabled HDTV set and number of streaming audio appliances At the same time and in the same building sensor network is used for home automation including presence detection temperature and lighting regulation doorbell indication and security and safety monitoring Most homes also have at least one microwave oven and number of Bluetooth Low Energy gadgets During the typical evening all of these devices are active and have to be actively coordinated in order to provide satisfactory level of service Power Consumption number of recently ﬁnished as well as currently related EU projects conﬁrm diversity of usage and applications of power consumption efﬁciency and reliability in WSNs These projects delivered number of algorithms and protocols for reducing energy consumption that show the importance of the power consumption Further the design of the wireless networks to consider energy efﬁciency as very important pillar in order to optimize economic operational and environmental concerns In presence of enormous high trafﬁc volume techniques such as intelligent distribution of frequently accessed content over the network nodes and content caching can result in relevant energy consumption reductions and prolong the lifetime of nodes that are low on battery energy Networking Behavior Prediction Big Data Analytics solutions can predict how the needs in resources use change among places and throughout the time within complex system network that adopt such solution would have ability to learn from the previous situations and states and intelligently adopt to new demands Particularly using appropriate learning techniques the system will enable devices to learn from past observations in their surroundings For example we identify the following use case scenario In Smart City trafﬁc lights and pedestrian crossings various presence detectors are IEEE technology equipped while community WiFi network is mounted on number of light posts lining the same street During rush hours there is high demand for WiFi trafﬁc due to large number of people using personal devices potentially impacting trafﬁc Big Data in Distributed Applications management system b Mobile users consume images videos and music which increase in volume over time In such case network congestion is consequence of the high dynamics in demands that exceeds the system potential for adaptability Positioning and in Future Networks The world is changing rapidly New services are needed and many of those new services require Autonomous vehicles transportation trafﬁc trol need this kind of service If we consider the problem from the point of view of the smart city we notice that there are many new user groups such as pedestrians cleaning and maintenance services management and administration There are several ches to help with positioning One started with Long Range Positioning systems like Decca and LORAN and continued with Global Positioning System GPS that is positioning system based on Medium Earth Orbit satellites GPS is part of Global Navigation Satellite System GNSS GNSS also includes for example European Galileo and Russian GLONASS However satellites are not solving the positioning problem totally In many regions positioning needs help from mobile communication networks that is called assisted GPS The high latitudes in North and in South and cities with skyscrapers are for examples problematic regions In contrast to the earlier and existing mobile generations where positioning been only an feature future radio networks will allow for highly accurate positioning not only for personal navigation purposes but also for unforeseen aware services and applications like robotics intelligent transportation systems ITSs and drones just to name few While seeking to meet the demanding communication requirements of in terms of capacity and networks will exploit large bandwidths and massive antenna arrays which together with even denser base station BS deployments create also convenient environment for radio positioning Hence it is widely expected that future networks should enable and even improve indoor and outdoor positioning techniques embedded to radio access network RAN as well as the ones that utilizes measurements from GNSS or sensors Deﬁnition Live Video Streaming in Wireless Networks In recent years video streaming both and live become an important part of our everyday lives from social networks content delivery platforms to industrial robotic and experimentation systems Due to rise of processor power and camera sensor resolution of consumer devices such as smartphones the image quality criteria perceived by consumers dramatically increased High Deﬁnition video is becoming must for all the use cases where video streaming is involved Not only that but also new video formats are emerging such as stereoscopic video and Ultra High Deﬁnition Video which contain even more data that to be mitted Therefore Internet service providers mobile carriers and content providers are encountering many issues as transmission of such content requires signiﬁcantly larger bandwidth Additionally the issues become even more challenging due to device mobility which can affect the Quality of Service and Quality of Experience especially Nejkovic et when it comes to live video broadcast in varying network conditions Here we identify potential use case of novel networking paradigms SDN and VNF in combination with Big Data technologies Large amount of network equipment and status data is analyzed The results of data analysis are semantically annotated and stored into RDF triple store so semantic reasoning can be performed in order to draw new conclusions which could lead to of virtual networking assets generation of SDN rules parameter tuning or other optimizations with objective to satisfy QoS parameters and maintain the quality of high deﬁnition live video streaming in varying network conditions where devices are moving intensively such as mobile robotic and experimentation systems In several publications so far this topic been discussed problems identiﬁed and several solutions proposed However in most cases these solutions suffer from low quality large latency in live streaming and frequent freezes in the video playout due to sudden drops of the available bandwidth In it was shown prioritization introduced by an OpenFlow controller can reduce video freezes caused by network congestion Therefore the utilization of SDN technologies in this case seems promising In results conﬁrm that it is now possible to realize THz wireless communication system for commercial applications where Ultra HD video streaming is needed However it is not suitable for use cases like experimentation and mobile robot where wireless nication is of utmost importance We can conclude that there are still many open questions in case of deﬁnition live video streaming using wireless networks which makes it suitable for future research and application of next generation networking in synergy with Big Data and semantic technologies Trust Based on Blockchain for Process Monitoring IoT and smart objects are technologies for monitoring of complex business processes especially in logistics domain and industrial production systems However most of these processes involve multiple parties In absence of central authority the trust between these parties becomes an important issue Despite the fact that monitoring enables to effectively keep track of the execution of processes where multiple organizations are involved it does not fully solve the problem of trust among them As the devices involved in monitoring process might belong to different organizations there is still possibility that one of the parties can misconﬁgure its devices in order to achieve some own goal in an illegal way with possibility to disrupt the process execution itself affecting the ﬁnal outcome Blockchain technology is recognized as solution for issues related to trust in process monitoring systems Blockchain provides shared immutable ledger which guarantees that the information can be accessed and validated by all the participants of process both during its execution and after it is completed which builds the trust among them This use case represents potential area where we can make use of synergy of various novel technologies and paradigms such as IoT Big Data and next generation networking together with blockchain Big Data in Distributed Applications Trusted Friend Computing With the advent of networks an increasing number of devices will be connected permanently and at high speed to the Internet and to each other While many cations will beneﬁt from this new connectivity by being able to interact more quickly with data providers such as the cloud or other resources there is also growing need for data sharing This is particularly the case in the context of Big Data which also includes the issue of moving large amounts of private data One possible approach to this problem is to move calculations close to the data and provide access to both data and local computing resources The Trusted Friend Computing TFC concept aims to enable community of users to securely share their IT resources without central organization collecting and storing information It is distributed paradigm where data computing power software or the network can be shared reliably and resiliently This paradigm deﬁnes an original IT architecture built around the notion of community of users called friends of given software application Instead of using the traditional approach where the IT architecture is to share resources the TFC approach focuses on the software application used by the community One of the important advantages of this approach is to avoid heavy executable codes transfers since all friends already possess the calculation modules useful for the community Inspired by the social network model and using concept similar to virtual private networks VPNs the idea is to allow friends to invite other users of the software to join the community to share their resources The community is therefore built by individual cooptation and is by nature distributed decentralized and elastic To achieve this objective several major technical challenges must be addressed We can among other things mention Clearly deﬁne security model for sharing IT resources in the context of TFC applications The deﬁnition of community management and accounting needs The development of platform to enable and facilitate the implementation of applications that comply with the TFC model Finally user community focused on using speciﬁc application must be identiﬁed and the application must be enhanced with TFC features One of these communities is that of physicians involved in the diagnosis of genetic diseases and using the searchNGS tool This Java software analyzes sequencing data NGS to detect changes in DNA sequences for the diagnosis of genetic diseases In order to be able to easily integrate into any Java software including searchNGS TFC capabilities the tool was used This tool been improved to support the different functionalities required for cations TFC s security model is based on the notion of conﬁdence link as presented in conﬁdence link is channel that allows two friends to communicate Nejkovic et safely at any time The conﬁdence link also authenticates users with security tiﬁcate ensuring the identiﬁcation of communicating partners Each member of network can extend the network by creating additional conﬁdence links with other friends thus adding new members to the network All friends as well as all conﬁdence links form connected graph where the nodes are friends and the arcs are the dence links We call such graph community of trusted friends or more simply community None of the friends in the community have global view of the infrastructure Each friend only knows his direct friends the users with whom established conﬁdence link Applications can publish resources on network of friends or search and access the resources of other network members When publishing resource speciﬁc access rights can be given to limit access to the resource for example by differentiating between direct and indirect friends in network The model also includes the ability to record the use of each member s resources which allows the use of resources to be billed based on their utilization rate thereby encouraging members to share their resources Today and certainly even more so tomorrow the use of mobile networks for professional applications will be reality These applications are less and less conﬁned to work desktops but are now used outside the enterprise environment for efﬁciency and ease of use concept such as TFC can truly beneﬁt from mobile communications network such as networks to provide professional munities with secure access anytime anywhere to vast computing and data resources Virtual and Augmented Reality Applications Virtual VR and augmented reality AR applications are not exceptions when it comes to potential use cases where utilization of networks could be highly ﬁcial The arrival of of mobile network will unlock the full potential of VR and AR technology which is still limited by current network characteristics The complex scenes and sophisticated input mechanisms that are used to create the VR and AR experiences require large amount of data that to be processed Lag stutter and stalls are unacceptable for user experience and comfort This is not huge problem for local applications but is quite challenging when done remotely if the user is on the move and not using the ﬁxed network connection In this case the quality of VR and AR experience is heavily dependent on three network components high capacity low latency and uniform experience This way many novel services and applications that involve the usage of augmented and virtual reality would see lights of the day such as immersive movies video games live shows concerts sport events immersive education platforms immersive social interactions immersive professional project collaboration and many others To sum up these services would affect the way that people play learn and communicate Big Data in Distributed Applications Current Solutions and Future Challenges Positioning Possible Error Sources State of the Art When considering the positioning aspect BS deployments increased transmission bandwidths and large antenna arrays enable efﬁcient utilization of both ToA and TDoA and DoA positioning measurements However in order to exploit these types of measurements for positioning speciﬁc prior knowledge about the network and user device is often assumed available In case of temporal measurements the clocks of the user device and network nodes are often assumed to be synchronized More speciﬁcally with ToA measurements all clocks in the network including the user device and the BSs are typically assumed to be synchronized among each other whereas with TDoA measurements only the BS clocks are assumed to be synchronized Nonetheless clock synchronization errors can result in large cies in ranging measurements and thus to be carefully considered in practical positioning system implementation Besides the aforementioned clock errors the ranging measurements as well as measurements can be deteriorated by the errors related to BSs locations In addition to the inaccurate BSs location information uncertainties in the orientation of the BS antennas the user device antennas may cause signiﬁcant error to the positioning results when utilizing ments like DoA measurements for positioning Whereas the BS position and antenna orientation error can be typically considered the clock errors are often with certain behavior However it is appropriate to assume that the behavior of the BS clocks can be sufﬁciently small and thus there can be only possible constant clock offset between the BS clocks Nonetheless any unknown or uncertain system parameter such as clock offset BS positions and antenna orientation can be estimated using classical simultaneous localization and mapping SLAM approaches where the user device position and the unknown system parameters are estimated simultaneously while the user device is moving within the network coverage area Since the speciﬁcations are still under development and networks are only beginning to emerge to the market the positioning studies rely on high computational load computer simulations using realistic radio wave propagation models with extensive ray tracing algorithms In positioning approach was studied by considering asynchronous clocks in the user device and in the network BSs It was shown that regardless of the clock errors positioning accuracy was achieved by using the ToA and DoA measurements Moreover while the user device was moving in the network the network BSs were synchronized similar to the SLAM principle This type of approach was later used in for including applications for proactive investigating communications radio resource management and geometric beamforming positioning approach based on signals from single BS was studied in In this case by utilizing only single BS for the positioning requirements for Nejkovic et the clock synchronization can be considerably alleviated Based on the ToA and AoA measurements the user device position was estimated with accuracy and the antenna orientation of the user device with accuracy Moreover the developed estimation algorithm was also designed to exploit reﬂected or scattered radio wave components and therefore it was able to provide position estimates also for the reﬂection locations This type of utilization of radio paths introduces various new communications aspects from advanced forming techniques to interference management The positioning approach was also studied in for train scenario utilizing downlink synchronization signal blocks for positioning purposes Again despite of the challenging BS geometry of the train scenario positioning accuracy was achieved by jointly using the ToA and AoA measurements The positioning with uncertain BS antenna orientations was studied in where the positioning was based on type of signals used in tional beam training procedures By using received signal power surements the user device position and the unknown BS antenna orientations were jointly estimated achieving positioning error and antenna orientation error Semantic Analysis of Network Topology and Sensor Data for Localization in Networks Challenges positioning been considered as one of the key features of future generation network and still an open question in many areas such as robotics based experimentation and exploration autonomous vehicles and intelligent portation systems This task becomes quite challenging in these cases especially when it comes to indoor localization and outdoor localization of aerial devices in varying network conditions drones positioning can be divided in two categories and There are various positioning methods which perform with ferent values of accuracy latency and in certain conditions It is identiﬁed that current research in localization is going towards cooperation Therefore the semantic coordination of both user and network operator devices could be used for determining the precise location taking into account two factors Network topology how the devices are arranged in space within the network such as distance frequency band at which the device is operating etc The information about network topology can be semantically annotated leveraging some speciﬁc language as representation Service utilization and sensor data large amount of service utilization and sensor data is collected from both the customer and network operator devices such as monitoring and status It can be analyzed leveraging various data analysis nique Furthermore the data can be semantically annotated according to the results obtained as output of data analysis techniques Big Data in Distributed Applications For this purpose we deﬁne ontologies and rules which are used to perform semantic reasoning about the precise location leveraging the semantic annotations about both the network topology and service data taking into account the performance metrics and QoS parameters such as accuracy latency and Fig Fig Semantic analysis of network topology and sensor data for localization in networks Infrastructure Design of Semantic Driven Big Data in Networking Semantic Driven Big Data State of the Art Problems of coordination power consumption and network behavior prediction feature smart adoption of data processing results by the system that we propose to address using semantics In particular core of the proposed infrastructure is server centralized or distributed that collects relevant knowledge in the given environment and uses the knowledge to make necessary informative decisions for example about network coordination network sensors power consumption etc The server collects networking data and interprets data semantically For the knowledge representation the server uses ontology framework approach The ﬁrst version of the framework been previously successfully applied in the case of coordination of technologies that operate in the same unlicensed frequency band The Nejkovic et coordination and spectrum sensing is modelled as an interactive process where system nodes communicate and share knowledge about relevant spectrum conditions Semantic channels are established within the system for the interaction between ticipating communication devices The ontology framework could be extended for different cases such as solution presented in that could give further directions for management of semantic Big Data for intelligence System that incorporates sensors and user equipment acquire large collection of data Collecting storing analyzing and retrieving data from industrial sensors or other machinery connected to the Internet of Things become of increasing importance as growing number of organizations is looking to take advantage of available data One possible semantic framework approach been successfully proven in many cases In the case of semantic coordination is presented The coordination and spectrum sensing is modelled as an interactive process where system nodes communicate and share knowledge about relevant spectrum conditions Ontologies are used for knowledge representation as bases for automatic reasoning about optimal channel allocations and for coordination Moreover in the semantic technology was used for the implementation of network intelligence on top of the platform by using reasoning for the network state estimation in order to perform the spectrum coordination On the other side in approach for unmanned vehicle mission coordination in robotic experimentation testbeds is sented In this case the ontologies are used to represent the knowledge about device capabilities constraints domain expert knowledge and both the mission code and sensor data aspects that are taken into account during the eration of the coordinated device missions Furthermore in the paper is presented the novel approach and algorithm for automatic code generation with huge potential with its extension to applications Semantic Driven System Architecture Challenges The challenge is to exploit semantic technologies at the backend as ﬂexible dation for advanced frontend data processing tasks Possible system architecture sists of ﬁve modules given in Fig and described in more details in the following Data Acquisition Module DAM Redis is an database with option of persistence on disk so it represents tradeoff where very high write and read speed is achieved at the price of the limitation of data sets that can t be larger than memory We assume data sources at the order of million data series with about million measurements annually for few tens of years with several bytes of data item size Hence size of the total data load could be estimated at the order of petabyte PB The row data low information density and as such it is very susceptible for compression Hence it can be expected that times pression rate can be achieved easily simple encoding Big Data technologies can be used for scale data sets processing Distributed storage and processing frameworks are used for that such as the open source Apache Hadoop Framework Apache Hadoop enables distributed data cessing across clusters of computers Popular MapReduce distributed processing model Hadoop Distributed File System HDFS and distributed table store HBase are Hadoop components Big Data in Distributed Applications The Module PPM identiﬁes instability intervals that are semantically annotated stored and retrieved later on during search by end user Anomaly detection is an important problem that been under extensive research in diverse application domains We distinguish two basic types of approaches with respect to domain speciﬁcation as well as online or ofﬂine processing These approaches are not mutually exclusive but in opposite they can be used together to achieve synergy effect Results obtained in such way can be used for instant reaction but also for longer term planning activities They can provide users valuable information which can be used proactively further improve system Fig System architecture for Semantic Driven Big Data in networking Nejkovic et efﬁciency and give competitive advantages In most cases reﬁning of input data is needed as ﬁrst step Anomaly detection in time series data obtained by sensors is very demanding task but really important in the same time simple algorithm for instability intervals detection is sioned The process complexity shall be encapsulated into separate instability parameters construction module that would extract the parameters from data series in an independent manner Semantic description of the bility interval may be more or less complex depending on the end user application requirements However it contains pointer to the corresponding row data record that are stored separately such that semantic search may retrieve row data also We estimate more than potential instability intervals within one series annually with average of sensors detecting the instability at one measurement time instant resulting in instability intervals annually If we assume semantic annotation of the intervals of triplets per interval it totals to triplets This data size is proven to be practically successfully implemented on single server hardware with GB of RAM Note that the system is easily scalable by simple multiplication of the servers assuming erated queries are implemented Semantics Module SM is based on platform for scalable linked data semantic datasets management The platform is envisioned to feature advanced Web based collaborative ontology editor and to be ﬂexible with respect to the used triplestore By default we assume triplestore that is based on Jena as one of the most proven Open Source semantic technologies on the market Semantic data is represented in standard formats and manipulated by semantic queries written in the standard SPARQL query language In this way the technology would allow different reasoners to be adopted The expected data processing and storage efﬁciency is based on the effective use of semantic descriptions of physical characteristics of sensors their organization and ment price type of measurement units etc For the purpose number of standard ontologies may be loaded into the system and used such as time ontology measurements ontology etc For application speciﬁc purposes an online orative ontology editor will be used to allow to adjust existing ontologies and develop new ones When we have sensor data attention on the ontologies for sensor data and metadata should be put The most corresponding is the Semantic Sensor Network SSN ontology Systems that adopt the SSN ontology are built on an RDF database triple store Big volume of sensor data collected are challenging to triple stores because the evaluation of SPARQL queries becomes expensive Triple stores are not optimized to evaluate time series interval queries Emrooz is good solution for such case Emrooz is open source and scalable database capable of consuming SSN observations represented in RDF and evaluating SPARQL queries for SSN observations Emrooz can be implemented on Apache Cassandra and Sesame Big Data in Distributed Applications Web Application Module WAM is envisioned to be implemented as an advanced Ajax that communicates with using RESTful service API and SPARQL queries It should consist of the following NLP user enters search query in simpliﬁed English such that low cognitive load for end user is required while in the same time certain domain language speciﬁcs are exploited in order to lower complexity of the language use and processing The input is then processed and converted to SPARQL query for semantic search over the semantically annotated data Data cessing algorithms can then be used to retrieve intervals of unstable states with recorded date and time of the start and end of the data sequence Every time series is semantically annotated also name of the physical property type of the measurement unit name of the subsystem machine location attached sensor etc Semantic descriptions of data are automatically generated during preprocessing and are later used for making autocomplete dations to the user to help him easily search the time series descriptions Then key segments are identiﬁed related to the given search query where each of the segments can describe date time abnormality in measured physical unit on some sensor with some features and unstable data overlapping with an extracted instability interval In this way we are able to make more effective and user friendly queries b Reporting Analytics Results of the data analysis are visually presented by means of visually appealing charts graphs tables etc Semantic ﬁltering is applied for powerful faceted search and browsing through the search results Also data analysts are able to reconﬁgure online data sentation into simple web applications that could be instantly used by the other team members Semantic similarity between the input query and annotations of the instability intervals is used as the indicator of coincidence between data sets and the search query Multilingual support can be provided by existing features deﬁned in the standard language support As consequence of the NLP module that is based on representing concepts by the supporting ontologies gualism is supported naturally Though some additional effort would be required depending on the type and number of additional languages Additional features Different characterizations and taxonomies of the bility intervals are possible including for example classiﬁcation of the abnormal events as low middle high or critical We can also specify the intensity of the abnormality as percentage of the deviation of maximal measured value in the instability interval from the target value For example we may deﬁne abnormality as deviation between and Estimation of conditional probability of overlapping of two or more instability intervals will be based on simple analytics algorithms such as counting x of or of or in searching results indicate overlapping of the bility intervals Advanced simpliﬁed English based search for causality chains for identiﬁcation of the root cause is possible Similarly set of intervals rooted by speciﬁed interval can be determined as well Nejkovic et Reasoning and Predicting Module RPM is envisioned to be implemented as an advanced neural network Neural networks can be adopted for learning nent of the RPM Neural networks have been proven effective in interference detection and classiﬁcation within wireless networks as well as in series prediction tasks that are crucial for coordination Security Challenges Security solutions for can be divided into ﬁve groups Software Deﬁned Network SDN Network Function Virtualization NFV Mobile Cloud MC communication channels and privacy policies Primary focuses with target technologies described in are security of centralized control points SDN NFV ﬂow rules veriﬁcation in SDN switches SDN control access to SDN and core network elements SDN NFV MC isolation for VNFs and virtual slices NFV security of control channels SDN and channels themselves user identity veriﬁcation for roaming and clouds services privacy policies security of users identity and location privacy policies encryption and technologies privacy policies security of data storage systems and web services in clouds MC access control security for clouds MC Each of this target and security technologies are deeply investigated in Security of will be big challenge because it will connect branches of critical infrastructures However to make safe technology security solutions will sider not only this integrated critical infrastructure but also society as whole The basic challenges mentioned in and are High network trafﬁc huge number of IoT devices Security of radio communications Cryptographic integrity of user data plane Roaming Security updating security parameters between operators of networks Denial of Service and Distributed Denial of Service attacks on infrastructure and end devices Coordination of distributed control systems like Stratum layers of protocols Eavesdropping This attack may lead to intercepting messages by an attacked receiver and is very hard to detect Jamming This attack may lead to disrupting communication between legitimate users or block access to radio resources Very often is realized via an infected receiver Man in The Middle attack Attacker takes control over communication between legitimate users Basic requirements like authentication authorization availability or data dentiality Some of the current technologies fulﬁlling these requirements may be not effective enough in context Big Data in Distributed Applications Simulations very popular method of ﬁnding problems predicting behavior and developing improvements in system is analyzing the simulation of this system In cellular communication new simulation systems have to be developed because of huge number of new services applications requirements and performance indicators There are three basic types of simulations and Authors in describe the following challenges connected to all of these three types of simulation Variety of application technologies environments and performance indicators Complexity of simulation and simulators It is caused by growing memory demands and time of simulation which is result of huge MIMO and complexity of channels Integration of all these three types of simulations Integration of and simulation may be useful in the evaluation of nonlinear operations like NOMA in complex environments Integration of and level simulation is useful in the evaluation of the end performance of all network Other challenges like reusability scalability ﬂexibility multiple levels of abstraction or parallel processing are deeply investigated in Research and Propagation Issues One of the expectations for is to ensure prospective technologies which will be integrated and will allow creating networked society Seven main challenges in this ﬁeld mentioned in are communication communication cooperative devices communication massive communication tion and deployments The principals of propagation of centimeter waves are very similar to millimeter waves but have different characteristic The most important differences in those characteristics are free space path loss diffraction reﬂection and scattering material penetration These problems can be solved by deploying Multi Input Single Output System described in OFDM can be used as base for developing new system of encoding digital data on multiple carrier frequencies OFDM will allow to avoid multipath effect gain spectral efﬁciency and simplify equalization in comparison with Systems Millimeter Waves and Standardization Millimeter waves have bigger spectrum in cellular frequency bands then centimeters waves This provides new opportunities and challenges like Very high capacity and data rates Short wavelengths necessitating large array antenna solutions to maintain useful link budgets Antenna sizes will be smaller design challenges Nejkovic et Standardization of is still process However some decisions have been already made The World Radio Communication Conference promoted bands lower than GHz or between GHz in Third Generation Partnership Project completed ﬁrst speciﬁcation in Below GHz bandwidth requirements regarding cellular network did not change because of similarity of propagation conditions in new and existing bands More technical information about higher bands and accepted by standards can be found in and Modulation Schemes Extreme data rates huge number of IoT devices high resolution streaming videos this are only examples what will be used for The main challenge is to support very fast entry to the network even for transferring trivial data To achieve this goal proper modulation scheme have to be chosen Orthogonal Frequency Division Multiplexing is classic modulation scheme based on dividing available bandwidth into several parallel Each of these channels called also can transmit independent data Multiplexing in time and frequency is possible However authors in proposed three modulations schemes with better Peak to Average Power Ratio and better spectrum efﬁciency These modulations schemes are Filter Bank FBMC each is ﬁltered independently Cycle preﬁx is not used is used for orthogonality Generalized Frequency Division Multiplexing GFDM adaptable multiple carrier transmission methods Each is ﬁltered independently There is orthogonality Available spectrum is spread into segments Filtered Orthogonal Frequency Division Multiplexing an extension of classic OFDM Bandwidth is divided into depending on the application Each provide proper service The spectrum is accommodating range of services which optimize its usage Comparison of all these modulation schemes results and conclusions can be found in Machine Learning in Software Deﬁned Networks The technology entails signiﬁcant increase in the amount of processed data The continuous collection and analysis of such data leads to Big Data problems that are caused by the volume variety and velocity properties However key aspect of the operation of each network is its management and control Recently most of network functions routing switching ﬁrewalling conversion of protocols etc were realized by dedicated hardware The complexity of network infrastructure increases number of challenges in organizing managing and optimizing network operations The popular idea for solving these problems is ware Deﬁned Networking SDN paradigm SDN allows to migrate many of network functions from the devices to the networking controllers The SDN controller manages ﬂow control analyses network trafﬁc and routes packets according Big Data in Distributed Applications to forwarding policies Consequently SDN controller serves as sort of operating system for the network Taking into account aforementioned challenges and problem of processing large data sets there is need for developing efﬁcient and much more complex management methods Such management methods require making decisions in the real time There are lot of known data processing methods However many of them can not be directly applied for effective processing and management of large data sets in modern ronments such as networks Modern solutions require complex decision making techniques that analyze historical temporal and frequency network data One of the possible solutions could be the application of Machine Learning ML methods which are successfully used in the processing of Big Data The capabilities of SDN centralized control global view of the network network analysis and dynamic forwarding policies may ﬁt well to the application of Machine Learning techniques These possibilities are included in the FCAPS Fault Conﬁguration Accounting Performance Security management ISO standard In each of the following areas of application one can ﬁnd intelligent methods In the fault management area ML methods allow not only detection but also solving the causes of failures in networks Automation dealing with failures will allow for imization of downtime and human intervention and as result minimization of losses Machine Learning can play important role also in conﬁguration management Networks such as are characterized by frequent topological changes This requires modiﬁcations in the conﬁguration which can be prone to errors and difﬁcult to mize Considering the multiplicity of conﬁguration parameters analyses of ML can help to automate this process by dynamic resources allocation or services ﬁguration Appropriate methods can also allow veriﬁcation of the used conﬁguration and its possible withdrawal and rollback Accounting management is tightly connected with monitoring of network resources and pricing plans ML methods can help identify fraud and dishonest activities of network users It is also possible to analyze the use of resources and create new service packages Smart solutions can also signiﬁcantly improve the QoS level An important area of management is performance management Guaranteeing adequate level of performance is key factor for efﬁcient network The use of ML methods can result in trafﬁc load prediction and in result proactive and adaptive network performance management Security Management become crucial issue in networks Modern security approaches consist of tools for identifying threats and vulnerabilities The use of ML methods can help in detection of anomalies ﬁnding and abuses veriﬁcation in the network However this approach high risk of blocking the correct network trafﬁc high false positive rate Identifying the nature of the is crucial to choosing appropriate remedies allowed returning to the proper functioning of the network The aforementioned opportunities show wide ﬁeld for applying ML methods in Software Deﬁned Networking paradigm Machine Learning can play the major role in autonomous network management for networks Some of the available solutions such as IBM s or CogNet are successfully supported by Machine Learning methods Nejkovic et Conclusion Use case opportunities will increase enormously with networks deployment Not only that the existing applications and solutions will be enhanced but many novel use cases and scenarios will become feasible The potential for further use cases in future services and applications is huge in industries and national priorities including domains from entertainment and telecommunication services to healthcare smart cities remote industrial machine operation virtual sports attendance and many others However the future scenarios will place much more diverse requirements on the system that need to be explored and analyzed It is identiﬁed that the main enablers of future networks are Internet of Things IoT Big Data technologies together with novel networking paradigms Networking SDN and Network Functions Virtualization NFV New architectures will rely on large number of nected smart devices generating enormous amount of data each moment The generated data needs to be analyzed in order to make the right decision as soon as possible almost in real time On the other side the increased ﬂexibility of network infrastructure management and control is also required which is enabled by NFV and SDN Furthermore there is need for evolution of the current architectures by adding the additional network intelligence layer that would enable more complex scenarios such as device coordination The possible approaches for embedding the network intelligence are either using the semantic technology or machine learning techniques The future may seem far ahead but the phase for deﬁning the requirements is now Any new technology or system that we design for needs to be deployed and evaluated and it is expected to last at least until the end of the next decade References Buzzi et survey of techniques for networks and challenges ahead IEEE Sel Areas Commun The data challenge Qualcomm Technical report http Accessed Jan Yang Zhang Introduction In Yang Zhang W eds Interference Coordination for Cellular Networks SpringerBriefs in Electrical and Computer Engineering pp Springer Cham https Salem et energy consumption optimization in networks using multilevel beamforming and large scale antenna systems In Proceedings of IEEE Wireless Communications and Networking Conference pp https López et An approach to data analysis in networks Entropy https Ejaz et Internet of Things IoT in wireless communications IEEE Access https Pérez et capabilities in networks NFV SDN coordination in complex use case In Proceedings of EuCNC pp Khan et Location awareness in networks using RSS measurements for public safety applications IEEE Access Big Data in Distributed Applications Mohammed Humbe Chowhan review of big data environment and its related technologies In Proceedings of International Conference on Information Communication and Embedded Systems ICICES pp Hu Opportunities in Networks Research and Development Perspective CRC Press Boca Raton TS NR Positioning Protocol NRPPa TR Study on scenarios and requirements for next generation access technologies TR Study on positioning use cases Stage TR Stage functional speciﬁcation of User Equipment UE positioning in Soret et Interference coordination for new radio IEEE Wirel Commun Milosevic Dimitrijevic Drajic Nikolic Tosic LTE and WiFi existence in GHz unlicensed band Facta Universitatis Electron Energ Tosic et Semantic coordination protocol for LTE and coexistence In Proceedings of European Conference on Networks and Communications EUCNC Athens Greece pp Rizvi et An investigation of energy efﬁciency in wireless networks In Proceedings of International Conference on Circuits System and Simulation ICCSS pp Abdalla Arifﬁn The enhanced user prediction in In Proceedings of the Fourth International Conference on Advances in Computing Communication and Information Technology CCIT pp What is GNSS https Accessed Jan Kacianka Hellwagner Adaptive video streaming for UAV networks In Proceedings of the ACM International Workshop on Mobile Video MoVid pp Petrangeli et Live streaming of deﬁnition video over the Internet In Proceedings of the International Conference on Multimedia Systems Article pp https Nallappan et Live streaming of uncompressed video using Terahertz wireless links In Proceedings of IEEE International Conference on Communications ICC pp https Meroni Di Ciccio Mendling process monitoring dynamically binding objects to running processes In Proceedings of the Forum and Doctoral Consortium Papers Presented at the International Conference on Advanced Information Systems Engineering CAiSE Essen Germany pp Meroni Plebani Combining monitoring with blockchain analysis and solutions In Matulevičius Dijkman R eds CAiSE LNBIP vol pp Springer Cham https Baresi et mArtifact an process monitoring platform In Proceedings of BPM Demo Track with International Conference on Business Process Modeling BPM Barcelona Spain pp Weber Xu Riveret Governatori Ponomarev Mendling Untrusted business process monitoring and execution using blockchain In Rosa Loos Pastor eds BPM LNCS vol pp Springer Cham https Wolf et DNAseq workﬂow in diagnostic context and an example of user friendly implementation Biomed Biotechnol Nejkovic et Wolf et Safe variant annotation sharing across laboratories In Variant Detection Spain Böck et Single CpG hypermethylation allele methylation errors and decreased expression of multiple tumor suppressor genes in normal body cells of and breast cancer patients Int J Cancer Parallel Programming with Java https Accessed Jan Wolf Kuonen Dandekar Parallélisme et distribution orienté objet In Proceedings of the Conférence d informatique Parallélisme Architecture et Système Neuchâtel Switzerland pp Wolf Loïc Kuonen FriendComputing organic application centric distributed computing In Proceedings of the Second International Workshop on Sustainable Ultrascale Computing Systems NESUS Krakow Poland pp Kuonen Clement Bapst Securing the grid using virtualization the ViSaG model In Proceedings of the Fifth International Conference on Cloud Computing GRIDs and Virtualization Cloud Computing Venice Italy pp Enabling Mobile Augmented and Virtual Reality with Networks AT T https Accessed Jan VR and AR pushing connectivity limits Qualcomm Technologies https Accessed Jan Augmented and Virtual Reality the First Wave of Killer Apps Qualcomm Technologies https Accessed Jan Koivisto et Joint device positioning and clock synchronization in networks IEEE Trans Wirel Commun Koivisto Hakkarainen Costa Kela Leppänen Valkama efﬁciency device positioning and communications in dense networks IEEE Commun Mag Talvitie Valkama Destino Wymeersch Novel algorithms for joint position and orientation estimation in mmWave systems In Proceedings of IEEE Globecom Workshops pp Talvitie Levanen Koivisto Pajukoski Renfors Valkama Positioning of trains using new radio synchronization signals In Proceedings of IEEE Wireless Communications and Networking Conference WCNC pp Costa Koivisto Talvitie Leppänen Valkama device positioning in mmWave systems under orientation uncertainties In Proceedings of Asilomar Conference on Signals Systems and Computers Witrisal et positioning for indoor applications RFID UWB and beyond In Proceedings of IEEE International Conference on RFID pp Nam Joshi Unmanned aerial vehicle localization using distributed sensors Int Distrib Sens Netw Zhang Lu Wang Wang Cooperative localization in networks survey ICT Express Dammann Raulefs Zhang On prospects of positioning in In Proceedings of IEEE International Conference on Communication Workshop ICCW pp Tosic Nikolic Nejkovic Dimitrijevic Milosevic Spectrum sensing coordination for FIRE LTE testbeds Invited Paper In Proceedings of International Conference on Electrical Electronic and Computing Engineering IcETRAN Big Data in Distributed Applications Managing semantic big data for intelligence Proc STIDS intelligent wireless Internet of Things networks Nikolic Tosic Milosevic Nejkovic Jelenkovic Spectrum coordination for In Proceedings of Telecommunication Forum TELFOR Belgrade Serbia pp https Nejkovic Petrovic Milosevic Tosic The SCOR ontologies framework for robotics testbed In Telecommunication Forum TELFOR Belgrade Serbia pp https Petrovic Nejkovic Milosevic Tosic semantic framework for RIoT device mission coordination In Proceedings of Telecommunication Forum TELFOR Belgrade Serbia pp https Jelenkovic Tosic Nejkovic Semantic driven code generation for networking testbed experimentation Enterp Inf Syst https Redis http Accessed Jan Cois Palko data collection and real time analytics using Redis In Proceedings of STRATA Reilly Nakamura LINE Storage Storing billions of rows in and HBase per Month http Apache Hadoop http Accessed Jan Apache HBase http Accessed Jan Yao et Online anomaly detection for sensor systems simple and efﬁcient approach Perform Eval Apache Jena http Accessed Jan OWL Web Ontology Language Overview http Accessed Jan Prud Hommeaux Seaborne SPARQL query language for RDF dation Stocker Shurpali Taylor Burba Rönkkö Kolehmainen Emrooz scalable database for SSN observations International Workshop on Semantic Sensor Networks and Terra Cognita ISWC pp In Proceedings of First Joint Broekstra Kampman van Harmelen Sesame generic architecture for storing and querying RDF and RDF schema In Horrocks Hendler J eds ISWC LNCS vol pp Springer Heidelberg https Todorovic Rancic Markovic Mulalic Ilic Named entity recognition and classiﬁcation using context hidden Markov model In Symposium on IEEE Proceedings of Neural Network Applications in Electrical Engineering NEUREL pp Hermans et lightweight approach to online detection and classiﬁcation of interference in sensor networks ACM SIGBED Rev Iyer Hermans Voigt Detecting and avoiding multiple sources of interference in the GHz spectrum In Abdelzaher Pereira Tovar eds EWSN LNCS vol pp Springer Cham https Schmidhuber Deep learning in neural networks an overview Neural Netw Ahmad et security analysis of threats and solutions In Proceedings of IEEE Conference on Standards for Communications and Networking CSCN pp Nejkovic et Fang Qian Hu Security for mobile wireless networks IEEE Access Cho et System level simulation for cellular communication systems In Proceedings of Ninth International Conference on Ubiquitous and Future Networks ICUFN pp Mattisson Overview of requirements and future wireless networks In Proceedings of ESSCIRC IEEE European Solid State Circuits Conference pp Nichita et propagation current solutions and future proposals In Proceedings of IEEE International Symposium on Electronics and Telecommunications ISETC pp Third Generation Partnership Project http Accessed Jan Nagul review on modulation schemes and their comparisons for future wireless communications In Proceedings of Conference on Signal Processing and nication Engineering Systems SPACES pp Suthaharan Big data classiﬁcation problems and challenges in network intrusion prediction with machine learning SIGMETRICS Perform Eval Rev Casetti et Cognitive Network Management for white paper In Working Group on Network Management and QoS https Accessed Jan et Efﬁcient machine learning for big data review Big Data Res Ayoubi et Machine learning for cognitive network management IEEE Commun Mag Imran Zoha Challenges in how to empower with big data for enabling IEEE Netw Jakóbik Big data security In Pop Kołodziej Di Martino B eds Resource Management for Big Data Platforms CCN pp Springer Cham https Xie et survey of machine learning techniques applied to software deﬁned networking SDN research issues and challenges IEEE Commun Surv Tutor Abdallah et network management framework for SDN In Proceedings of IFIP International Conference on New Technologies Mobility and Security NTMS pp Open Access This chapter is licensed under the terms of the Creative Commons Attribution International License http which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give priate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this chapter are included in the chapter s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the chapter s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder Big Data Processing Analysis and Applications in Mobile Cellular Networks Sanja B Olivera Nastasija Horacio Siegfried Enes and Apostolos BioSense Institute University of Novi Sad Novi Sad Serbia novovic Cloud Competency Centre National College of Ireland Dublin Ireland horacio Computer Science and Engineering Department Faculty of Automatic Control and Computers University Politehnica of Bucharest Bucharest Romania Faculty of Computer Science University of Vienna Vienna Austria Department of Informatics Aristotle University of Thessaloniki Thessaloniki Greece papadopo Abstract When coupled with context data collected in mobile cellular networks provide insights into patterns of human activity interactions and mobility Whilst uncovered patterns have immense potential for improving services of telecom providers as well as for external applications related to social wellbeing its inherent massive volume make such Big Data sets complex to process niﬁcant number of studies involving such mobile phone data have been presented but there still remain numerous open challenges to reach nology readiness They include eﬃcient access in ner high performance computing environments scalable data analytics innovative data fusion with other ﬁnally linked into the cations ready for operational mode In this chapter we provide broad overview of the entire workﬂow from raw data access to the ﬁnal cations and point out the critical challenges in each step that need to be addressed to unlock the value of data generated by mobile cellular networks Keywords Data analysis HPC Big Data Cellular networks This article is based upon work from COST Action elling and Simulation for Big Data Applications cHiPSet supported by COST pean Cooperation in Science and Technology c The Author s Ko lodziej and Eds cHiPSet LNCS pp https Brdar et Mobile Cellular Networks From Data to Applications There is tremendous growth of new applications that are based on the analysis of data generated within mobile cellular networks Mobile phone service providers collect large amounts of data with potential value for improving their services as well as to enable social good applications As an example every time user makes via mobile phone interaction SMS call internet call detail record CDR is created and stored by mobile network operator CDRs not only log the user activity for billing purposes and network management but also vide opportunities for diﬀerent applications such as urban sensing transport planning disaster management analysis and monitoring epidemics of infectious diseases Several studies have reviewed applications to analyse CDRs however most focus on speciﬁc aspects such as data analytics for internal use in telecom panies graph analytics and applications or public health This vey aims to cover the entire workﬂow from raw data to ﬁnal application with emphasis on the gaps to advance technology readiness Figure depicts our main concept which shall be used to summarise the state of the art work and identify open challenges Fig Mobile cellular networks from location data to applications The rest of this paper is structured as follows Section provides some background on mobile cellular networks and the nature of the data sets able It also sets the basis for diﬀerent approaches to anonymization Section presents discussion of approaches and architectures to deal with the nature of detecting patterns from telecom data Then Sect discusses approaches to analyze mobile operators data sets via graph analysis and machine learning Section enumerates some relevant external data sources that can complement mobile phone data while Sect Mobile Cellular Networks elaborates on diverse pertinent applications Finally Sect furnishes the mary and objectives for future research eﬀorts Data Anonymization and Access With the pervasive adoption of smartphones in modern societies in addition to CDRs there is now growing interest in xDRs Extended Data Records They enclose information on visited web sites used applications executed tions etc Coupled with triangulation applications can infer grain phone locations thus making data volumes even larger Telecom data typically include spatial and temporal parameters to map device activity nectivity and mobility Telecom operators follow rigorous procedures for data anonymization to serve privacy such that anonymized records can not be linked to subscribers under any normal circumstances Furthermore before releasing any data to third ties data sets are usually aggregated on temporal spatial scales For example the numbers of calls as well as the duration of calls between any pair of antennas are aggregated hourly and movement trajectories are provided with reduced spatial resolution Diﬀerential privacy paradigm adds noise to inal data up to the level not aﬀecting the statistics signiﬁcantly to preserve users privacy Another approach suggested by the Open Algorithms OPAL initiative proposes moving the algorithm to the data In their model raw data are never exposed to outside parties only vetted algorithms run on telecom companies servers An example of preserving privacy of users by releasing only data is Telecom Italia Big Data Challenge Opened data sets accumulated activity and connectivity across deﬁned spatial cells of the city of Milan and in the Province of Trentino in min resolution Despite aggregation data sets are still rich source of information especially when fused with other data such as weather news social networks and electricity data from the city To get some useful insight about the data we further describe and visualize activity and connectivity maps from Telecom Italia data sets and mobility from Telekom Srbija data set Activity The activity data set consists of records with square id time interval activity activity activity activity internet traﬃc activity and country code for each square of grid network The data is aggregated in ten minutes time slots We did further aggregation on daily level to gain overall insight into daily base activity Figure illustrates an aggregated activity of mobile phone users in the city of Milan We observe that areas with highest activity refer to urban core of the city whereas areas with lower activity levels refer to peripheral parts of the city The same analysis is performed for the Province of Trentino and corresponding results are presented in Fig Although Brdar et the inspected area of the Trentino Province exceeds signiﬁcantly the urban area of the city of Trentno the same pattern in distribution of mobile phone activity is present high activity in urban area along lower activity in rural areas From the visual inspection of Fig we observe that higher activity areas spatially refer to transit areas with main roads which was expected Fig Aggregated activity over spatial area of the city of Milan Connectivity Connectivity data provides directional interaction strength among the squares cells of the grid network Records consist of timestamp square square and strength which represents the value weight of aggregated telecom traﬃc multiplied with constant k to hide exact number of calls and sms recorded by single base station As in we performed additional spatial aggregation and analyzed connectivity patterns between diﬀerent city zones of Milan through the lens of graph theory For illustration purposes we created single undirected weighted graph for typical working day from the data set In Fig we present the obtained spatial graph of connectivity links During the work week the city center acts as hub the strongest links are gathered close to the city center while on weekends and holidays the opposite pattern occurs The second type of connectivity data presents connectivity from the city of Milan to other Provinces in Italy Additional aggregation is applied to extract Mobile Cellular Networks Fig Aggregated activity over spatial area of Trentino Province Fig Connectivity across the city of Milan Brdar et Fig Connectivity from the city of Milan to Provinces daily base connectivity patterns Figure presents connectivity links from ferent areas of the city of Milan to Provinces in Italy We may conclude that the distribution of connectivity links is regular to all Provinces and that the majority of links start from central areas of the city of Milan Mobility Mobile phone data can reveal the approximate location of user and its mobility trace based on geographical location of the Radio Base Stations which registered Mobile Cellular Networks Fig Mobility across the city of Novi Sad Serbia the traﬃc In the authors proposed novel computational framework that enables eﬃcient and extensible discovery of mobility intelligence from scale data such as CDR GPS and Location Based Services data In the authors focus on usage of Call Detail Records CDR in the context of mobility transport and transport infrastructure analysis They lyzed CDR data associated with Radio Base Stations together with Open Street Map road network to estimate users mobility CDR data can provide ized view of users mobility since data is collected only when the telecom traﬃc happens To illustrate mobility data set we created Fig that presents map with mobility traces across the city of Novi Sad on July for the time interval between and extracted from raw CDR data through gation of visited locations sequences of anonymous users Data originate from Serbian national operator Telekom Srbija released under ment From mobility traces we can detect few locations in the city that acts as trajectory hubs Big Data Processing The typical workﬂow applied for processing data such as mobile phone data used in this case study contains numerous queries across tions and timestamps of interest aggregations and summarization Brdar et Existing solutions are rarely focusing on the execution time scalability and throughput that are of high importance for the implementation and near time settings In this section we present brieﬂy some important concepts and architectural issues related to processing Big Data Big Data Architectures Over the last decade we have witnessed tremendous progress and innovation in data processing systems and the associated computation Among many others these include computational systems data streaming technologies and NoSQL database systems major challenge is to build systems that on the one hand could handle large volumes of batch data and on the other hand oﬀer the required scalability performance and low latency required for integration and processing of massive continuous data streams In the following paragraphs we discuss some of the architectural principles underlying Big Data systems that address this challenge in particular the Lambda and the Kappa architectural alternatives Lambda Architecture Big Data systems often face the challenge of how to integrate processing of new data that is being constantly ingested into system with historical batch data Newly arriving data is usually processed using processing techniques while historical data is periodically reprocessed using batch processing The Lambda architecture is blueprint for Big Data system that uniﬁes stream processing of data and batch processing of historical data The Lambda architecture pursues generalized approach to developing Big Data systems with the goal of overcoming the complexities and limitations when trying to scale traditional data systems based on incrementally updated tional databases In an incremental database system the state of the database its contents is incrementally updated usually when new data is processed In contrast to incremental database systems the Lambda architecture advocates functional approach relying on immutable data new data is added on top of the immutable historical data batch data already present in the system As opposed to traditional distributed database systems where bution of tables across multiple machines to be explicitly dealt with by the developer key underlying principle of the Lambda architecture is to make the system aware of its distributed nature so that it can automatically manage bution replication and related issues Another key aspect of the Lambda tecture is its reliance on immutable data as opposed to incrementally updated data in relational database systems Reliance on immutable data is essential for achieving resilience with respect to human errors The Lambda architecture promises to tackle many important requirements of Big Data systems including scalability robustness and fault tolerance including with respect to human errors support for reads and updates extensibility easier debugging and maintainability At of Mobile Cellular Networks abstraction the Lambda architecture is comprised of three layers the batch layer the serving layer and the speed layer The batch layer stores the raw data also often referred to as batch data historical data or master data set which is immutable Whenever new data arrives it is appended to the existing data in the batch layer The batch layer is responsible for computing batch views taking into account all available data The batch layer periodically recomputes the batch views from scratch so that also the new data that been added to the system since the computation of the last batch views is processed The serving layer sits on top of the batch layer and provides read access to the batch views that have been computed by the batch layer The serving layer usually constitutes distributed database which is populated with the puted batch views and ensures that the batch views can be randomly accessed The serving layer is constantly updated with new batch views once these become available Since the serving layer only needs to support batch updates and dom reads but random writes updates it is usually signiﬁcantly less plex than database that needs to support random reads and writes While the serving layer enables fast access to the batch views it must be clear that these views may not be completely since data that been acquired since the latest batch views have been computed have not been considered The speed layer is provided on top of the serving layer in order to support views on the data The speed layer mitigates the high latency of the batch layer by processing the data as it arrives in the system using fast incremental algorithms to compute views of the data As opposed to the batch layer which periodically recomputes the batch views based on all historical data form scratch the speed layer does not compute views from scratch To minimize latency it only performs incremental updates of the views taking into account just the newly arrived data The views provided by the speed layer are of temporary nature Once the new data arrived at the batch layer and been included in the latest batch views the corresponding views can be discarded Figure depicts the main architectural aspects of the Lambda architecture Data streamed in from data sources sensors Web clients etc is being fed in parallel both into the batch layer and the speed layer which compute the corresponding batch views and views respectively The lambda architecture can be seen as between two conﬂicting goals speed and accuracy While computation of views is being done with very short latencies computation of batch views is typically very latency process On the other hand since the speed layer does not take into account all of the available data views are usually only approximations while batch views provide accurate answers considering all data available in the master data store at certain point in time In order to get view of all the available data batch data and new data queries have to be resolved such that Brdar et Fig The Lambda architecture they combine the corresponding and views which can either be done in the serving layer or by the client applications The Lambda architecture been widely recognized as viable approach to unifying batch and stream processing by advocating stream processing and batch on immutable data There are however some potential drawbacks associated with the Lambda architecture Although major objective of the lambda architecture is to reduce the complexity as compared to traditional distributed database systems this goal often can not be fully realized While the batch layer usually hides complexity from the developers typically by relying on some MapReduce framework Hadoop the speed layer may still exhibit signiﬁcant complexities to the developers of Big Data solutions In tion having to develop and maintain two separate data processing components the stream layer and the batch layer adds to the overall complexity Another potential issue with the Lambda architecture is that constantly recomputing the batch views from scratch might become prohibitively expensive in terms of resource usage and latency Kappa Architecture limitation of the Lambda architecture is that two diﬀerent data processing systems the stream layer and the batch layer have to be maintained These layers need to perform the same analytics however realized with diﬀerent technologies and tools As consequence the system becomes more complex and debugging and maintenance become more diﬃcult This drawback is being addressed by the Kappa architecture The Kappa architecture constitutes simpliﬁcation of the Lambda tecture by uniformly treating data and batch data as streams sequently batch processing as done in the lambda architecture is replaced by Mobile Cellular Networks Fig The Kappa architecture stream processing The Kappa architecture assumes that historical batch data can also be viewed as bounded stream which is often the case What is required however is that the stream processing component also supports cient replay of historical data as stream Only if this is the case batch views can be recomputed by the same stream analytics engine that is also responsible for processing views Besides the ability to replay historical data the order of all data events must be strictly preserved in the system in order to ensure deterministic results Instead of batch layer and speed layer the Kappa architecture relies on single stream layer capable of handling the data volumes for computing both views and batch views Overall system complexity decreases with the Kappa architecture as illustrated in Fig However it should be noted that the Kappa architecture is not replacement of the Lambda architecture since it will not be suitable for all use cases Big Data Frameworks There is plethora of Big Data frameworks and tools that have been oped in the past decade As result both the Lambda architecture and Kappa architecture can be implemented using variety of diﬀerent technologies for the diﬀerent system components In the following we brieﬂy discuss few works that are most typically used to implement Big Data systems based on the Lambda or Kappa architecture Hadoop The Apache Hadoop ecosystem is collection of tools for developing scalable Big Data processing systems The Hadoop File System HDFS is distributed ﬁle system for storing large volumes of data on distributed memory machines clusters transparently handling the details of data bution replication and The Hadoop MapReduce engine utilizes HDFS to support transparent parallelism of batch processing that can be Brdar et formulated according to the MapReduce programming model Hadoop is often used to implement the batch layer in data processing systems that implement the Lambda Architecture Spark Apache Spark introduces Resilient Distributed Data sets RDDs and Data Frames DFs Spark can work nicely within the Hadoop tem although this is not mandatory since Spark is with respect to task scheduling and fault tolerance Moreover it supports large collection of data sources including HDFS Spark supports iterative MapReduce tasks and improves performance by explicitly enabling caching of distributed data sets wide range of functions support categorization of application components into data transformations and actions In addition Spark provides stream processing functionality rich machine learning library powerful library for SQL cessing on top of Data Frames and also library speciﬁcally designed for graph processing GraphX Spark is often used for implementing the speed layer in Lambda or the stream layer in Kappa architecture Kafka Apache Kafka is scalable message queuing and log aggregation platform for data feeds It provides distributed message queue and messaging model for streams of data records supporting distributed data storage The framework is run as Kafka cluster on multiple servers that can scale over multiple data centers Kafka supports eﬃcient replay of data streams and thus it is often used to implement systems that resemble the Kappa architecture Samza Apache Samza is scalable distributed stream processing platform that been developed in conjunction with Apache Kafka and that is often used for implementing Big Data systems based on the Kappa architecture Samza can be integrated easily with the YARN resource management framework Resource Management Frameworks YARN is resource negotiator included with Apache Hadoop YARN decouples the programming paradigm of MapReduce from its resource management capabilities and delegates many scheduling functions task to components Apache Mesos is resource negotiation engine that supports ing and management of large cluster of machines between diﬀerent computing frameworks including Hadoop MPI Spark Kafka etc The main diﬀerence between YARN and Mesos is the resource negotiation model Whereas YARN implements resource negotiation approach where clients specify their resource requirements and deployment preferences Mesos uses approach where the negotiator oﬀers resources to clients which they can accept or decline Mobile Cellular Networks Data Analysis Data Analysis is the scientiﬁc process of examining data sets in order to discover patterns and draw insights about the information they contain In the case of data collected by mobile phone providers typically in the form of CDRs the analysis focuses in two main directions i graph analysis and ii machine ing Moreover the data analysis must incorporate the teristics of such data Graph Analytics Graph mining is heavily active research direction with numerous tions that uses novel approaches for mining and performing useful analysis on datasets represented by graph structures Current research directions can be categorized into the following groups i Graph clustering used for ing vertices into clusters ii Graph Classiﬁcation used for classifying separate individual graphs into two or more categories iii Subgraph mining used for producing set of subgraphs occurring in at least some given threshold of the given input example graphs One of the core research directions in the area of graph clustering is the discovery of meaningful communities in large network from the tive of data that evolves over time In the majority of applications graphs are extremely sparse usually following degree distribution However the original graph may contain groups of vertices called communities where vertices in the same community are more than vertices across communities In the case of CDR data the graph sponds to user interactions and communities correspond to groups of people with strong activity within the group delimited by aries To enable eﬃcient community detection in potentially massive amounts of data the following problems must be tackled i the algorithmic techniques applied must scale well with respect to the size of the data which means that the algorithmic complexity should stay below where n is the number of graph nodes and ii since these techniques are unsupervised the algorithms used must be ﬂexible enough to be able to infer the number of communities ing the course of the algorithm Moreover the temporal dimension of the data must be taken into account when detecting communities to better understand the natural evolution of user interactions Some algorithms that qualify for this task are Louvain Infomap Walktrap FastGreedy etc The result of community detection analysis is set of grouped vertices that have very strong inner connectivity The results could be presented on the map since telecom data is georeferenced In Fig we present geographical map of Milan city with wide suburban area overlayed with the results of community detection analysis in Communities that have smaller overall area are sented with higher bars From visual inspection of Fig we can notice that the dense urban area of the city larger number of small communities while Brdar et in the sparsely populated suburban area there are few very large ties High number of communities within small spatial area is reﬂecting dynamic nature of telecom traﬃc in urban areas which is strongly related to people ﬂow and its dynamic across the city Fig Communities over the city of Milan in Collective classiﬁcation and label propagation are two important research directions in the area of graph classiﬁcation for vertex classiﬁcation Iterative classiﬁcation is used for collective classiﬁcation to capture the similarity among the points where each vertex represents one data point either labeled or belled Label propagation is converging iterative algorithm where vertices are assigned labels based on the majority vote on the labels of their bors In the case of CDR data these algorithms can be used to draw insights about users and their neighborhoods by ﬁnding the correlations between the label of user and i its observed attributes ii the observed attributes including observed labels of other users in its neighborhood iii the unobserved labels of users in its neighborhood The dimension of the data also plays an important role as the correlations will bring new insight into the propagation of labels and the way user neighborhood is built Subgraph mining deals with the identiﬁcation of frequent graphs and graphs that can be used for classiﬁcation tasks graph clustering and building indices In the case of CDR data subgraph mining can help to detect hidden patterns in active user communities delimited into boundaries by contrasting the support of frequent graphs between various diﬀerent graph classes or to classify user interaction by considering frequent patterns using the dimensions as cardinal feature Mobile Cellular Networks Machine Learning data analysis is an important and evolving domain of machine learning The main direction when dealing with such data is forecasting and prediction in support of the process Classical machine learning techniques from simple ones for sequential tern mining Apriori Generalized Sequential Pattern FreeSpan pan SPADE to more complex ones Linear Multilinear Logistic Poisson or Nonlinear Regression can be used to capture the dependencies between tial and temporal components and help with making accurate predictions into the future and extract new knowledge about the evolution of users and their interests With the increasing evolution and adoption of neural networks new deep learning architectures are developed for the analysis of data and used for making and quantifying the uncertainty associated with predictions These techniques can be employed in the process of making accurate predictions for data when working in both big data and data scarce regimes managing to quantify the uncertainty associated with predictions in manner Data Fusion Identiﬁed patterns from telecom data reach true value when combined with other sources As illustrated in Fig processed and analyzed telecom data can be fused with diverse data sources in context of various applications We rized several fusion scenarios in Table The list is not exhaustive only highlights diversity of the combinations and some of the examples might integrate mobile phone data with more than one external source Satellite data environmental data IoT POI National statistics and other sources can add to the value of mobile phone data For example satellite data can provide information on land cover types and changes and IoT can collect valuable ground truth measurements Bringing together heterogeneous datasets with mobile phone data and using them jointly is challenging due to typical mismatch in the resolutions of data multimodal and dynamic nature of data Some applications on mobile phone data demand external sources only for training and validation learning model to predict indicators based on features extracted from telecom data Here special attention is needed to understand the bias and avoid spurious correlations Other scenarios demand continuous information ﬂow from external source and dynamic integration air quality measurements fused with aggregated mobility from telecom data The main challenge here is the timely processing of external data and proper alignment with mobile phone data Fusion scenarios reported in Table illustrate heterogeneity of external data sources all having an important role in unlocking the value of mobile phone data coming from telecom operators The quality of ﬁnal application depends on the Brdar et Table Data fusion scenarios mapping external data sources with telecom data External data source Examples POI Satellite data NASAs Tropical Rainfall Measurement Mission TRMM satellite anomalous patterns of mobility and calling frequency for deriving impact map of ﬂoods aggregated activity by day and by antenna satellite for calculating vegetation index average number of calls between all market pairs Environmental data The air quality estimated by regional model staying at home and travel patterns Availability of environmental freshwater measured as the total length of the rivers in each spatial unit estimate of mobility obtained from CDRs Logs of the climatic conditions temperature relative humidity air pressure and wind speed from weather stations inferring the social network for each subject Events on famous POIs across city users presences in the area POIs from Google Earth for land use inference aggregated number of calls managed by each of base transceiver station towers POIs aggregated distributions of number of connected devices and downloaded information from xDR records Inductive loop vehicle detectors mobility rush hours traﬃc Travel surveys daily commuting from mobility traces patterns Census on journey to work activity and connectivity around laborshed area Demographic and health surveys connectivity and mobility across country National statistics on development human mobility patterns Household income and expenditure survey top up credit amounts mobility and social network features Census Surveys IoT Infrastructure The street network highways and primary streets from OpenStreetMap metro network bus routes xDR data aggregated into OD matrices Customer sites of each power line per grid square and line measurement indicating the amount of ﬂowing energy aggregated people dynamics features from the mobile phone network activity Mobile Cellular Networks Fig Fusion of mobile phone data with other sources availability of external sources eﬃciency of data processing and the quality of delivered information and its integration Applications plethora of research work been published related to the usage of telecom data for multitude of purposes Telecom data contains rich user behaviour mation and it can reveal mobility patterns activity related to speciﬁc locations peak hours or unusual events Extracting frequent trajectories home and work location detection origin destination matrices are further examples of edge that may be mined from rich telecom data Telecom operators have great interest to analyze collected data for optimizing their services For example pricing schemes can maximize operators proﬁt as well as users grade of service Dynamic data pricing frameworks combining both spatial and temporal traﬃc patterns allow estimating optimal pricing rewards given the current network capacity Telecom data signiﬁcantly enriched many diﬀerent ﬁelds and boosted external social good applications Studies in transportation urban and energy planning public health economy and tourism have beneﬁted most from this valuable new resource that surpasses all alternative sources in population coverage spatial and temporal resolution Transportation planning applications need information on diﬀerent modes of trips purposes and times of day With telecom data transportation models can eﬀectively utilize mobility footprints at large scale and resolution This was validated by an MIT study on the Boston metropolitan area where the authors Brdar et demonstrated how CDR data can be used to represent distinct mobility patterns In another example origin destination matrices inferred from mobile phone data helped IBM to redesign the bus routes in the largest city of Ivory Coast Abidjan Mobility patterns derived from telecom data could be very valuable for lic health applications in particular epidemiology Surveillance prioritization and prevention are key eﬀorts in epidemiology Mobile phone data demonstrated utility for dengue HIV malaria schistosomiasis Ebola epidemic and cholera outbreaks Another suitable public health cation is concerned with air quality where recent studies embraced telecom data to better quantify individual and population level expose to air pollution In the authors highlighted the need to dynamically assess exposure to N that high impact on peoples health Their method incorporated individual travel patterns Urban studies highly explored the potential of mobile phone data and ered that it can be used for urban planning detecting social function of land use in particular residential and oﬃce areas as well as and rush hour patterns and extracting relevant information about the structure of the cities Recent applications propose an analytical process able to cover understand and characterize city events from CDR data and method to predict the population at large scale in city All urban studies ﬁt into the wider context of smart city applications and therefore more breakthroughs on the usage of mobile phone data are expected With the growing role of tourism there is increased interest to investigate utility of mobile phone data to understand tourists experiences evaluate keting strategies and estimate revenues generated by touristic events Mobility and behaviour patterns have been recently used to derive trust and reputation models and scalable data analytics for the tourism industry The Andorra case study proposed indicators in high spatial and temporal resolutions such as tourist ﬂows per country of origin ﬂows of new tourists revisiting patterns proﬁling of tourist interests to uncover valuable patterns for tourism Special attention is given to large scale events that attract foreign people Arguably tourists via their mobile devices have quickly become data sources for sourced aggregation with dynamic spatial and temporal resolutions Other high impact applications include social and economical ment disaster events management such as cyclones landfall or quakes and food security Although many studies demonstrated utility of mobile phone data in various applications reaching the operational level is still not that close If we recall the summary of workﬂow s steps provided in Fig all further described in the previous sections we can realize that technologies used in each step need to match with speciﬁc application Mobile Cellular Networks Summary and Vision This chapter provided an overview of all steps in discovering knowledge from raw telecom data in the context of diﬀerent applications Knowledge about how people move across city where they are gathering what are home work and leisure locations along with corresponding time component are valuable for many applications The biggest challenges in this process are privacy and regulation settings and data fusion with external sources Eﬀorts directed toward providing access to telecom human ioral data in manner are necessary settings raise critical issues concerning computational infrastructure big data works and analytics There is lack of research and benchmark studies that evaluate diﬀerent computational architectures and big data frameworks Only few studies tackled issues of parallelization and distributed processing In authors proposed mobility intelligence framework based on Apache Spark for processing and analytics of large scale mobile phone data Another example is the study that provided computational pipeline for the community detection in mobile phone data developed in Apache Hive and Spark technology and marked diﬀerent architectures and settings More of these studies are needed to choose the right architecture and processing frameworks Graph analytics together with machine learning have become indispensable tools for telecom data analytics but the streaming nature of data demands for change detection and online adaption External data sources mentioned in the data fusion section are also advancing new satellites launched enhanced IoT ecosystems and will help us to understand context better Future research must address all critical aspects to reach technology readiness for operational environment This will enable applications based on mobile phone data to have high impact on decision making in urban transport public health and other domains and will certainly open opportunities for new applications References Acs Castelluccia case study privacy preserving release of temporal density in Paris In ACM SIGKDD International Conference on Knowledge Discovery and Data pp ACM New York https Aggarwal Wang Managing and Mining Graph Data Springer New York https Alexander Jiang Murga trips by purpose and time of day inferred from mobile phone data Transp Res Part C Emerg Technol https Barlacchi et dataset of urban life in the city of milan and the province of trentino Sci Data Becker et tale of one city using cellular network data for urban planning IEEE Pervasive Comput https Brdar et Berlingerio Calabrese Di Lorenzo Nair Pinelli Sbodio AllAboard system for exploring urban mobility and optimizing public transport using cellphone data In Blockeel Kersting Nijssen F eds ECML PKDD LNCS LNAI vol pp Springer Heidelberg https Blondel Decuyper Krings survey of results on mobile phone datasets analysis EPJ Data Sci https Blondel Guillaume Lambiotte Lefebvre Fast unfolding of communities in large networks Stat Mech Theory Exp https Bogomolov Lepri Larcher Antonelli Pianesi Pentland Energy consumption prediction using people dynamics derived from cellular work data EPJ Data Sci https Bosetti Poletti Stella Lepri Merler Domenico Reducing measles risk in turkey through social integration of Syrian refugees arXiv preprint Brdar Unveiling spatial ogy of HIV with mobile phone data Sci https Callegari Garroppo Giordano Inferring social information on eign people from mobile traﬃc data In IEEE International Conference on Communications ICC pp IEEE Chen et prediction of urban population using mobile phone location data Int Geogr Inf Sci Clauset Newman Moore Finding community structure in very large networks Phys Rev Cook Holder Mining Graph Data Wiley Hoboken https Dang et Mobility framework for mobility intelligence from data In IEEE International Conference on Data Science and Advanced Analytics DSAA pp IEEE Dewulf et Dynamic assessment of exposure to air pollution using mobile phone data Int J Health Geogr Ding Li Zhang Jin Time dependent pricing for mobile networks of urban environment feasibility and adaptability IEEE Trans Serv Comput Finger et Mobile phone data highlights the role of mass gatherings in the spreading of cholera outbreaks Proc Natl Acad Sci Fortunato Community detection in graphs Phys https Furletti Trasarti Cintia Gabrielli Discovering and understanding city events with big data the case of rome Information Gavric Brdar Culibrk Crnojevic Linking the human mobility and connectivity patterns with spatial HIV distribution NetMob Challenge pp Caro Parra Inferring modes of transportation using mobile phone data EPJ Data Sci Ferres Caro Bravo The eﬀect of go on the pulse of the city natural experiment EPJ Data Sci Mobile Cellular Networks Rydergren Breyer Rajna Travel demand estimation and network assignment based on cellular network data Comput Commun Yu Zhao Yin Yao Qiu Big data analytics in mobile cellular networks IEEE Access Jacques et Social capital and transaction costs in millet markets Heliyon Ahas Saluveer Derudder Witlox Mobile phones in traﬃc ﬂow geographical perspective to evening rush hour traﬃc analysis using call detail records PloS ONE https Jiang Fiore Yang Ferreira Frazzoli review of urban computing for mobile phone traces current methods challenges and opportunities In Proceedings of the ACM SIGKDD International shop on Urban Computing ACM Kreps Kafka distributed messaging system for log processing In Proceedings of the International Workshop on Networking Meets Databases NetDB Kreps Questioning the Lambda Architecture July https Accessed Dec Leal Malheiro Burguillo modelling of crowdsourced data Data Sci Eng https Leal Veloso Malheiro Burguillo Scalable modelling and recommendation using crowdsourced repositories tron Commer Res Appl https Leng Noriega Pentland Winder Lutz Alonso Analysis of tourism dynamics and special events through mobile phone metadata arXiv preprint Lepri Oliver Pentland Vinck Fair transparent and accountable algorithmic processes Philos Technol Lima Domenico Pejovic Musolesi Disease containment gies based on mobility and information dissemination Sci https Louail et From mobile phone data to the spatial structure of cities Sci Lu et Detecting climate adaptation with mobile network data in Bangladesh anomalies in communication mobility and consumption patterns ing cyclone Mahasen Climatic Change Mari et modeling unveils drivers of endemic schistosomiasis Sci Marz Warren Big Data Principles and Best Practices of Scalable Realtime Data Systems Manning New York City Montjoye et On the use of mobile phone data Sci Data https Noghabi Paramasivam Pan Ramesh Bringhurst Gupta Campbell Samza stateful scalable stream processing at linkedin Proc VLDB Endow Brdar et Brdar Evolving connectivity graphs in mobile phone data In NetMob The Main Conference on the Scientiﬁc Analysis of Mobile Phone Datasets pp Vodafone Oliver Matic Mobile network data for public health opportunities and challenges Front Publ Health Pappalardo Pedreschi Smoreda Giannotti Using big data to study the link between human mobility and development In IEEE International Conference on Big Data Big Data pp https et Flooding through the lens of mobile phone activity In IEEE Global Humanitarian Technology Conference GHTC pp IEEE October https Peak et Population mobility reductions associated with travel tions during the Ebola epidemic in Sierra Leone use of mobile phone data Int Epidemiol Pei Sobolevsky Ratti Shaw Li Zhou new insight into land use classiﬁcation based on aggregated mobile phone data Int Geogr Inf Sci Phithakkitnukoon Leong Smoreda Olivier Weather eﬀects on mobile social interactions case study of mobile phone users in Lisbon Portugal PloS ONE Pons Latapy Computing communities in large networks using random walks Graph Algorithms Appl Ramraj Prabhakar Frequent subgraph mining algorithms survey cedia Comput Sci https Rehman Khan Fong Graph mining survey of graph mining niques In International Conference on Digital Information Management ICDIM pp https Land use detection with cell phone data using topic models Case Santiago Chile Comput Environ Urban Syst Rosvall Bergstrom Maps of random walks on complex networks reveal community structure Proc Natl Acad Sci https Sen Namata Bilgic Getoor Galligher tive classiﬁcation in network data AI Mag Senanayake Jean Ramos Chowdhary Modeling and making in the spatiotemporal domain In Conference on Neural Information cessing Systems Steele et Mapping poverty using mobile phone and satellite data Roy Soc Interface https Brdar Papadopoulos Community detection in social networks In Ordonez Bellatreche L eds DaWaK LNCS vol pp Springer Cham https Veloso Leal Malheiro Burguillo Scalable data analytics using crowdsourced repositories and streams J Parallel Distrib Comput https Wang et Building replicated logging system with Apache Kafka Proc VLDB Endow https Mobile Cellular Networks Wesolowski et Quantifying the impact of human mobility on Malaria Science Wesolowski et Impact of human mobility on the emergence of dengue demics in Pakistan Proc Natl Acad Sci https White Hadoop The Deﬁnitive Guide edn Reilly Newton Wilson assessments of tion displacement using mobile phone data following disasters the Nepal Earthquake PLoS Curr et Rapid and near Zaharia et Resilient distributed datasets abstraction for cluster computing In Proceedings of the USENIX Conference on Networked Systems Design and Implementation NSDI USENIX ciation Berkeley http Zaharia et Apache spark uniﬁed engine for big dataprocessing Commun ACM https Zhu Ghahramani Learning from labeled and unlabeled data with label propagation Technical report Carnegie Mellon University June Zuﬁria et Identifying seasonal mobility proﬁles from anonymized and aggregated mobile phone data Application in food security PloS ONE Open Access This chapter is licensed under the terms of the Creative Commons Attribution International License http which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give appropriate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this chapter are included in the chapter s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the chapter s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder Medical Data Processing and Analysis for Remote Health and Activities Monitoring Salvatore B Michal Dragan Sabri Jose Mateusz Andrzej Farhoud Aleksandra Stojnev Ana Andrzej Agnieszka Dorin Cristina and Ioan University of Palermo Palermo Italy Research and Academic Computer Network Warsaw Poland University of Nis Niˇs Serbia Linnaeus University Sweden Universidad Carlos III Madrid Madrid Spain molina Interdisciplinary Research Institute in Wroclaw Wroc law Poland University of Turku Turku Finland Cracow University of Technology Poland ajakobik Universidade Lisboa Lisbon Portugal alrespicio Technical University of Romania Abstract Recent developments in sensor technology wearable puting Internet of Things IoT and wireless communication have given rise to research in ubiquitous healthcare and remote monitoring of human s health and activities Health monitoring systems involve cessing and analysis of data retrieved from smartphones smart watches smart bracelets as well as various sensors and wearable devices Such tems enable continuous monitoring of patients psychological and health conditions by sensing and transmitting measurements such as heart rate electrocardiogram body temperature respiratory rate chest sounds or blood pressure Pervasive healthcare as relevant application domain in this context aims at revolutionizing the delivery of medical services through medical assistive environment and facilitates the independent living of patients In this chapter we discuss data collection fusion ownership and privacy issues models technologies and solutions for c The Author s Ko lodziej and Eds cHiPSet LNCS pp https Medical Data Processing and Analysis medical data processing and analysis big medical data analytics for remote health monitoring research challenges and opportunities in medical data analytics examples of case studies and practical tions Keywords Internet of Things IoT Remote health monitoring Pervasive healthcare PH Introduction Recent developments in sensor technologies wearable computing Internet of Things IoT and wireless communications have given rise to research on mobile and ubiquitous and remote monitoring of peoples health and ities Health monitoring systems include processing and analysis of data retrieved from smart watches smart bracelets that is bands as well as various connected sensors and wearable devices Such systems enable continuous monitoring of patients psychological and health ditions by sensing and transmitting measurements such as heart rate cardiogram ECG body temperature respiratory rate chest sounds and blood pressure Collected fused and analyzed sensor data are important for sis and treatment of patients with chronic diseases such as hypertension and diabetes or for monitoring and assistance of elderly people The area of health informatics is explored by researchers with diﬀerent academic backgrounds puter scientists physicians mathematicians statisticians and sociologists All of them have something to contribute from medical knowledge though puter science that is simulations and data analysis to sociological and keting such as apps dissemination and social interventions There are models for instance diﬀerential equations or system dynamics that can be involved in understanding of cesses For example infectious disease models can use sensoric data that are collected from human contacts these models can be useful in risk assessment of patients This chapter addresses major challenges and opportunities related to the medical data collection modelling and processing in the context of monitoring of human s health and behavior To illustrate the use of discussed concepts and technologies we present three case studies The ﬁrst case study describes Big Data solution for remote health monitoring that considers challenges pertaining to monitoring the health and activities of patients We present the design and implementation of remote health monitoring system architecture that may be employed in public private or hybrid cloud The second case study describes system designed to support distributed monitoring of human health during trip It is assumed that travel agency that organizes trip to high tains equips the guide and tourists with sensors that monitor health parameters The third case study describes the use of machine learning in the context of monitoring daily living activities of people with dementia Detected anomalies Vitabile et in the daily living activities of people with dementia may help medical experts to distinguish between health decline symptoms caused by dementia and health decline symptoms incurred by the side eﬀects of the prescribed medications The rest of this chapter is structured as follows Section discusses data collection fusion ownership and privacy issues Models technologies and tions for medical data processing and analysis are addressed in Sect Section discusses big medical data analytics for remote health monitoring Research challenges and opportunities in medical data analytics are addressed in Sect Section describes examples of case studies and practical scenarios The chapter is concluded in Sect Medical Data Collection Fusion Ownership and Privacy Medical Data Collection Medicine always been trying to measure properties of patients such as clinical picture and how to understand their health and disease Computer ence would then incorporate the results of these measurements experiments and observation into models Recently the utilisation of IT tools for medicine such as machine learning undergone an accelerating growth however all els of such system are incomplete without real data especially For example with patient Electronic Health data can be provided with reasonably accuracy on clinical picture procedures or International standard Health Level Seven allows operability between IT providers and solutions However still in many countries most of medical data is analog and must be digitized image and sound cessing Moreover powerful computer facilities and ubiquitous wearable devices with multiple sensors have made possible the collection processing and storage of data on individual or group of patients Individual data can be used for risk assessment for particular health disorder For example the risk of being aﬀected by an infectious disease can be calculated based on sensor data and naires Data collected from group of patients potentially being in contact can be used for infectious disease risk assessment in given community There is concern about Data Protection Act in EU from legal perspective about ing such kind of data analysis which will be described in detail in this chapter The consent of each patient should be requested which could be very diﬃcult to obtain Blockchain for Privacy and Security The main application of Blockchain in medicine is traceability of drugs in many actors setup Such technique been already applied in Africa and Asia for verifying originality of drugs fake drugs are big lem there Blockchain technology can also help in investigating breaks where tractability of items in supply chain with acteristics is crucial Medical Data Processing and Analysis Data Fusion Data fusion DF is growing ﬁeld aiming to provide data for situation understanding Globally threat detection and facility protection are some of the vital areas in DF research Fusion systems aim to integrate sensor data and information in databases knowledge bases contextual information user mission in order to describe situations In sense the goal of information fusion is to attain simulation of subset of the world based on partial observations of it The ability to fuse digital data into useful information is hampered by the fact that the inputs whether or are generated in diﬀerent formats some of them unstructured Whether the information is unstructured by nature or the fact that information exchange metadata standards are ing or not adhered to all of this hinders automatic processing by computers Furthermore the data to be fused may be inaccurate incomplete ambiguous or contradictory it may be false or corrupted by hostile measures Moreover much information may be hard to formalize imaging information Consequently information exchange is often overly complex In many instances there is lack of communication between the various information sources simply because there is mechanism to support this exchange The high data ﬂow either or is unable to process and leads to time delays extra costs and even inappropriate decisions due to missing or incomplete information The key aspect in modern DF applications is the appropriate integration of all types of information or knowledge observational data knowledge models priori or inductively learned and contextual information Each of these categories distinctive nature and potential support to the result of the fusion process Observational Data Observational data are the fundamental data about the individual as collected from some observational capability sensors of any type These data are about the observable characteristic of person that are of interest Contextual Information Context and the elements of what could be called Contextual Information could be deﬁned as the set of circumstances rounding the acquired data that are potentially of relevance to its Because of its fusion implies the development of possible estimate taking into account this lateral knowledge We can see the context as background not the speciﬁc entity event or behaviour of prime interest but that information which is inﬂuential to the formation of best estimate of these items Learned Knowledge In those cases where priori knowledge for DF cess development can not be formed one possibility is to try and excise the knowledge through online machine learning processes operating on the vational and other data These are procedural and algorithmic methods for discovering relationships among concepts of interest that being captured by the system sensor data and contextual information There is tradeoﬀ Vitabile et involved in trying to develop algorithmic DF processes for complex problems where the insertion of human intelligence at some point in the process may be much more judicious choice At this time there are multitude of solutions to fusion most of which are centred in speciﬁc applications producing results The ability to automatically combine the results of smaller problems into larger context is still missing Medical Data Security Requirements Enforced by the Law and Related Ownership and Privacy Issues All organizations that are collecting processing and storing medical are obliged by the international and local law regulations and standards to improve their data protection strategies The example of data protection laws required by USA is the The Health Insurance Portability and Accountability Act HIPAA used for compliance and secure adoption of electronic health records EHR HIPAA sets the rules for sensitive patient data protection with its Privacy and Security Rules According to HIPA company that processes protected health information must store it in system that is secure as far as physical network and process security is concerned Security Rule of HIPAA deﬁnes requirements for health information that is held or transferred in electronic form It requires that entities implement the following protections Ensure the conﬁdentiality integrity and availability of all they create receive maintain or transmit Identify and protect against reasonably anticipated threats to the security or integrity of the information Protect against reasonably anticipated impermissible uses or disclosures and Ensure compliance by their workforce Additionally it regulates technical safeguards when hosting sensitive patient data including facility access in place as well as policies for using workstations electronic media and all processes of transferring removing disposing and using electronic media or electronic information It enforces authorized unique user IDs emergency access procedures automatic log oﬀ encryption of data and audit reports Also activity tracking logs of all activity on hardware and software are necessary organizations must ensure the security and availability of patients data for both professionals and patients The complete suite of HIPAA Administrative Simpliﬁcation Regulations can be found at CFR Part Part and Part and includes Transactions and Code Set Standards Identiﬁer Standards https Medical Data Processing and Analysis Privacy Rule Security Rule Enforcement Rule Breach Notiﬁcation Rule General Data Protection Regulation GDPR is regulation in EU law on data protection and privacy within the European Union EU and the European Economic Area EEA It also speciﬁes the conditions for exporting of personal data outside the EU and EEA areas It speciﬁes Rights of the data subject Controller role Transfers of personal data to third countries or international organizations Independent supervisory authorities Cooperation and consistency Remedies liability and penalties Delegated acts and implementing acts It speciﬁes actions to be taken that enforces the processing on systems that are processing medical data Several examples may be Data holders must notify the authorities in case of breach Subjects have right to access their data The right to be forgotten that gives individuals the power to request the removal of their personal data Privacy must be designed in Privacy is included by default not by unticking box Right to rectiﬁcation the data shall have the right to obtain from the troller without undue delay The data subject have the right to have plete personal data completed including by means of providing tary statement Every organization holding personal third party data must have data tection oﬃcer The GDPR imposes stiﬀ ﬁnes on data controllers and processors for compliance up to million or of the worldwide annual revenue of the prior ﬁnancial year in case of braking the basic principles for processing medical data To obtain the HIPAA required security level the company may follow two paths choose reputable HIPAA training company that oﬀers certiﬁcation tials and analyze the company procedures and systems by itself obtain evaluation by an independent third party auditor like for instance Coalﬁre or https http http Vitabile et For ensuring GRPD standards the company may obtain the customized GDPR certiﬁcate for protection hire authorized For both HIPAA and GDPR compliance there is one main central pendent party that may certify the company GDPR speciﬁc challenges rises inside remote systems First of all under the GDPR personal data may not be stored longer then needed This is why tion procedures have to be implemented and when the data expired they have to be removed from systems Data can be stored on multiple locations under tiple vendors processed by many services The deletion of data completely have to consider also backups and copies of data stored on the remote equipment Additionally breaching response may also be the issue Breach notiﬁcation have to be shared among data holders breach event have to be well deﬁned Processing of personal data outside the European Economic Area EEA is the next problem due to multiple location Controllers will need to deﬁne data strategy taking into account localization laws The transparency of security procedures of medical systems or third party certiﬁcates are necessary to ensure security controllers about the quality of rity services Medical service providers must be subject of an audit to perform control framework with privacy and privacy by design control measures Medical services should be monitored to address any changes in technology and recommended updates to the system It includes newly introduced ment and sensors During processing large set of data visibility regarding metadata and data anonymization is big challenge The level of protection of metadata the tive ownership rights rights to process the collections of metadata intended uses of metadata should be examined carefully Publicly Available Medical Datasets and Reuse of Already Collected Data Comprehensive and personal health and activities datasets have very important role in data processing and analysis therefore several attempts have been made to create big and representative examples of datasets Examples of such datasets are UbiqLog and CrowdSignals which contains both the data from the smartphones and from wearable devices smart Biomedical signals datasets can be found at which oﬀers free Web access to large collections of recorded physiologic signals PhysioBank and related software for collecting them PhysioToolkit https https Medical Data Processing and Analysis The ExtraSensory dataset contains data from users and more than labeled examples minutes originated from smartphone and smartwatch sensors The dataset is publicly available for context recognition research Within already implemented Hospital Information System HIS framework lot of data is collected for administrative purposes but not have been analysed from patients health perspective Organizational structure as ing staﬀ to patients transport paths rerouting shift distributions patients etc can be used for example in infectious disease ﬁeld There are models and tools describing the possible spread of pathogens and could indicate an eﬀective strategy in the ﬁght against infectious however available solutions are not using HPC as it would be possible Models Technologies and Solutions for Medical Data Processing and Analysis Usability of can rely on collecting techniques and data analysis Various data sets survey diagnosis and tests time series spatial panel longitudinal data etc shall be analysed by diﬀerent methods regressions decision trees structural equation modelling social network analysis modelling machine learning Innovative Use of Commercial Electronic Devices Smartphones and Watches Bracelets Wearables for Remote Monitoring and low cost electronic devices can be easily purchased and used Some properties as body temperature and blood pulse can be measured with high accuracy but developer are trying to target more and more features FDA Federal Drug Agency as well as EMA European Medicines Agency certify digital Health Software and Devices Most of devices on the market do not satisfy minimum conditions for certiﬁcation and accreditation Some of them as air quality monitoring devices laser dust sensor for less than have been massively used in heavily polluted cities However medical organization recommends using home devices due to very low data quality and data provided from certiﬁed oﬃcial stations should be enough More data is not always better as it is shown in this example Some issues come with digital health apps and software mhealth Nowadays an app can be found for almost everything Let s consider advances in image recognition using deep learning by smartphone for measuring blood pressure by photoplethysmography The biggest advantage of this method is simplicity so nothing more than smartphone is needed However even it works well after calibration for most users there are patients for whom the error of this dure can reach So apps oﬀering this feature like icare and HDCH were taken away from Google Play and App Store but their equivalents are still very popular for example in China Vitabile et Another type of sensors are bluetooth beacons to track social interactions that can be used for infectious disease control and psychological health one of certiﬁed suppliers for these beacons is Smartphone or wearable sensors can communicate between each other and collect contact data IoT Platforms for Applications The IoT platform is an open source alternative is tively new to the IoT ecosystem but is being used extensively in diﬀerent research projects or even for education It provides ready to use cloud service for connecting devices to the Internet to perform any remote sensing or actuation over the Internet It oﬀers free tier for connecting limited number of devices but it is also possible to install the software outside the cloud for private management of the data and devices connected to the platform without any limitation Fig platform This platform is hardware agnostic so it is possible to connect any device with Internet connectivity from Arduino devices Raspberry Pi Sigfox devices Lora solutions over gateways or ARM devices to mention few The platform Medical Data Processing and Analysis provides some out of the box features like device registry munication in both for sensing or actuation data and conﬁguration storage so it is possible to store time series data identity and access management IAM to allow third party entities to access the platform and device resources over APIs third party Webhooks so the devices can easily call other Web services send emails SMS push data to other clouds etc It also provides web interface to manage all the resources and generate dashboards for remote monitoring The general overview of this platform is available at Fig The main beneﬁt of using this platform aside that it is open source is the sibility to obtain communication with the devices in by using standard interfaces This way it is possible to develop any application desktop mobile Web service that interacts with devices by using and proven interface based on REST Meanwhile the devices can use more eﬃcient in terms of bandwidth or memory footprint binary protocols to communicate with the cloud In the other way this platform provides client libraries for connecting several state of the art IoT devices like LinkitOne Texas Instruments lium Waspmotes Raspberry Pi etc The client libraries provide comprehensive way of connecting devices and sending information to the cloud without having to deal with complex IoT protocols Libelium MySignals This is development platform for medical devices and eHealth applications The platform can be used to develop eHealth web tions and to test own sensors for medical applications MySignals is an example of commercial product which is oﬀered and supported by Spanish company called It allows measurement of more than biometric parameters such as pulse breath rate oxygen in blood electrocardiogram signals blood pressure muscle electromyography signals glucose levels galvanic skin response lung capacity snore waves patient position airﬂow and body scale parameters Data gathered by sensors can be stored in the MySignals or third party Cloud to be visualized in external Web and mobile app s Libelium oﬀers an API for developers to access the information as well as Open source HW version which is based on Arduino One of the drawbacks is the restriction for sending the information coming from MySignals to third party cloud server using directly WiFi radio which is limited only for HW version option is not available for SW version running on libelium Atmega node FIWARE is an open source IoT platform combining nents that enable the connection to IoT with Context Information Management and Big Data services in the cloud FIWARE uses rather simple standard APIs for data management and exchange that can be used to develop smart applications It provides enhanced cloud hosting capabilities https Vitabile et and number of components for functions oﬀered as Service The adoption of FIWARE technology by the eHealth market is promoted by the industrial accelerator FICHe project launched by European Commission and several industrial projects that have been developed in the FICHe context CardioWeel is an advanced driver assistance system that provides information about drivers health status using the human electrocardiogram ECG acquired from the drivers hands This system is hosted on FIWARE cloud using the available virtual machines The development of healthcare applications using FIWARE components is described in and in Big Medical Data Analytics for Remote Health Monitoring With the advance of remote health monitoring and pervasive healthcare cepts an increased research work been published to cover the topics ranging from theory concepts and systems to applications of Big medical data systems for ubiquitous healthcare services Big Medical Data System Architecture The analysis and health problem detection can be performed on mobile device phone watch bracelet etc leveraging edge computing principles or at nearby computing infrastructure IoT gateway or server Physiological health and social media data providing insight into people activities and health conditions could be enriched and correlated with nal and environmental data collected within Smart City infrastructure weather conditions environment temperature city events traﬃc ditions As such fusion analysis and mining of IoT medical data and data could better detect potential health problems and their relationships with the environment The fusion processing and analytics of sensor data collected from personal mobile and health devices as well as Smart city infrastructure are performed at the computing components providing eﬃciency and minimal latency in detection of critical medical conditions that requires prompt actions Also this can provide personalized health system for general where individuals can be provided with healthcare tailored to their needs Moreover such system should support eﬃcient collection and analysis of massive quantities of heterogeneous and continuous health and activities data Big Data from group or crowd of users The storage aggregation ing and analysis of Big health data could be performed within public private or hybrid cloud infrastructure The results of data analysis and mining are provided to physicians healthcare professionals medical organisations pharmaceutical companies etc through tailored visual analytics dashboard applications There is number of research and development challenges that must be addressed Medical Data Processing and Analysis Fig system for remote monitoring of people health and activities general tecture prior to wider application of such personalized healthcare systems that ously monitor peoples health and activities and respond appropriately on critical events and conditions These include but are not limited to security privacy and data ownership sensor data fusion scalable algorithms and systems for analytics and data mining and models for processing and analytics The general architecture of distributed system based on medical IoT devices and Big Data processing and analytics for remote monitoring of peoples health and activities is given in Fig Remote monitoring systems can operate at multiple scales providing sonal and global sensing As illustrated in there are three distinct scales for sensing personal group and community sensing Personal or individual monitoring systems are designed for single ual and are often focused on personal data collection and analysis Typical scenarios include tracking the users exercise routines measuring activity levels or identiﬁcation of symptoms connected with psychological disorders Although collected data might be for sole consumption of the user sharing with medical Vitabile et professional is common Popular personal monitoring technologies are Google GlassTM FitBitTM and The FuelBandTM If individuals share common interest concern or goal while participating in monitoring applications they form group Group monitoring systems can be popular in social networks or connected groups where data can be shared freely or with privacy protection Common examples are health monitoring tions including contests related to speciﬁc goals running distance weight loss calorie intake etc If number of people participating in health and activities monitoring is very large it is called community or crowd monitoring Crowd modeling and toring implies collective data analytics for the good of the community However it involves cooperation of persons who will not trust each other highlighting the need for strong privacy protection and possibly low commitment levels from users Examples of community monitoring involve tracking the spread of diseases across area ﬁnding patterns for speciﬁc medical conditions etc The impact of scaling to monitoring applications is to be explored but many research issues related to information sharing data ownership data fusion security and privacy algorithms used for data mining providing useful feedback etc remain open Khan et in presented the latest research and development eﬀorts and achievements in the ﬁeld of smart healthcare regarding high performance puting HPC and healthcare architectures data quality and scale machine learning models for smart healthcare fog puting and as well as wearable computing Quality of Service QoS and for smart healthcare The challenges in designing algorithms methods and systems for care analytics and applications have been examined in along with survey on smart healthcare technologies and solutions The healthcare applications services and systems related to Big healthcare data analytics are reviewed and challenges in developing such systems are discussed The current state and projected future directions for integration of remote health monitoring technologies into clinical and medicine practice have been presented in Several of the challenges in sensing analytics and visualization that need to be addressed before systems can be designed for seamless integration into clinical practice are highlighted and described Big IoT Data for Remote Health Monitoring With the development of Internet of Things IoT concepts the idea of IoT healthcare systems been an interesting topic for large number of researchers Baker et in have presented research related to wearable IoT healthcare system standard model for application in future IoT care system was proposed along with an overview of existing challenges ing security privacy wearability and operation Also the authors reviewed several research works that address IoT healthcare challenges related to rehabilitation diabetes management disease monitoring tion and general activity and health monitoring purposes Furthermore Medical Data Processing and Analysis part model was proposed as an aid in the development of future IoT healthcare systems and it includes wearable sensor and central nodes short range communications long range communications secure cloud storage architecture and machine learning Islam et in have given an overview of existing healthcare network studies and network architectures platforms tions and industrial trends in this area Furthermore they highlighted security and privacy issues and proposed security model aiming to minimize security risk related to health care Speciﬁcally they presented an extensive overview of IoT healthcare systems one of them being the Internet of Things an operating symbiosis of mobile computing medical sensors and munications technologies for healthcare services An important part of IoT healthcare is obtaining insights from large data generated by IoT devices The focus of was on IoT architecture nities and challenges but from the data analysis point of view In this paper brief overview of research eﬀorts directed toward IoT data analytics as well as the relationship between Big Data analytics and IoT were given Furthermore analytic types methods and technologies for big IoT data mining are discussed In authors have used smartphone and smartwatch sensors to recognize detailed situations of people in their natural behavior Initial tests were ducted over labeled data from over minutes from subjects dedicated application for data retrieval was implemented and presented As stated in the paper the main contribution is the emphasis on conditions namely naturally used devices unconstrained device placement natural environment and natural behavioral content The main challenge when recognizing context in conditions is high diversity and variance of the data However it was shown that everyday devices in their natural usage can capture information about wide range of behavioral attributes In distributed framework based on the IoT paradigm been posed for monitoring human biomedical signals in activities involving physical exertion Also validation use case study was presented which includes itoring footballers heart rates during football match and it was shown that data from BAN devices can be used to predict not only situations of sudden death but also possible injuries Ma et in have presented Big health application system based on health IoT devices and Big Data The authors presented the architecture of health care application leveraging IoT and Big Data and introduced technology challenges and possible applications based on this architecture One important subset of the data used for healthcare and improved quality of life are the data retrieved from healthcare services running on smarthphones such as Endomondo RunKeeper and Runtastic These applications can provide data about diﬀerent lifestyles such as sleep diet and exercise habits all of Vitabile et which can be correlated with various medical conditions In such manner cians can calculate risk for conditions or in cases of diagnosed ones can adapt treatments or provide emergency responses when needed Cortes et in have provided an extensive analysis of traces gathered from Endomondo sport tracker service The main Big Data challenges associated with analysis of the data retrieved from this type of applications were discussed They include data acquisition data redundancy workload ﬂow rate data cleaning invalid and uncertain data from sensors data integration aggregation and representation deﬁne common data representation across various applications properties of the data query processing data modeling and analysis queries are important aspect of knowledge discovery and interpretation online and oﬄine analysis The concept of smart health which integrates mobile health principles with sensors and information originated in smart cities been introduced in The authors provided detailed overview of the smart health principles and discussed the main research challenges and tunities in augmentation of smart healthcare with smart city concepts Processing and Analysis of Big Health and Mobility Data Streams The healthcare industry generates large amounts of data driven by record ing compliance and regulatory requirements and patient care The current trend is toward rapid digitalization of these massive amounts of data and their fusion with data retrieved from personal and mobility sensors When it comes to health care in terms of patient records treatment plans prescription information everything needs to be done quickly accurately and in some cases ently enough to satisfy stringent industry regulations However with extended datasets eﬀective data processing and analysis can uncover hidden insights that improve patient care If the data is collected out of the medical oﬃces they can model patient s behavior more accurately Furthermore the results can be used to provide better healthcare at lower cost component requires that systems for analysis and processing must deal not only with huge amounts of data but also with Big Data streams This requirement led to proliferation of studies related to processing and analysis of big heterogeneous streams for healthcare purposes These streams have all main characteristics of Big Data Volume in order to create complete overview of person s or group health it is necessary to take into account data collected from related sources obtained not only from medical instruments and BAN sensors but also from social media mobile devices or data This data can be from terabytes to exabytes does not need to be stored but must be eﬀectively and analyzed and processed in timely manner Velocity Data streams with unparalleled rate and speed are generated and must be processed and analysed accordingly In order to be eﬀective and to Medical Data Processing and Analysis provide responses in emergency situations healthcare systems must process and analyze torrents of data in real time but also be capable to perform batch operations Variety Data comes in all varieties from structured numeric data obtained from sensors and medical devices to unstructured text documents email video audio and mobility data Systems must be capable of processing all varieties of data from text to graph data In order to eﬀectively process and analyze Big health data streams niques for Big Data must be taken into account and adapted for data streams Big Data in health informatics is pool of technologies tools and techniques that manage manipulate and organize enormous varied and intricate datasets in order to improve the quality of patients status Fang et in have given comprehensive overview of challenges and techniques for big computational health informatics both historical and art Several techniques and algorithms in machine learning were characterized and compared Identiﬁed lenges were summarized into four categories depending on Big Data tics they tackle volume velocity variety and veracity Furthermore eral pipeline for healthcare data processing was proposed This pipeline includes data capturing storing sharing analyzing searching and decision support Wang et in have presented strategic implications of Big Data in care including but not limited to historical development architectural design and component functionalities of Big Data analytics Diﬀerent capabilities of healthcare data analytics were recognized from diﬀerent implementation cases and various beneﬁts were identiﬁed Recognized capabilities include Analytical capability for patterns of care analytical techniques typically used in Big Data analytics system to process data with an immense ume variety and velocity via unique data storage management analysis and visualization technologies Unstructured data analytical capability unstructured and data in healthcare refer to information that can neither be stored in tional relational database nor ﬁt into predeﬁned data models Decision support capability the ability to produce reports about daily healthcare services to aid managers decisions and actions Predictive capability ability to build and assess model aimed at ing accurate predictions of new observations where new can be interpreted temporally and or Traceability ability to track output data from all the system s IT components throughout the organization s service units As result the best practice for Big Data analytics architecture been posed This architecture loosely consists of ﬁve main architectural layers data data aggregation analytics information exploration and data governance Ma et in have underlined the importance of data fusion based approach to provide more valuable personal and group services such as personalized health guidance and public health warnings Various technologies and areas that are Vitabile et related to this topic were covered including mobile wearable and cloud puting Big Data IoT and Cyber Physical systems Authors proposed and layered architecture and described each layer along with tools and technologies that can be used Also diﬀerent applications were identiﬁed related to medical recommendations and wearable healthcare systems High Performance Data Analytics Eﬃcient analytics of large amounts of data in health care such as patient genomic data or images demands computational communication and memory resources of High Performance Computing HPC architectures HPC involves the use of many interconnected processing elements to reduce the time to solution of given problem Many powerful HPC systems are erogeneous in the sense that they combine CPUs with tors such as Graphics Processing Units GPUs or Field Programmable Gates Arrays FPGAs For instance ORNL s is currently credited as the leading HPC system in the list of most powerful computer systems equipped with computing nodes and each node comprises two IBM CPUs and six NVIDIA GPUs for theoretical peak power of approximately petaﬂops Many approaches have proposed for using HPC systems While CPUs are suitable for tasks processors such as the Intel Xeon Phi or GPU comprise larger number of lower frequency cores and perform well on scalable applications such as DNA sequence analysis or deep learning Medical Data Analytics Research Challenges and Opportunities Adaptive Processing and Analytics In the context of medical IoT the particular beneﬁts could arise from the fog and edge computing paradigms They represent the model where sensitive data generated by body worn medical devices and smart phone sensors are processed analyzed and mined close to where it is generated on these devices themselves instead of sending vast amounts of sensor data to the cloud that could exhaust network processing or storage resources and violate user privacy Only gated context enriched are sent to the cloud for further ing and analytics The right balance between should provide fast response to detected events as well as conservation of network bandwidth imization of latency security and preservation of privacy Typically Internet of Things IoT is composed of small resource constrained devices that are connected to the Internet In essence they are dedicated to perform speciﬁc tasks without the need of providing general means for forming complicated computations This means that most Medical Data Processing and Analysis of the data is transmitted to the cloud without prepossessing or analysis This implies that the amount of data that is transmitted to the cloud is increasing even more rapidly than the number of IoT devices itself Indeed cloud ing is being recognized as success factor for IoT providing ubiquity reliability and scalability However IoT solutions based on cloud puting fail in applications that require very low and predictable latency or are implemented in geographically distributed manner with lot of wireless links promising technology to tackle the and geographical distribution required by IoT devices is fog computing The fog computing layer is an intermediate layer between the edge of the network and the cloud layer The fog computing layer extends the computation paradigm geographically providing local computing and storage for local services Fog computing does not outsource cloud computing It aims to provide puting and storage platform physically closer to the end nodes provisioning new breed of applications and services with an eﬃcient interplay with the cloud layer The expected beneﬁt is better quality of service for applications that require low latency Lower latency is obtained by performing data analysis already at the fog computing layer Data analysis at the fog computing layer is lightweight and therefore more advanced analyses and processing should be done at the cloud layer Naturally some applications do not require computation or they need high processing power and therefore they are performed at the cloud layer For example in the case of smart community where homes in neighborhood are connected to provide community services low latency is expected for making urgent decisions and thus computation is performed within the neighborhood instead of cloud layer which can be located on another continent Virtualisation Technologies in To support of diﬀerent applications and to achieve elasticity in shared resources fog computing takes advantages of virtualization technologies Virtualization and application partitioning techniques are the two key technology solutions that are employed in fog computing platform tualization includes the process of abstracting and slicing the heterogeneous computing nodes into virtual machines VMs hiding the details about the erogeneity of hardware devices from the applications that are running on the fog layer Among diﬀerent virtualization technologies Linux containers tainerization advantages in short implementation time eﬃcient resource utilization and low management cost Application partitioning divides each task into smaller to be executed concurrently on distributed grids which can improve the task oﬄoading eﬃciency of applications when the node is loaded by computationally intensive applications Containers provide level virtualization without need for deployment of virtual Hence they are lightweight and signiﬁcantly smaller in size than VMs Containers provide and isolated computing environment for applications and facilitate lightweight portability and interoperability for IoT Vitabile et applications Moreover data and resource isolation in containers oﬀers improved security for the applications running in fog nodes Recently researchers have shown increased interest in studying and deploying healthcare applications Kaˇceniauskas et have developed software to simulate blood ﬂows through aortic valves as cloud service They have experimentally evaluated the performance of XEN VMs KVM VMs and Docker containers and have found that Docker containers outperform KVM and XEN VMs In framework to monitor patients symptoms been proposed This solution employs Raspberry Pi that read the medical data through the sensors attached and sent it to server The server is running Docker containers healthcare framework been proposed by Li et Diﬀerent sensors send streams of medical data to docker containers for ing Koliogeorgi et presented the cloud infrastructure of the AEGLE project which integrates cloud technologies and heterogeneous reconﬁgurable computing in large scale healthcare system for Big analytics AEGLE runs each service and library in diﬀerent Docker containers It also uses netes for fault tolerant management and scaling of the containerized tions conceptual model and minimal viable product implementation that enables the analyses of genomic data harvested by portable genome sequencer using mobile devices been proposed by et They have used Docker to process the data and to ensure that their system will work across ferent devices and operating systems Moreover they would like to prepare the Docker image available for mobile phones In this way it will be possible to run their system containers on mobile phones Rassias et introduced platform for teleconsultation services on medical imaging Their platform consists of client application and an ecosystem of microservices Each system s service is running in Docker container Bahrami et proposed method to better protect sensitive data Their method provides resource recommendation for container uler according to HIPAA s Health Insurance Portability and Accountability Act regulations Finally recent paper by Sahoo et presents fault tolerant scheduling framework for distribution of healthcare jobs based on their types They have evaluated the framework with both ization and virtualization Containers achieved faster time and less response time for both and task Case Studies and Practical Solutions Big Data Solution for Remote Health Monitoring Taking into account all the challenges related to monitoring data and control ﬂows that should be employed in public private or hybrid cloud we have designed Remote Health Monitoring system architecture and have implemented the system tailored to analysis of mobility data for the purposes Medical Data Processing and Analysis Fig The general architecture of remote health monitoring cloud infrastructure of healthcare The general architecture of Remote Health Monitoring system is given in Fig To provide collection of massive and fast sensor data streams distributed message broker is employed that maintain set of topics for diﬀerent users or medical conditions where diﬀerent sensor data and events will be streamed These topics would be monitored by number of processing and analytics jobs within Big Data stream engine Each job acquires only the data from the topics of interest so diﬀerent jobs are used to identify speciﬁc medical condition and patient behaviour This architecture could be applied to both personalized and group healthcare systems The main advantages are Data decoupling every measurement is streamed as separate record to predeﬁned topic so occurrence of missing data in records is easily detectable they are absent from the topic if data from certain sensors is missing it can be easily left out The separation at jobs level jobs can monitor only subset of topics provides perfect ground for implementation of diﬀerent privacy protocols for each job The user level separation this model allows both personal and collective analysis as jobs can monitor data on individual or group level Comprehensive visualization the results from each jobs can be visualized separately which can simplify the visualization itself however alerts and notiﬁcations from diﬀerent jobs can be combined and visualized to provide an insight to general state Big health data stream processing and analysis currently relies on systems for massive stream analytics comparative analysis of the existing art stream processing solutions is presented in including both open source tions such as Storm Spark Streaming and and commercial streaming tions such as Amazon Kinesis and IBM Infosphere Stream Comparison includes processing model and latency data pipeline fault tolerance and data guarantees These and similar solutions can be used in healthcare use cases Vitabile et For the implementation of prototype system Apache and Apache Spark have been used Kafka is chosen as it can support ent topics and messaging capabilities necessary for handling Big Data streams Apache Spark Streaming is selected because of its main paradigm which implies that stream data is treated as series of bounded batch data technique called micro batching and for its machine learning library MLlib We do not use mobile health application for data collection but we created simulation of tiple users based on personal data used for ing such data to the system Each users simulation component reads ﬁle that contains diﬀerent records in csv format and publishes them to adequate Kafka topic Actual data have been obtained from UbiqLog and CrowdSignals datasets These datasets contain information retrieved from smartphones and smart bracelets such as heart rate step count accelerometer and gyroscope data location application usage and WiFi distinct record is generated for every measurement or event and it contains user id topic and value Id is used to separate diﬀerent users topic to identify type of measurement or event and value to hold actual data For each type of record there is predeﬁned Kafka topic These topic streams are monitored by number of Apache Spark ing jobs Each job acquires the data from monitored topics and performs speciﬁc health and activities related analysis We implemented demo job for tion of skin temperature based on heart rate and step count values Firstly in oﬄine mode we created regression model for these parameters Then in Spark Streaming job we loaded that model and generated alert in the form of console notiﬁcation if predicted temperature is higher than the predeﬁned threshold This illustrates that the system can be used for complex event processing Our vision is to create jobs that can detect complex events extrapolate valuable information and run mining algorithms on available data thermore visualization and alert generating component of the system will be implemented Smartphone Ad Hoc Network SPAN for Application Practical Scenario In this section we propose system designed to support distributed monitoring of trip participants health Let s assume that there is travel agency which organizes the climbing in high mountains manjaro As the agency makes reasonable policy they equip the guide and the participants with sensors that measure health parameters heart rate pressure body temperature etc in order to monitor the people s health The is equipped with satellite mobile phone and can exchange information with the agency and rescue teams The communication between the guide and the participants is organized in ad hoc manner using Bluetooth Each participant is equipped with smartphone and the health sensor The sensor is connected to the smartphone by Bluetooth Low Energy Medical Data Processing and Analysis Technology One of the trends observed recently in the global tions is constant growth of the number of smart devices Mobile devices are getting richer in functionality thanks to more and more computing resources better batteries and more advanced embedded equipment sensors cameras localization modules An important parts of each mobile device are the wireless communication interfaces WiFi Bluetooth which are used to exchange data between the mobile device and peripheral devices headphones free system etc or other mobile devices Thanks to the wireless communication it is also possible to integrate smartphones with external sensors Thus Sikora et decided to create the SmartGroup Net SGN mobile application which allows on direct communication between people without the necessity to be in range of cellular network infrastructure The communication is realized over Bluetooth Low Energy BLE to create mobile network allowing local exchange of messages which contain selected information about the state of the network nodes This is an example of Smart Phone Ad hoc Network SPAN In the considered test study the Polar Bluetooth Smart heart rate sensor and Wiko Sunny phones are used see Fig Fig smartphone with SGN application running together with Polar heart rate sensor Mobile Application SmartGroup Net SGN is the mobile application which allows on supporting outdoor search actions performed by group of people The application is useful for diﬀerent public services that could be supported during rescue action by civilian volunteers equipped with mobile phones Other potential applications of SGN include increasing safety of people moving in SPAN network can be formed by portable devices smartphones tablets etc which communicate over WiFi or Bluetooth and operate in mode The main advantage of the SPAN network is independence not only from the network operator but also from the tariﬀ plan the communication in an mode is free of charges Vitabile et Fig An example of the SGN application user screen with buttons and ﬁelds tion groups in areas with limited access to the mobile network like high mountains climbing coordination and management of rescue actions in crisis situation area earthquake plane crash etc and other coordinated actions involving sensors carried by people or devices In all mentioned scenarios it is important to maintain proper group formation and topology An exemplary screen of the SGN application is shown in Fig Information about the current location and the destination of the mobile phone user as well as the locations of other participants of the action are presented on the map downloaded online from the Open Street Map and stored in the cache or installed in the oﬄine mode as image ﬁle The presented application uses the BLE protocol for local exchange of mation between the participants of the action The information sent by each participant include his identiﬁcation data action number user s name and id location geographical coordinates status normal or emergency heart rate These data are saved in the broadcast packet encrypted and sent to other users depending on the radio transmission range Each of the participants of the action can simultaneously broadcast messages with their own data and listen to sages from the other group members Thus the leader guide can obtain the information about health status of the participants Depending on the situation appropriate preventive measures can be taken http Medical Data Processing and Analysis Machine Learning for Monitoring Daily Living Activities of People with Dementia Dementia is very complex condition aﬀecting the mental health of people and having negative impact upon their daily life independence and abilities The current statistics show that dementia rapid spread worldwide as each year around million new cases are reported Moreover it is estimated that since dementia become the cause of deaths worldwide Even if there is cure for dementia the lives of people with dementia could be improved by detecting and treating challenging behavioural and psychological symptoms In this context our vision is to leverage on novel Big Data ICT technologies and Machine Learning ML algorithms to analyze the daily living activities of the people with dementia aiming to determine anomalies in the pattern of their daily living activities Such patterns might be analyzed by medical experts or sional caregivers to take actions to improve patients overall health For example the anomalies found in the daily living activities of people with dementia could help medical experts in diﬀerentiating between health decline symptoms duced by dementia and health decline symptoms generated by the side eﬀects of the prescribed medications and their interactions polypharmacy eﬀects This would allow medical experts to better evaluate the health status of patient and better adjust its medication plan This section proposes system designed based on the Lambda architecture for the analysis of daily living activities of people with dementia using machine learning algorithms The system collects data about the daily living activities patterns of the monitored persons using set of Raspberry Pi s that are installed in the house of the monitored person The collected data is sent to Big Data platform where machine learning algorithms are applied to identify anomalies in the behavior of people with dementia The proposed system been implemented in the context of the MEDGUIDE project which is an innovative Ambient Assisted Living AAL project that aims to provide solutions for supporting the of people with dementia and their caregivers Challenges of ML for Heterogeneous Data Streams ML for data streams is subclass of the machine learning domain in which the analyzed data is generated in In several challenges that might appear when ML is applied on heterogeneous distributed data streams are identiﬁed the input data is generated by many sources b the data formats are both structured and unstructured c the streaming data high speed d the data is not completely available for processing in some cases the data might be noisy characterized by missing values and of poor quality Moreover several ML algorithms must be adapted to larger datasets because they were designed for smaller sets of data and the samples usually have many features because the data is monitored by big number of sensors According to several critical issues that must be considered when analyzing real time data streams are data incompleteness data streams heterogeneity high speed of data streams and data large scale Vitabile et In two major approaches for data streams processing are presented In the ﬁrst approach the persists only small part of the elements which characterize the entire data stream while in the second approach ﬁxed length windows are used In the second approach selection of ﬁxed parts of data are loaded temporarily into memory buﬀer and the ML algorithms take as input the data from that memory buﬀer The authors of identify another major challenge that might exist when the ML algorithms are applied on data streams in many cases the speed of the lying ML algorithm is much slower than the speed of the coming data possible solution for this challenge is to skip some of the instances from the data stream However in that case the outcome might be negative if the instances that are skipped contain insightful information In two steps that are typically applied for predictive analytics and for prescriptive analytics are presented The ﬁrst step is the analytic model building in which the analytic model is generated by trying out diﬀerent approaches iteratively The second step is dedicated to the analytic model validation when the model is tested on diﬀerent datasets and improved either by changing the conﬁgurable parameters or the complete algorithm The Experimental Platform Our system design is based on Lambda tecture and that can be used for the analysis of the daily living activities of the people that have dementia using diﬀerent ML algorithms Fig System for analyzing daily living activities of people with dementia using ML algorithms Figure presents the architecture of system that is used for the analysis of the daily living activities of people with dementia The system applies ML algorithms to detect anomalies in the daily living activities patterns of the monitored patients that might require special attention from specialized healthcare professional Medical Data Processing and Analysis The system collects data about the daily living activities patterns of the monitored persons using set of Raspberry Pi s that are installed in the house of the monitored person The Raspberry Pi s collect information about diﬀerent types of activities such as sleeping feeding hygiene indoor activities and door activities The number of times that the elder goes to the toilet and the wandering behavior during night can be used as indicatives of diﬀerent stages of dementia The caregiver being notiﬁed that the monitored patient presents anomalous behavior with respect to the regular pattern of activities may take preventive actions as to minimize the eﬀects of dementia The data collected by the Raspberry Pi s is sent in the form of AVRO sages to Big Data platform created using the following technologies Apache Zookeeper Apache Kafka Apache Spark and Apache Cassandra Zookeeper is server that manages the coordination of tasks for the nodes of the distributed system that is used for the processing of the streaming data Kafka is streaming platform that handles the data The data comes in the form of AVRO messages and is inserted in the system through the Kafka REST server The Kafka REST server uses an AVRO schema that is persisted in the Schema Registry server to get the information from the messages The AVRO schema imposes set of restrictions on the messages such as the number of ﬁelds and the data types of the ﬁelds The data is then processed in an implementation of Lambda architecture Kafka Connect is server that takes the messages from the Kafka topics and inserts them in Cassandra In the case of batch processing the entire data stored in the database is used as training data for the classiﬁcation algorithms that are used for the prediction of the daily living activities of the people with dementia while in the case of stream processing only the last streaming information is considered The function in the case of Lambda architecture is represented by the ML algorithm that is used for the prediction of the daily living activities In the case of stream processing it is unfeasible to retrain the ML algorithm each time new streaming data comes into the system because this process would take too much time compared to the time in which the prediction should be made Considering this time restriction depending on the size of the streaming data the ML algorithms should be retrained at frequency of several minutes or several hours The predicted data can then be sent to the caregivers or to the healthcare professionals using healthcare application created with Spring and Angular Spring is used for and for the communication with Apache Cassandra and also for retrieving the data generated by the monitoring sensors and the results obtained after the application of the ML algorithms Spring is also used for sending notiﬁcations to the caregivers if corrective measures must be taken Angular is JavaScript framework that is used for the creation of the end side By using the healthcare system the caregivers should be able to see data analytics to detect anomalies in the daily living activities of the monitored persons and should also be able to take immediate actions when they are notiﬁed Vitabile et Detecting the of Polypharmacy upon the Daily Life Activities of People with Dementia Using Machine Learning Algorithms This section presents how we have used the previously described experimental platform in the context of detecting the deviations in the daily life activities and their causes of people with dementia using ML algorithms We have chosen to focus on polypharmacy as it represents one of the main challenges in the treatment of dementia aﬀecting over of the elders with dementia In particular the Random Forest Classiﬁcation algorithm been used for detecting the deviations of the daily life activities from the patients baseline routine while the clustering algorithm been used for correlating these deviations with the of interactions The two ML algorithms have been implemented using two Spark jobs Concepts Deﬁnitions In our approach day containing monitored activities is deﬁned as day duration duration duration an where duration ai represents the total duration in hours of the activity ai while n is the number of total activities considered In our approach we have considered the following ﬁve types of daily life activities as signiﬁcant enough for allowing the detection of polypharmacy side eﬀects sleeping feeding toilet hygiene functional mobility and community mobility Table presents an example with information provided by sensors regarding the activities performed by patient with dementia during January Table Information provided from sensors regarding the activities performed by patient with dementia during day Start time End time Activity Sleeping Toilet hygiene Feeding Functional mobility Sleeping Feeding Community mobility Functional mobility Feeding Toilet hygiene Functional mobility Sleeping Medical Data Processing and Analysis The representation of the day January according to is day The baseline for patient with dementia is day when the patient typical behavior and it is represented similar to the representation of day see In our approach the baseline for each monitored patient is deﬁned by the doctor based on discussions with the patient its family and caregivers We consider that monitored day is deviated day if it contains at least one activity which the total duration higher or lower than threshold compared to the same activity type in the baseline deviated day may be associated with semantic annotation consisting of interaction and an associated The information regarding dementia drugs interactions and associated are stored as instances in Polypharmacy Management Ontology we have designed This ontology is simpliﬁed version of the Interactions Ontology DINTO enhanced with drugs side eﬀects taken from Classifying Day as Normal or as Having Signiﬁcant Deviations from the line Using the Random Forest Classiﬁcation Algorithm To classify day as normal or as having signiﬁcant deviations from the baseline we have used the Random Forest algorithm suitable for Big Data classiﬁcation provided by Apache Spark We have trained the Random Forest classiﬁcation algorithm using data set consisting of days labeled as having or not having signiﬁcant deviation from the normal baseline In the testing phase set of new monitored days is classiﬁed based on the data set used in the training phase as having deviation from the baseline Identifying the Causes that Produced the Deviation of Day from the Baseline To identify the causes which led to deviation in the daily life activities of patient with dementia we have deﬁned clustering method consisting of two steps Cluster the deviated days The Spark MLlib implementation of the clustering algorithm been applied on set of deviated days each day being annotated with interaction and causing the deviation Each cluster will contain similar annotated days and will be labeled with the annotation interaction and its adverse eﬀect of the cluster s centroid Assign new deviated day to cluster resulted in the previous step new deviated day will be assigned to the cluster for which the Euclidean distance between that day and the cluster s centroid is minimum Consequently the deviated day will be annotated with the interaction and its adverse eﬀect corresponding to the assigned cluster s centroid Figure presents the resources and the data ﬂows for detecting the eﬀects of polypharmacy upon the daily life activities of people with dementia using the Random Forrest Classiﬁer and the Clustering Method Vitabile et Fig The resources and the data ﬂows for detecting the of polypharmacy upon the daily life activities of people with dementia Summary Health informatics is already an established scientiﬁc ﬁeld and advances of and are already part of clinical practices of the XXI tury medicine Digital medicine enables optimization of cesses and medicine through the possibility of analyzing huge amount of data at low cost The computer assisted tools may be more eﬃcient and safer than analog approaches that involve physician In this chapter we have described data collection fusion ownership and privacy issues models technologies and solutions for medical data processing and analysis big medical data analytics for remote health monitoring research challenges and opportunities in medical data analytics three case studies The ﬁrst case study described Big Data solution for remote health monitoring that considers challenges pertaining to monitoring the health and activities of patients The second case study described system designed to support distributed monitoring of human health during trip The third case study described the use of machine learning in the context of monitoring daily living activities of people with dementia References https Apache Cassandra http Apache Kafka distributed streaming platform https Medical Data Processing and Analysis Apache Spark cluster computing http Apache Zookeeper https CardioWheel https Dementia https FIWARE https FIWARE Accelerator Programme FICHe project https ICT integrated system for coordinated polypharmacy management in elders with dementia https Machine learning library MLlib guide https The top causes of death https Akbar et probabilistic data fusion for IoT cations IEEE Access https Kim Crisan EpiMobile pathogen point of care diagnosis and global surveillance using mobile devices Bahrami Malvankar Budhraja Kundu Singhal Kundu provisioning of containers on cloud In IEEE national Conference on Cloud Computing CLOUD pp IEEE Baker Xiang Atkinson Internet of Things for smart healthcare technologies challenges and opportunities IEEE Access https Bellavista Zanni Feasibility of fog computing deployment based on Docker containerization over RaspberryPi In Proceedings of the International ference on Distributed Computing and Networking ICDCN pp ACM New York Benkner et PEPPHER eﬃcient and productive usage of hybrid computing systems IEEE Micro https Celesti et How to develop IoT cloud systems based on FIWARE lesson learnt Sens Actuator Netw Chrysos Intel R Xeon Phi Architecture Intel Whitepaper Bonnaire Marin Sens Stream processing of healthcare sensor data studying user traces to identify challenges from big data perspective Procedia Comput Sci Dubey et Fog computing in medical architecture mentation and applications In Khan Zomaya Abbas eds book of Distributed Computing in Smart Healthcare SCC pp Springer Cham https Esteban Patricio Molina feature selection roach to the group behavior recognition issue using static context information IJDSN https Fang Pouyanfar Yang Chen Iyengar Computational health informatics in the big data age survey ACM Comput Surv https http Fisher Fletcher Anapalli Pringle III Development of an platform Adv Internet Things Vitabile et Garcia Vivacqua Pi Molina ambient assisted living to monitor the elderly s health outdoors IEEE Softw https Gil Molina InContexto multisensor architecture to obtain people context from smartphones IJDSN https Serrano Molina Rogova information fusion for harbor surveillance Inf Fusion https Serrano Patricio Molina scene recognition from visual data in smarthomes an information fusion approach Pers Ubiquit Comput https Hapfelmeier Mertes Schmidt Kramer Towards machine learning In Proceedings of the ECML PKDD Workshop on Instant tive Data Mining Hassanalieragh et Health monitoring and management using Things IoT sensing with processing opportunities and challenges In IEEE International Conference on Services Computing pp June https Herrero Patricio Molina Cardoso Contextual and human factors in information fusion In Human Systems Integration to Enhance Maritime Domain Awareness for Security pp https Hosseinpour Meng Westerlund Plosila Liu Tenhunen review on fog computing systems Int Adv Comput Technol Islam Kwak Kabir Hossain Kwak The Internet of Things for health care comprehensive survey IEEE Access https Jaiswal Sobhanayak Mohanta Jena based work for patient s data collection in smart healthcare system using In International Conference on Electrical and Computing Technologies and Applications ICECTA pp November Jarynowski Marchewka Buda risk assessment of infectious diseases in women sexual and reproductive health pp Kaˇceniauskas Paceviˇc Starikoviˇcius Maknickas Davidaviˇcius Development of cloud services for simulations of blood ﬂows through aortic valves Adv Eng Softw Kamar Chatterjee Hamie Internet of Things in learning perspective of platforms Int Adv Res Comput Sci Kessler et Programmability and performance portability aspects of erogeneous systems In Design Automation and Test in Europe Conference and Exhibition DATE pp IEEE Khaleghi Khamis Karray Razavi Multisensor data fusion review of the Inf Fusion https Khan Zomaya Abbas eds Handbook of Distributed Computing in Smart Healthcare SCC Springer Cham https Medical Data Processing and Analysis Koliogeorgi et AEGLE s cloud infrastructure for resource monitoring and containerized accelerated analytics In IEEE Computer Society Annual posium on VLSI ISVLSI pp IEEE Ode Hasnuddin Abidin Internet of Things for early detection of lanslides In Prosiding Seminar Nasional Riset Kuantitatif Terapan vol Lane Miluzzo Lu Peebles Choudhury Campbell survey of mobile phone sensing IEEE Commun Mag https Lee et Big healthcare data analytics challenges and applications In Khan Zomaya Abbas eds Handbook of Distributed puting in Smart Healthcare SCC pp Springer Cham https L Heureux Grolinger Machine learning with big data challenges and approaches https Elyamany Capretz IEEE Access https Li Xu Luo Cao Mathew Ma CareNet building secure infrastructure for healthcare In Proceedings of the ACM International Workshop on Security in Software Deﬁned Networks Network Function Virtualization pp ACM New York Likotiko Petrov Mwangoka Hilleringmann Real time solid waste monitoring using cloud and sensors technologies Online Sci Technol Llinas Information fusion process design issues for hard and soft information developing an initial prototype In Yager Reformat Alajlan N eds Intelligent Methods for Cyber Warfare SCI vol pp Springer Cham https Herrero Corredera Cooperative management of net of intelligent surveillance agent sensor Int Intell Syst https Ma Wang Zhou Wen Zhang Intelligent healthcare systems assisted by data analytics and mobile computing Wirel Commun Mob Comput Article ID Ma Wang Yang Miao Li Big health application system based on health Internet of Things and big data IEEE Access https Marjani et Big IoT data analytics architecture opportunities and open research challenges IEEE Access https Memeti Li Pllana Kolodziej Kessler Benchmarking OpenCL OpenACC OpenMP and CUDA programming productivity performance and energy consumption In Proceedings of the Workshop on Adaptive Resource Management and Scheduling for Cloud Computing pp ACM New York https Memeti Pllana Accelerating DNA sequence analysis using Intel R Xeon PhiTM In IEEE vol pp August Vitabile et Memeti Pllana HSTREAM language extension for erogeneous stream computing In IEEE International Conference on tational Science and Engineering CSE pp October https Mora Gil Terol Szymanski An putational framework for healthcare monitoring in mobile environments Sensors NVIDIA What is Computing April http Accessed Nov Padilla Molina Information fusion and machine learning in spatial prediction for local agricultural markets In Bajo et eds PAAMS CCIS vol pp Springer Cham https Padua D ed Encyclopedia of Parallel Computing Springer Boston https Perez Memeti Pllana simulation study of smart living IoT tion for remote elderly care In Third International Conference on Fog and Mobile Edge Computing FMEC pp April https Bustamante Molina How machine learning could detect anomalies on platform In Bajo et eds PAAMS CCIS vol pp Springer Cham https Molina Bicharra Information fusion for improving in big data applications In Pop Ko lodziej Di Martino B eds Resource Management for Big Data Platforms CCN pp Springer Cham https Qiu Wu Ding Xu Feng survey of machine ing for big data processing EURASIP Adv Sig Proces https https Raghupathi Raghupathi Big data analytics in healthcare promise and potential Health Inf Sci Syst Rassias Andrikos Tsanakas Maglogiannis Versatile cloud laboration services for medical imaging teleconsultations In IEEE International Symposium on Medical Systems CBMS pp IEEE Rawassizadeh Kotz Datasets for mobile wearable and IoT research GetMobile Mob Comput Commun https http Rawassizadeh Momeni Dobbins Rahnamoun Lesson learned from collecting quantiﬁed self information via mobile and wearable devices Sens Actuator Netw https Rodgers Pai Conroy Recent advances in wearable sensors for health monitoring IEEE Sens J https Sahoo Dehury Eﬃcient data and job scheduling rithms for healthcare cloud Comput Electr Eng Medical Data Processing and Analysis Sikora Krzyszto Marks Application of bluetooth low energy col for communication in mobile networks In International Conference on Military Communications and Information Systems ICMCIS pp May https Singh Hoque Tarkoma survey of systems for massive stream analytics arXiv preprint Snidaro Llinas information fusion survey and discussion Inf Fusion https Solanas et Smart health health paradigm within smart cities IEEE Commun Mag https Sotiriadis Vakanas Petrakis Zampognaro Bessis Automatic migration and deployment of cloud services for healthcare application development in FIWARE In International Conference on Advanced Information Networking and Applications Workshops WAINA pp IEEE The List of Most Powerful Computer Systems http Accessed Dec Vaizman Ellis Lanckriet Recognizing detailed human context in the wild from smartphones and smartwatches IEEE Pervasive Comput https Valsamis Tserpes Zissis Anagnostopoulos Varvarigou ing traditional machine learning algorithms for big data stream analysis the case of object trajectory prediction Syst Softw https https Viebke Pllana The potential of the Intel R Xeon Phi for supervised deep learning In IEEE International Conference on High Performance Computing and Communications pp August https Waehner How to apply machine learning to event processing https Wang Kung Byrd Big data analytics understanding its bilities and potential beneﬁts for healthcare organizations Technol Forecast Soc Change https http Welbourne Tapia CrowdSignals call to crowdfund the community s largest mobile dataset In Proceedings of the ACM International Joint ference on Pervasive and Ubiquitous Computing Adjunct Publication UbiComp Adjunct pp ACM New York https Zazo Hastings Croset Martinez Steinbeck An ontology for interactions In Proceedings of the tional Workshop on Semantic Web Applications and Tools for Life Sciences http Vitabile et Open Access This chapter is licensed under the terms of the Creative Commons Attribution International License http which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give appropriate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this chapter are included in the chapter s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the chapter s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder Towards Human Cell Simulation Simone Zuzana Marco Mauro Giancarlo Natalija Esko Tomas Sabri Adam Salvatore Aleˇs and Marco B Roman Department of Informatics Systems and Communication University of Milan Italy nobile Dipartimento di Elettronica Informazione Bioingegneria Politecnico di Milano Milan Italy Dipartimento di Matematica Fisica Universit degli Studi della Campania Luigi Vanvitelli Caserta Italy Faculty of Applied Informatics Tomas Bata University in Zlin Zlin Czech Republic Centre for Systems Biology Milan Italy Department of Computer Science Linnaeus University Sweden Department of Computer Science University of Niˇs Niˇs Serbia Department of Mathematics Tampere University of Technology Tampere Finland Department of Biopathology and Medical Biotechnologies University of Palermo Palermo Italy Faculty of Electrical Engineering and Computer Science University of Maribor Maribor Slovenia Abstract The faithful reproduction and accurate prediction of the notypes and emergent behaviors of complex cellular systems are among the most challenging goals in Systems Biology Although mathematical models that describe the interactions among all biochemical processes in cell are theoretically feasible their simulation is generally hard because of variety of reasons For instance many quantitative data kinetic rates are usually not available problem that hinders the execution of simulation algorithms as long as some parameter estimation methods are used Though even with candidate parameterization the simulation of mechanistic models could be challenging due to the extreme tational eﬀort required In this context model reduction techniques and Computing infrastructures could be leveraged to igate these issues In addition as cellular processes are characterized by multiple scales of temporal and spatial organization novel hybrid tors able to harmonize diﬀerent modeling approaches continuous deterministic discrete stochastic spatial should be designed This chapter describes putative uniﬁed approach to tackle these challenging tasks hopefully paving the way to the tion of comprehensive models that aim at the comprehension of the cell behavior by means of computational tools c The Author s Ko lodziej and Eds cHiPSet LNCS pp https Spolaor et Keywords simulation Big data Biochemical simulation Computational intelligence modeling Fuzzy logic computing Model reduction modeling Parameter estimation modeling Systems biology Introduction Cells are inherently complex systems composed by wide variety of molecule types whose functioning is ﬁnely regulated by an intricate network of tions In order for cells to respond to environmental cues surviving and ing all of their components have to act together in orchestrated manner This wealth of complexity is the main reason for the richness of cellular behaviours that can be found in nature but is also major issue in advancing to complete understanding of these systems In the last decades mathematical modeling and simulation proved to be essential tools to understand and describe how biological functions emerge from the complex network of interactions existing between cellular components However even though modeling and simulation proved successful in describing single processes or limited amount of interacting pathways extending this approach to deﬁne and simulate turned out to be an unfeasible task besides the notable exception reported in as it will be mentioned below especially in the case of human cells As matter of fact the deﬁnition and simulation of models is challenging for several reasons In particular the problem is exacerbated by the complex organization of cell systems the diﬃculties encountered in integrating diﬀerent data sources and mathematical formalisms in single modeling framework the huge demand of computational power needed to perform the simulation Although some of these challenges were already discussed and highlighted before see for example we hereby provide brief summary of the main challenges in the deﬁnition and simulation of models biomolecular systems are composed of wide variety of heterogeneous nents ranging from small molecules complex polymers including proteins sugars and ribonucleic acids and protein complexes All these components are further organized in functionally coherent pathways and organized in ized compartments the organelles in eukaryotic cells ultimately giving rise to complex observable phenotypes cells display complex spatial and functional hierarchical organization that results in phenomena occurring at wide range of spatial and temporal scales Moreover this organization often gives rise to complex dynamics cellular systems are inherently stochastic that is the dynamics of cellular processes is characterized by biological noise which is exploited by the cell to obtain speciﬁc responses that would be impossible in its absence Towards Human Cell Simulation Thus some cellular pathways gene expression must be modeled and simulated as stochastic processes the diﬀerent nature of the cell components entails that they are measured with diﬀerent experimental techniques Some of these components can be measured with high accuracy and with high throughput genomic or RNA sequencing mass spectrometry while others are very diﬃcult or impossible to measure kinetic information on the reaction rates Thus modelers have to take into account the presence of vast amounts of data often in qualitative or form together with limited quantitative information the availability of multiple types of data and the need to model diﬀerent layers of organization led to the deﬁnition of multiple modelling frameworks Because of this models of biochemical systems are usually focused on one of the three main layers in which cellular processes are generally divided namely signalling perceive environmental changes process mation and regulation of behaviour gene regulation control of expression levels of gene products metabolism the production and consumption driven by enzymes of small molecules essential for the life of cells Even though attempts to deﬁne single framework were made before the integration of multiple modeling approaches is still challenging However uniﬁed modeling framework for these three layers would provide reliable means to capture their peculiarities as was shown in the availability of large amounts of experimental data combined with the massive complexity of cells components leads to huge computational ments even when considering the simulation of single cell Thus dynamic mechanistic all knowledge about cal basically impossible to simulate on any existing computing architecture However we will see that by means of some assumptions about the system such complexity can be mitigated using hybrid modeling and model reduction techniques Considering all these challenges together it comes to surprise that up to date the only available example of model is the one presented in the pioneering work of Karr et In this seminal work the authors succeeded in simulating of one of the simplest known organisms the Mycoplasma genitalium adopting for each cellular process suitable mathematical ism In particular the authors showed the feasibility of predicting diﬀerent cell phenotypes from genotype by relying on computational approaches To the best of our knowledge this results was not achieved again for any more complex organism However the integration of multiple formalism into single modeling framework was already explored to smaller extents also in human cell models for example in It is out of question that the simulation of models will prove to be challenge for modelers and computer scientists in the coming decades and this is especially true in the case of human cells Here we propose set of modeling approaches and techniques that would allow us to advance towards the simulation of human models Spolaor et dynamic model would prove useful to understand how types emerge from the complex interactions existing between cellular nents Achieving dynamic simulation of human cell in silico would have an even more considerable impact in the ﬁelds of molecular and systems biology bioengineering and medicine Such model once validated could allow to uncover new and potential unknown processes inside human cells providing reliable platform to generate new hypothesis to be tested in laboratory In this regard in silico tests would guide the experimental design greatly reducing the costs both in term of time and resources of wet laboratory Moreover human cell models could be exploited to automatically assess the eﬀects of vast number of perturbations in physiological or pathological conditions in order to unveil potentially new drug targets or test known drugs in manner We envision that human cell models could lead to breakthroughs in many ﬁelds of application including medicine and personalized medicine macology and drug discovery biotechnology and synthetic biology Regardless of the methodology used to create model there are some aspects that will always characterize this kind of approach Performance Computing HPC is necessary to mitigate the huge tional eﬀort in particular by distributing the computations over massively allel machines and dynamics modelling requires proper kinetic parameterization to perform predictive simulations and such parameters are often even measure by means of laboratory ments leading to problem of parameter estimation biological models are often characterized by multiple scales temporal and spatial which are not easy to handle to reduce the huge computational eﬀort due to models both model reduction techniques or phenomenological simpliﬁcations can be aged All these topics will be introduced and discussed in this paper This manuscript is organized as follows in Sect we describe how HPC can mitigate the exceptional computational demand required by the simulation of models in Sect we propose modeling approaches for the deﬁnition of models while in Sect we suggest some techniques that could be employed to tackle the problems mentioned above in order to create uniﬁed modeling approach ﬁnally in Sect we give some ﬁnal remarks and highlight potential future directions High Performance Computing and Big Data As it was highlighted in the previous section High Performance Computing HPC architectures and handling of huge amounts of data will be necessary and enabling tools for the simulation of human cell model HPC involves the use of many interconnected processing elements to reduce the time to solution of given problem Many powerful HPC systems are heterogeneous in the sense that they combine CPUs with accelerators such as Graphics Processing Units GPUs or Field Programmable Gates Arrays FPGAs There exist several HPC approaches developed to improve the formance of advanced and data intensive modeling and simulation applications Towards Human Cell Simulation Parallel computing paradigm may be used on CPUs cessing units such as GPUs hardware platforms such as FPGAs or over distributed infrastructure such as cluster Grid or Cloud While CPUs are suitable for tasks cessors such as the Intel Xeon Phi or GPU comprise larger number of lower frequency cores and perform well on scalable applications such as DNA sequence analysis biochemical simulation or deep learning Widely used parallel programming frameworks for heterogeneous tems include OpenACC OpenCL OpenMP and NVIDIA CUDA OpenMP is set of compiler directives library routines and environment variables for programming parallel computing systems more OpenMP been extended to support programming of heterogeneous systems that contain CPUs and accelerators OpenCL supports portable gramming of hardware provided by various vendors while CUDA runs only on NVIDIA hardware CUDA compiler libraries and software enable programmers to develop and accelerate applications on GPU As concerns distributed parallel computing the available frameworks include the Message Passing Interface MPI or Apache Spark MPI is speciﬁcation of library routines helpful for users that write portable programs in Fortran or Python Basic assumption behind MPI is that multiple processes work concurrently using sages to communicate and collaborate with each other The MapReduce work and its implementation Hadoop software stack hides the details about data distribution data availability and and allows to scale up to thousands of nodes inside cluster or Cloud computing systems Lastly Apache Spark is parallel computing platform that provides wide variety of tools for structured data processing including SQL queries SparkSQL streaming applications Spark Streaming machine ing MLlib and graph operations GraphX by means of various programming interfaces in Java Scala Python and The data size in Bioinformatics Computational Biology and Systems ogy is increasing dramatically in the recent years The European Bioinformatics Institute EBI one of the largest repositories had approximately petabytes of data about genes proteins and small molecules in in parison to petabytes in Big data problems in these ﬁelds are not only characterized by Velocity Volume Value Variety and Veracity but also by incremental and geographically distributed data While part of these data may be transferred over the Internet the remaining are not transferable due to their size cost privacy and other ethical issues Moreover the computational time required by algorithms designed for the simulation of detailed tic models see Sect scales poorly when the models are characterized by huge number of components Thus in recent years research in Bioinformatics Spolaor et Computational Biology and Systems Biology started to adopt diﬀerent HPC approaches to deal with Big Data In Hadoop Blast Basic Local Alignment Search Tool in short HBlast parallelized BLAST algorithm is presented HBlast exploits the MapReduce programming framework adopting hybrid virtual partitioning approach that automatically adjusts the database partition size depending on the Hadoop ter size as well as the number of input query sequences Sadasivam et considered in time eﬃcient approach to multiple sequence alignment as essential tool in molecular biology They proposed novel approach that combines the dynamic programming algorithm with the tational parallelism of Hadoop data grids to improve accuracy and to accelerate of multiple sequence alignment Li et developed in ClustaWMPI an accelerated version of ClustalW tool for aligning multiple protein or nucleotide sequences In ClustalWMPI adopts MPI and runs on distributed workstation clusters as well as on tional parallel computers The work presented in describes new Molecular Dynamics approach named Desmond that achieves unusually high parallel scalability and overall simulation throughput on commodity clusters by using new parallel algorithms Desmond adopts novel parallel decomposition method that greatly reduces the requirement for communication novel technique that reduces the number of messages and novel highly eﬃcient communication primitives that further reduce nication time The estimation of kinetic parameters mandatory to perform cellular lations can be performed using global optimization methods see Sect for additional information These algorithms are intrinsically allel and can be accelerated using GPUs In acceleration of the Diﬀerential Evolution algorithm is considered In this work parallel mentation of an enhanced using Spark is proposed Two diﬀerent platforms have been used for the evaluation local cluster and the Microsoft Azure public cloud The proposal drastically reduces the execution time by means of ing selected local search and exploiting the available distributed resources The performance of the proposal been thoroughly assessed using ing parameter estimation problems from the domain of computational systems biology Additionally it been also compared with other parallel approaches MapReduce implementation and MPI implementation Coulier et presented in new framework named Orchestral for constructing and simulating models of multicellular systems from existing frameworks for simulation They combined the many existing frameworks for resolution models with the diverse landscape of models of cell mechanics They decoupled the simulation of diﬀusion kinetics inside the cells from the simulation of molecular actions occurring on the boundaries between cells Orchestral provides model for simulating the resulting model massively in parallel over wide range of Towards Human Cell Simulation distributed computing environments They proved the ﬂexibility and scalability of the framework by using the popular simulation software eGFRD to construct and simulate multicellular model of signaling over the OpenStack cloud infrastructure Finally HPC is exploited to accelerate the simulation of biochemical els that are deﬁned according to mechanistic formalisms refer also to Sect for some examples In this context GPUs were already fully employed to achieve considerable reduction in the computational times required by the simulation of both deterministic and stochastic els Besides accelerating single simulations of such models these ods prove to be particularly useful when there is need of running multiple independent simulations of the same model Hundreds or even thousands of simulations are often necessary to perform wide variety of analysis on validated models sensitivity analysis of kinetic parameters or parameter sweep ysis but also to perform parameter estimation PE during the deﬁnition of such models please refer to Sect for an extensive description This kind of tasks leverages at most the availability of the many cores of the GPUs greatly reducing the overall running time that is required to perform them Modeling Approach In the ﬁeld of Systems Biology several modeling approaches have been deﬁned Each approach exploits diﬀerent mathematical formalism and was developed to address the challenges posed by speciﬁc subset of biochemical processes metabolism gene regulation or signaling The deﬁnition of single homogeneous mathematical framework to model and simulate cell seems currently unfeasible while the integration of multiple formalisms already proved to be able to achieve outstanding results Following this principle we decided to deﬁne our human cell modeling framework by grating multiple modeling approaches namely i models in particular and models ii els iii models in particular boolean and fuzzy models These approaches together with their peculiarities and limitations will be brieﬂy described in the following subsections Modeling Biochemical systems are traditionally formalized as mechanistic and fully eterized models RBMs RBM is deﬁned by specifying the following sets the set S SN of molecular species the set R RM of biochemical reactions that describe the actions among the species in S the set K kM of kinetic constants associated with the reactions in R Spolaor et the set of the initial concentration Yi with i N for each species Si Any RBM can be represented in compact form AS BS where S SN K kM and B NM are the ichiometric matrices whose elements i j and B i j represent the number of reactants and products occurring in the reactions respectively Given an RBM and assuming the law of the system of coupled Ordinary ferential Equations ODEs describing the variation in time of the species centrations is obtained as follows dY dt B K where YN represents the state of the system at time t denotes the exponentiation form while the symbol denotes the Hadamard product The system can then be simulated using numerical method which is usually based on implicit integration Backward tiation Formulae due to the stiﬀness that characterizes these models When the chemical species have low concentration the dynamics of the system becomes instrinsically stochastic and the biochemical system should be simulated using speciﬁc approaches like Gillespie s Stochastic Simulation rithm SSA In SSA the simulation proceeds one reaction at time Both the reaction to be ﬁred and the time interval τ before the reactions occur are determined in probabilistic fashion Thus the simulated trajectory of the tem can radically diverge from the one predicted by deterministic simulation allowing the investigation of the emergent eﬀects due to the intrinsic noise and providing deeper knowledge of the system s behavior In the case of stochastic modeling the state of the system represents the exact number of molecules K denotes the vector of the stochastic constants encompassing all the physical and chemical properties of the reactions These parameters are used to calculate the propensity functions ultimately determining the probability of each reaction Rm to occur Propensity functions are deﬁned as am km dm where dm is the number of distinct combinations of reactant molecules ring in Rm The delay time τ before the next reaction will occur is calculated according to the following equation τ ln rnd am and rnd is random number sampled with uniform M where distribution in Mechanistic modeling is considered the most likely candidate to achieve detailed comprehension of biological systems since it can lead to tive predictions of cellular dynamics thanks to its capability to reproduce the Towards Human Cell Simulation temporal evolution of all molecular species occurring in the model Nonetheless the computational complexity of the simulation and analysis of such models increases with the size in terms of components and interactions of the systems limiting the feasibility of this approach Moreover the usual lack of tive parameters kinetic constants initial molecular concentrations of the species and the partial lack of knowledge about the molecular mechanisms sometimes due to the diﬃculty or impossibility to perform ad hoc experiments represent further limits to wide applicability of this modeling approach The problems of simulation performances and parameter estimation are discussed in the next Sections Modeling Modeling CBM is based on the idea that phenotypes of given biological system must satisfy number of constraints Hence by ing the space of all possible systems states it is possible to determine the tional states that biochemical in particular metabolic network can or can not achieve The fundamental assumption of modeling is that the organism will reach state that satisﬁes the given constraints The starting point of CBM is the transposed stoichiometric matrix S T matrix in which each row corresponds to chemical species metabolites while columns correspond to reactions involving those species Since metabolic networks typically include more reactions ﬂuxes than bolites the stoichiometric constraints and the steady assumption alone lead to an system in which bounded solution space of all feasible ﬂux distributions can be identiﬁed Additional constraints should be incorporated to further restrict the solution space this is usually performed by specifying linear bounds to minimum and maximum values of ﬂuxes Additioanl capacity constraints are generally set according to experimental data On top of CBM Flux Balance Analysis FBA can be used to identify mal distribution of ﬂuxes with respect to given objective function Thanks to the linear deﬁnitions of ﬂuxes constraints and objective function the solution space is convex polytope FBA exploits simplex method to eﬃciently identify the optimal ﬂuxes that maximize or minimize the objective function the maximization of ATP in the context of mithocondria energy metabolism CBM methods do not perform an actual simulation of the biochemical system but can be state to investigate the distribution of ﬂuxes Interestingly FBA very limited computational complexity so that it can be leveraged to study the behavior of metabolic systems on level Markovian Agents Markovian agents are modeling tool that is specially suitable for large scale phenomena composed of groups of single entities that behave as Markov chains Such entities said agents are individuals belonging to classes that are Spolaor et characterized by common description of their dynamics Agents may inﬂuence each other by means of technique called induction which accounts for their position in logic map that represents the space in which they can move or be positioned in the system The system is described by considering for each class the density of agents in each state and the probability of transition between states so that thanks to approach the evolution in time of the density in states may be approximately described by diﬀerential equations and closed form solution may be obtained with the signiﬁcant advantage that the higher is the number of agents in class the best the approximation describes the system The communication mechanism acts by enabling or disabling transitions thus inﬂuencing the probability of transitions between states This analytical description is suitable to study both transient and regime behavior of the system Markovian agents may be used to describe the interactions of reactions that happen in cell in large number of independent instances including the eﬀects of inhibiting factors as well as for describing the expression of cells in tissues and organs The technique been applied to study biological pathways cancer cells whole ecosystems such as forestry landscape and other complex systems The Markovian property make them able to describe processes that are characterized by exponentially distributed interarrival time in their evolution U From the formal point of view let Markovian agents model be composed by diﬀerent classes with each class c characterized by Markov chain with nc states the space Σ in which agents are located and can move is ﬁnite and can be continuous or discrete The distribution of agents in the space can be represented by density function δ Σ so that considering any space U Σ the number of agents in U is described by Poisson distribution δ x dx The model evolves by accounting for state changes of with mean agents in their class and induction eﬀects birth of agents and death of agents its evaluation can be obtained as counting process per each class that counts the number of agents in each state of its Markov chain in each position in space and in each instant Let χc l t c i l t be vector of size n c with each element χ c i l t representing the average number of agents of class c in state i at time t and in location If the space is discrete the evolution of the counting process is thus described by set of ordinary diﬀerential Equations for each class c and in location l bc χ l t χc l t Kc χ l t dχc l t dt where χ denotes the dependency on all the state of all agents in the model in any time instant matrix Kc is the main transition kernel that accounts for spontaneous and induced actions contribution and bc is the birth vector of new agents for the class in state If the space is continuous movement of agents is described by diagonal velocity matrix ωc described in Eq that can be obtained by summing the contributions for each direction Towards Human Cell Simulation ωc χ l t χc l t ωxc χ l t χc l t ωyc χ l t χc l t and Eq is modiﬁed accordingly and becomes Eq l t ωc χ l t χc l t bc χ l t χc l t Kc χ l t in which the second term accounts for the eﬀects of agents movement by vc Modeling In contrast with and models model do not require kinetic or stoichiometric information to be deﬁned Although these models can describe the system under consideration only in qualitative terms they provide an eﬃcient way to simulate the dynamic evolution of complex systems even when precise kinetic information is not available Thanks to their closeness to human language models are able to leverage qualitative and data and they are generally regarded as more intepretable by human experts Moreover their ﬂexibility allow modelers to represent in the same model highly heterogeneous components and the interactions existing among them models are deﬁned by set of υ variables V and set of φ logic rules F describing the interactions existing between the nents Evaluation of the rules in discrete time steps drives the system s ics this can be achieved by either synchronous deterministic or asynchronous stochastic update policy models are commonly employed in systems biology to model gene regulatory networks and signal processing Among them Boolean models are the most simple and widely used in this kind of models variables can assume only two discrete states often represented as and active or inactive present or not present Diﬀerent Boolean logic models were successful in predicting cellular behaviours however these tions often limit their ability of representing biomolecular processes In order to overcome these limitations more recently fuzzy logic was proposed as an alternative to the modeling of complex biochemical systems Fuzzy logic is powerful extension of boolean logic which allows variables to assume multiple states in continuous manner between and deal with any uncertainty related to the system More in particular fuzzy inference systems are composed of φ rules of type IF is in and is in and and vυ is in υ THEN is in IF is in and is in and and vυ is in υ THEN is in IF is in and is in and and vυ is in Vσ υ THEN is in Oσ Spolaor et where vj V with i υ while the sets Vi j and Oi with i σ and j υ are fuzzy sets that is the membership of the value assumed by generic variable v V for the fuzzy subset V is equal to degree α This is denoted by μV v α If all the considered sets are classical sets always holds μV v then the inference system is boolean An advantage of fuzzy reasoning is that thanks to the fuzzy sets it can handle uncertainty and conﬂicting conclusions drawn from the logic rules Thus it can allow for the dynamic simulation of qualitative and semiquantitative models even when precise kinetic information is missing Fuzzy logic been applied to vastly diﬀerent ﬁelds of research ranging from automatic control to medicine but it was successfully applied also in the ﬁeld of cellular biology for example to model signaling pathways and gene regulatory networks We plan to exploit fuzzy logic in our hybrid framework to overcome the lack of kinetic parameters and model those cellular processes that still are not understood in mechanistic detail or whose components can not be represented by crisp variables complex phenotype as microscopy imaging data Uniﬁed Modeling Approach In principle the SSA algorithm described in Sect can be used to simulate stochastic trajectory of any biological model including model and such dynamics would be exact with respect to the Chemical Master Equation CME underlying the corresponding set of biochemical reactions This approach could be even extended to consider the diﬀusion of molecules inside the cell like in the case of the Next Subvolume Method NSM However both SSA and NSM perform the simulations by applying single reaction at time proceeding with time steps that are inversely proportional to the sum of the propensities see Eq which in turn is proportional to the amount of reactants in the system These circumstances generally cause an explosion of the computational eﬀort due to exact stochastic simulation making it unfeasible for simulation An approximate but faster version of SSA called was posed by Gillespie to reduce the computational burden typical of SSA by ing that the propensities do not change during given the leap condition the number of reactions ﬁring can be approximated by Poisson random variables When the number of estimated reaction ﬁrings for all reactions increases the Poisson processes can be approximated by normal distribution with same mean and variance In this case Stochastic Diﬀerential Equations SDEs like the Langevin equations can be exploited to model the system which is then simulated using numeric solvers like the method strongly reducing the overall computational eﬀort Finally when the propensities become extremely large the noise term in the SDEs becomes negligible and can be removed so that the system can be modeled using simple ODEs The proper modeling approach must be carefully selected according to the characteristics of the chemical system Unfortunately cellular mechanisms are Towards Human Cell Simulation controlled by reactions and pathways spanning over multiple scales so that none of these modeling methods is really adequate By partitioning the reactions set R into multiple regimes according to their characteristics their propensity values it is possible to simulate each subsystem using the optimal modeling approach It is clear that the ﬁring of reactions in one regime can have huge impact to the others so that the synchronization to propagate the information across the mandatory and very delicate phases of hybrid simulators like in the case of the Partitioned Leaping Algorithm PLA By extending PLA by considering the additional modeling approaches described in Sect it is possible to achieve models In this project we pursue the integration of these modeling approaches pushing the limits of human cells simulation In order to mitigate the huge computational requirements we plan to exploit model reduction and automatic simpliﬁcation algorithms We also plan to perform an automatic inference of some missing parts of the model reactions rules parameters exploiting evolutionary and statistical methods Finally we will test approaches to work on multiple scales multiple cells or tissue simulation All these approaches will be described in the next subsections Model Reduction and Simpliﬁcation The complexity of cellular systems poses some limitations on the scale of the models that can be simulated In this context model reduction techniques can be used to tame the complexity before the execution of simulation algorithms is performed The theory of complex networks raised great development over the recent years The empirical and theoretical results analyzing several real tems show that complex networks can be classiﬁed using its probability bution function P k the probability that node is connected to k nodes of network network the grades distribution function ﬁtting the function Several studies examining the cellular metabolism of diﬀerent organisms have been conducted for determining the topological ture of metabolic network In this direction studies of and Albert have also analyzed many issues in networks In many organism the metabolic networks are composed of interconnected functional modules and follow the model Three statistical sures can be considered in network the connectivity degree the diameter of the graph and the clustering coeﬃcient The connectivity degree of node is the number of incident arcs and it allows also for calculating the distribution function of the connectivity degree The diameter provides an mation of the average number of hops between any pair of nodes in the network It is also linked to the shortest paths between each node pair as well as to the number of paths in the network Finally the clustering coeﬃcient gives measure of the properties of nodes to form agglomerates In addition metabolic network nodes can be classiﬁed into distinct groups considering the following parameters Spolaor et the degree the membership degree of node into its functional module and the participation coeﬃcient measure of the node interaction with network functional modules The above parameters can be used to deﬁne and hub nodes as well as peripheral provincial connector and kinless nodes These metrics pave the way to the topological sis of network providing information on the connectivity and the participation degrees of each node within the network The topological analysis of network can be completed by functional sis cellular network is hierarchically organized with several functional modules Methods for rational decomposition of the network into independent functional subsets are essential to understand their modularity and organization principles Using the modularization approach commonly used in the area of control theory cellular network can be viewed as an assembly of basic building blocks with its speciﬁc structures characteristics and interactions larization reduces the diﬃculty in investigating complex network Network decomposition is also needed for cellular functional analysis through pathway analysis methods that are often troubled by the problem of combinatorial sion due to the complexity of those networks Two main methods can be used for network functional analysis and as sequence for network model reduction and simpliﬁcation Flux Balance Analysis FBA and Extreme Pathways Analysis ExPA FBA is mathematical technique based on fundamental physical and mical laws that quantitatively describe the metabolisms of living cells FBA is modeling approach it assumes that an organism reaches under any given environmental condition that satisﬁes the ochemical constraints and uses the mass and energy balance to describe the potential cellular behavior FBA model been developed considering the mass and energy conservation law for each the sum of incoming ﬂuxes must be equal to the sum of the outgoing ones The space of all feasible solutions of linear equation constrained system lies within convex polyhedron in which each point of this space satisﬁes the constraints of the system When the system an optimal and limited solution this is unique and it is located on polyhedron vertex However the system can have multiple optimal solutions axis or plan that are used to detect network redundancies ExPA analysis detects the vital pathways in network They are the unique set of vectors that completely characterize the capabilities of work network operation is constrained to the region within cone deﬁned as the feasible set In some special cases under certain constraints this feasible set collapse in single point inside the cone The algorithm detects the extreme vectors of convex polyhedral cones Algorithm time execution is proportional to the number of nodes and pathways Many software frameworks for cellular networks analysis and simulation have been developed Some solutions such as Pajek allows for either large Towards Human Cell Simulation complex networks analysis and visualization or network structural properties and quantities analysis CellNetAnalyzer is MATLAB package for performing biochemical networks functional and structural analysis The BIAM framework implements an integrated analysis methodology based on topological analysis FBA analysis and Extreme Pathways analysis The framework supplies the needed tools for drawing network and analyzing its structural and functional properties Several network architectures dealing with diﬀerent application domains have been simulated and validated Topological and functional analysis can be combined to select the main functional nodes and paths of cellular network Redundant nodes and paths could be ignored before the execution of simulation rithms reducing the overall computational complexity of large scale simulation Parameter Estimation Mechanistic models are characterized by kinetic parameterization the K vector described in Sect precise estimation of such parameters is tory to perform faithful simulations of the system s dynamics The problem of Parameter Estimation PE can be formulated as minimization problem the goal is to reduce to zero distance between the target experimental time and simulated dynamics performed with the optimal vector of parameters Due to the characteristics of the ﬁtness landscapes deﬁned by the PE problem noisy classic optimization methods can not be employed eﬃciently On the contrary tational Intelligence CI methods based on evolutionary computation or swarm intelligence were shown to be eﬀective for this problem in particular the variant of PSO named Fuzzy PSO Moreover CI methods can be combined with probabilistic frameworks maximization methods to eﬃciently tackle the PE of stochastic models see for example However when the number of missing parameters in the model becomes extremely large like in the case of models conventional CI methods can show some limitations and methods must be employed Among the existing CI algorithms for large number of parameters ential Evolution variants like the recent DISH algorithm could be exploited algorithm was introduced in by Storn and Price and since then formed basis for set of successful algorithms for optimization domains such as continuous discrete or other search spaces and features The whole encompassing research ﬁeld around was surveyed most recently in and even since then several other and speciﬁc surveys studies and comparisons have also followed oretical insight and insights to inner workings and behaviors of during secutive generations been studied in the works like As the continuing research in enhancements and insight supports much vigorous research community the algorithm variants have also steadily placed top in competitions held annually at Congress on Evolutionary putation CEC For this reason we expect these Spolaor et advanced versions of to be eﬀective for the PE problem and outperform classic algorithms especially on high dimensional problem The most recent variants strain of is the based tive Diﬀerential Evolution SHADE which line of recent ments following taxonomy stemming from JADE that is based on jDE upgraded as cnEpSin jSO and most recently DISH These algorithms include diﬀerent mechanisms and to describe the basic outline ing principle to apply from the following paragraph on the basic canonical is described The canonical is based on parameter estimation through evolution from randomly generated set of solutions using population P which preset size of NP Each individual set of estimated parameter values in this population P consists of vector x with of length Each vector x nent corresponds to one attribute of the optimized task for which parameters are being estimated The objective function value f x evaluates quality of the solution The individuals in the population create improved oﬀspring for the next generation This process is repeated until the stopping criterion is met either the maximum number of generations or the maximum number of objective tion evaluations or the population diversity lower limit or overall computational time creating chain of subsequent generations where each following tion consists of eventually better solutions than those in previous generations Some of most used computational operators operating on population P over each generation and its vectors are parameter adaptation mutation crossover selection and population restructuring including tion of population size First all vectors in the initial population are formly generated at random between bounds xlower j xupper j D xi U xlower j xupper j D N P then three mutually and from current vector index i diﬀerent indices and are used to computing diﬀerential vector hence the name for algorithm and combine it in scaled diﬀerence manner vi F which is then taken into crossover with the current vector at index i uj i vj i if U CRi or j jrand xj i otherwise ﬁnally through selection yielding new vector xi at this location i for next generation G xi ui G if f ui G f xi G xi G otherwise As mentioned in the beginning of this subsection the work on is ing and still challenging To apply most eﬃciently on new challenge for Towards Human Cell Simulation parameter estimation like the discussed simulation in this chapter one of tive variants should be taken and adapted for the domain challenge at hand following recent experiences on applications in image processing energy scheduling and autonomous vehicle navigation To assess the feasibility of DISH for the PE problem we plan to compare its performances against methods in particular the aforementioned variants of and those algorithms that were shown eﬀective for the PE in previous studies PSO and Another approach for that may be beneﬁcial for the given application is through unconventional synergy of the with several diﬀerent research ﬁelds belonging to the computational intelligence paradigm which are the stochastics processes complex chaotic dynamics and complex networks CN As the key operation in metaheuristic algorithms is the randomness the popularity of hybridizing them with deterministic chaos is growing every year due to its unique features Recent research in chaotic approach for tics mostly uses straightforwardly various chaotic maps in the place of random number generators The observed performance of enhanced optimizer is signiﬁcantly diﬀerent mostly the chaotic maps secured very fast progress towards function extreme but often followed by premature convergence thus overall statistics given mixed results Nevertheless as reported in the the chaos driven heuristics is performing very well especially for some instances in the discrete domain The CN approach is utilized to show the linkage between diﬀerent individuals in the population Interactions in algorithms during the optimization process can be considered like user interactions in social networks or just people in society The population is visualized as an evolving CN that exhibits features degree distribution clustering and ties These features can be then utilized for the adaptive population control as well as parameter control during the metaheuristic run Analysis of CNs from algorithm can be found in and also in comprehensive study discussing the usability of network types Automatic Inference of Fuzzy Rules Fuzzy inference systems are typically constructed by consulting human experts who give the related fuzzy rules shapes of the corresponding fuzzy sets and all the other required information However when human experts are not available or in the presence of numerous system components rules the deﬁnition of the inference system results to be particularly time consuming and laborious An alternative approach is exploiting data mining methods in order to automatically build inference systems by leveraging available data In particular here we focus on GUHA General Unary Hypotheses ton method of automatic generation of hypotheses based on empirical data GUHA is based on particular ﬁrst order logic language which allows to treat symbolically sentences such as α appears often simultaneously with β in most cases α implies β α makes β very probable etc The GUHA method Spolaor et is implemented in the LISpMiner software which is freely downloadable from https Once the user provides relevant analytic tions regarding the data the LISpMiner software outputs the dependencies between the variables that are supported by the data In practice LISpMiner runs through millions of fourfold contingency tables from which it outputs those which support the dependence provided by the user From these ﬁndings the inference system can then be constructed GUHA and LISpMiner were already successfully employed in diﬀerent ﬁelds in the context of human cell modeling this approach could be exploited in order to automatically build large fuzzy inference systems In particular this data mining method could leverage the vast availability of transcriptomic data which nowadays can be generated in short time for reasonable cost and at resolution In such way we envision that the automatic generation of dynamic fuzzy models of cellular processes would be feasible Such models would represent signiﬁcant step forward towards the integration of cellular processes that are not known in full mechanistic detail or necessitate of qualitative or representation inside uniﬁed framework for human cell modelling and simulation Multiformalism Approaches Given the complexity and the heterogeneity of the that terize the challenge posed by modeling promising approach can be provided by multiformalism modeling Multiformalism modeling oﬀers the possibility of obtaining complex models by allowing the coexistence of ent modeling formalisms in the same model using model composition model generation model abstraction on the basis of diﬀerent supporting mechanisms Multiformalism approaches allow the representation of each subsystem with the most appropriate representation or with the description that is more familiar for the developer of that submodel easing the interaction between experts from diﬀerent domains without forcing any of them to relinquish established ing practices this allows to preserve existing know how and minimizes the eﬀort needed to integrate the overall model that is process that is supported by proper specialist in formalism design Multiformalism models may be supported by closed frameworks that support predeﬁned set of formalisms or by open frameworks that are designed to allow the deﬁnition of new formalisms The solution or analysis of multiformalism models may be performed by generating speciﬁc solvable model by generating or instantiating tion tool by orchestrating speciﬁc solvers for diﬀerent submodels by producing executable code Solution can be obtained by means of simulation analytical techniques or by applying multisolution that is the possibility of using nate tools explicitly decided by the modeler or automatically chosen according to the characteristics of the model to perform the analysis This approach also preserves in general tracking of numerical results back to logical elements in the model and can provide or results such as properties Towards Human Cell Simulation of parts of the system that emerge from results and may also be used to interface existing tools with new solvers extending their ity Multiformalism modeling approaches may support combinatorial malisms logic modeling discrete state space based formalisms continuous state space based formalisms and hybrid formalisms that may use specialized solution techniques More details about multiformalism modeling concepts and principles are available for the reader in and For similar and wider concept namely multiparadigm modeling the reader can refer to Future Developments In this chapter we described putative hybrid modeling and simulation several diﬀerent approaches RMBs CBMs boolean and fuzzy rules and leveraging to perform cell simulations In this context we highlighted some issues that vent the simulation of models proposing some approaches in order to achieve this challenging task In particular we propose the use of metaheuristics for global optimization to estimate the large number of missing kinetic ters The emphasis in future research will be on modifying and testing robust algorithms based on inspired by techniques successfully adopted for solving highly constrained and problems We will compare this class of algorithms against swarm intelligence techniques PSO and that were shown to be the most eﬀective in previous empirical studies Furthermore thorough analysis of the relatively good results of genetic algorithms can help to develop powerful metaheuristics Moreover it is sary to emphasize the fact that like most of the above mentioned metaheuristic methods they are inspired by natural evolution and their development can be considered as form of evolution Such fact is mentioned in the paper that even incremental steps in algorithm development including failures may be the inspiration for the development of robust and powerful metaheuristics Future directions in can be discussed not only in the journals like Swarm and Evolutionary Computation IEEE Transactions on Evolutionary Computation or Evolutionary Computation but also at forthcoming conferences like Swarm Evolutionary and Memetic Computing Conference SEMCCO IEEE Congress on Evolutionary Computation CEC and The Genetic and Evolutionary putation Conference GECCO all forthcoming also for year lot of work still needs to be done in order to achieve faithful sentation of human cell in silico The uniﬁed approach that we propose in this work although challenging to achieve and possibly able to capture wide variety of cellular behaviors must be considered just as starting point As matter of fact many additional layers of complexity can still be considered We assume that the biochemical systems is but this is often not Spolaor et the case Spatial modeling and simulation can be leveraged to capture the nization in space of molecules membrane receptors cell organelles and cell shape itself The combinatorial complexity of the formation of huge protein complexes or can also be tackled by means of speciﬁc modeling and simulation frameworks Moreover cells are not closed systems they respond to environmental cues and they continuously interact with with other cells by exchanging chemical signals Furthermore cell s life cycle is dinated by complex cell cycle program that allows them to grow and divide and they are constantly subjected to the evolutionary pressure posed by the environment External signals and cell cycle both require additional complex modeling approaches that are currently not considered in our approach Whilst we envision that human cell simulation will remain challenging task for the coming decades we are working in that direction as it carries the promise of elucidating the very basic mechanisms governing the functioning of our bodies and life itself Acknowledgements This chapter is based upon work from the COST Action cHiPSet supported by COST European Cooperation in Science and Technology The author AZ acknowledges the ﬁnancial support from the Slovenian Research Agency research core funding AZ also acknowledges EU support under project HPC RIVR This chapter is also based upon work from COST Action Improving Applicability of Optimisation by Joining Theory and Practice ImAppNIO supported by COST The authors RS and ZKO also acknowledges that work was supported by the Ministry of Education Youth and Sports of the Czech Republic within the National Sustainability Programme Project further supported by the European Regional opment Fund under the Project Further authors TK and AV acknowledges the support of Internal Grant Agency of Tomas Bata University under the Projects References Neri Idris Baba Algorithmic design issues in adaptive diﬀerential evolution schemes review and taxonomy Swarm Evol put https Albert Statistical mechanics of complex networks Rev Mod Phys Aldridge Muhlich Sorger Lauﬀenburger Fuzzy logic analysis of kinase pathway crosstalk in induced signaling PLoS Comput Biol Awad Ali Suganthan Reynolds An ensemble sinusoidal parameter adaptation incorporated with for solving mark problems In IEEE Congress on Evolutionary Computation CEC pp IEEE Oltvai Network biology understanding the cell s functional organization Nat Rev Genet Barbierato Bobbio Gribaudo Iacono Multiformalism to support software rejuvenation modeling pp Towards Human Cell Simulation Barbierato Gribaudo Iacono Modeling and evaluating the eﬀects of Big Data storage resource allocation in global scale cloud architectures Int J Data Warehous Min https Barbierato Gribaudo Iacono Modeling hybrid systems in SIMTHESys Electron Notes Theor Comput Sci Barbierato Gribaudo Iacono Simulating hybrid systems within SIMTHESys models In Fiems Paolieri Platis eds EPEW LNCS vol pp Springer Cham https Barbierato Gribaudo Iacono Exploiting CloudSim in multiformalism modeling approach for cloud based systems Simul Model Pract Theory Benkner et PEPPHER Eﬃcient and productive usage of hybrid puting systems IEEE Micro https Besozzi models of biochemical networks In Beckmann Bienvenu Jonoska N eds CiE LNCS vol pp Springer Cham https Bobbio Cerotti Gribaudo Iacono Manini Markovian agent models dynamic population of interdependent Markovian agents In Bargiela eds Seminal Contributions to Modelling and Simulation SFMA pp Springer Cham https Bordon Moˇskon Zimic Mraz Fuzzy logic as computational tool for quantitative modelling of biological systems with uncertain kinetic data Trans Comput Biol Bioinform Bowers et Scalable algorithms for molecular dynamics simulations on commodity clusters In Proceedings of the SC Conference IEEE Brest Greiner Mernik trol parameters in diﬀerential evolution comparative study on numerical mark problems IEEE Trans Evol Comput Brest Koroˇsec ˇSilc Zamuda Mauˇcec tial evolution and diﬀerential on dynamic optimisation problems Int Syst Sci Brest Mauˇcec Single objective tion algorithm jSO In IEEE Congress on Evolutionary Computation CEC pp IEEE Cash Backward diﬀerentiation formulae In Engquist B ed Encyclopedia of Applied and Computational Mathematics pp Springer Heidelberg https Computational Science Engineering edn Springer Heidelberg Cazzaniga et Computational strategies for understanding of metabolism Metabolites Cerotti Gribaudo Bobbio Calafate Manzoni vian agent model for ﬁre propagation in outdoor environments In Aldini Bernardo Bononi Cortellessa V eds EPEW LNCS vol pp Springer Heidelberg https Spolaor et Chellaboina Bhat Haddad Bernstein Modeling and analysis of kinetics IEEE Control Syst Mag https Pandey Ataman Hatzimanikatis Integration of metabolic regulatory and signaling networks towards analysis of perturbation and dynamic responses Curr Opin Syst Biol Chrysos Intel R Xeon PhiTM Architecture Intel Whitepaper Ciardo Jones III Miner Siminiceanu Logic and stochastic modeling with smart Perform Eval Conti Ruﬀo Vitabile Barolli BIAM new ysis methodology for digital ecosystems based on architecture Soft Comput Cordero Manini Gribaudo Modeling biological pathways an oriented like methodology based on mean ﬁeld analysis In Third International Conference on Advanced Engineering Computing and Applications in Sciences pp October https Cordero Fornari Gribaudo Manini Markovian agents tion models to study cancer evolution In Sericola Telek G eds Analytical and Stochastic Modeling Techniques and Applications pp Springer Cham https Coulier Hellander Orchestral lightweight framework for parallel lations of communication arXiv preprint Dada Mendes modelling and simulation in systems biology Integr Biol Das Abraham Chakraborty Konar Diﬀerential evolution using mutation operator IEEE Trans Evol Comput Das Mullick Suganthan Recent advances in diﬀerential evolution an updated survey Swarm Evol Comput Davendra Senkerik Scheduling the ﬂowshop scheduling problem with setup time with the enhanced diﬀerential evolution In IEEE Symposium on Diﬀerential Evolution SDE pp IEEE Nooy Mrvar Batagelj Exploratory Social Network Analysis with Pajek Cambridge University Press Cambridge Kronfeld Ziller Supper Planatscher Magnus Modeling metabolic networks in glutamicum comparison of rate laws in combination with various parameter optimization strategies BMC Syst Biol Dubrovin Jolma Turunen Fuzzy model for reservoir ation J Water Resour Plan Manag Eldar Elowitz Functional roles for noise in genetic circuits Nature Elf Ehrenberg Spontaneous separation of biochemical systems into spatial domains of opposite phases IEE Biol Elowitz Levine Siggia Swain Stochastic gene expression in single cell Science Faeder Blinov Hlavacek modeling of cal systems with BioNetGen In Maly I ed Systems Biology pp Springer Heidelberg https Towards Human Cell Simulation Fisher Plant Moore Kierzek QSSPN dynamic simulation of molecular interaction networks describing gene regulation signalling and cell metabolism in human cells Bioinformatics Franceschinis Gribaudo Iacono Marrone Mazzocca torini Compositional modeling of complex systems contact center scenarios in OsMoSys In ICATPN pp Gillespie general method for numerically simulating the stochastic time evolution of coupled chemical reactions Comput Phys Gillespie Approximate accelerated stochastic simulation of chemically reacting systems Chem Phys et Bridging the layers towards integration of signal tion regulation and metabolism into mathematical models Mol BioSyst Gribaudo Iacono An introduction to multiformalism modeling Gribaudo Iacono Levis An monitoring approach for cultural heritage sites the Matera case Concurr Comput Pract Exp https Gropp Lusk Skjellum Using MPI Portable Parallel Programming with the Interface vol MIT Press Cambridge Guimera Amaral Functional cartography of complex metabolic works Nature Guo Tsai Yang Hsu approach for incorporated with crossover and selecting framework on CEC benchmark set In IEEE Congress on Evolutionary Computation CEC pp IEEE Hadoop Apache Hadoop project https Accessed Nov Harris Clancy partitioned leaping approach for multiscale modeling of chemical reaction dynamics Chem Phys Harris et model analysis with formatics Hecker Lambeck Toepfer Van Someren Guthke Gene latory network inference data integration in dynamic models review Biosystems Manini Parameter estimation of kinetic rates in stochastic tion networks by the EM method In International Conference on BioMedical Engineering and Informatics BMEI vol pp IEEE European Bioinformatics report Institute https ar Accessed Dec scientiﬁc annual Jeong Tombor Albert Oltvai The organization of metabolic networks Nature Karr et computational model predicts phenotype from genotype Cell Kauﬀman Prakash Edwards Advances in ﬂux balance analysis Curr Opin Biotechnol Kessler et Programmability and performance portability aspects of erogeneous systems In Design Automation Test in Europe Conference Exhibition DATE pp IEEE Kitano Systems biology brief overview Science Spolaor et Klamt Gilles Structural and functional analysis of cellular networks with CellNetAnalyzer BMC Syst Biol Petri Windhager Zimmer Petri nets with fuzzy logic PNFL reverse engineering and parametrization PLoS ONE Lacroix Cottret Sagot An introduction to metabolic networks and their structural analysis Trans Comput Biol form TCBB Li ClustalW analysis using distributed and parallel computing Bioinformatics https Liu Heiner Yang Fuzzy stochastic Petri nets for modeling biological systems with uncertain kinetic parameters PloS ONE Macklin Ruggero Covert The future of modeling Curr Opin Biotechnol Mallipeddi Suganthan Pan Tasgetiren Diﬀerential tion algorithm with ensemble of parameters and mutation strategies Appl Soft Comput Marx Biology the big challenges of big data Nature Memeti Li Pllana Kolodziej Kessler Benchmarking OpenCL OpenACC OpenMP and CUDA programming productivity performance and energy consumption In Proceedings of the Workshop on Adaptive Resource Management and Scheduling for Cloud Computing pp ACM New York https Memeti Pllana Accelerating DNA sequence analysis using Intel R Xeon PhiTM In IEEE vol pp August Metlicka Davendra Chaos driven discrete artiﬁcial bee algorithm for location and assignment optimisation problems Swarm Evol Comput Mininno Neri Cupertino Naso Compact diﬀerential evolution IEEE Trans Evol Comput Morris Sorger Lauﬀenburger models for the analysis of cell signaling networks Biochemistry Mosterman Vangheluwe Computer automated ing an introduction Simulation https Nobile Besozzi Cazzaniga Mauri simulations of kinetics models with cupSODA Supercomput Nobile Cazzaniga Tangherloni Besozzi Graphics processing units in bioinformatics computational biology and systems biology Brief form https Nobile Besozzi Cazzaniga Mauri Pescini PSO method for parameter estimation in stochastic biological tems exploiting target series In Giacobini Vanneschi Bush eds EvoBIO LNCS vol pp Springer Heidelberg https Towards Human Cell Simulation Nobile Besozzi Cazzaniga Mauri Pescini Estimating tion constants in stochastic biological systems with PSO running on GPUs In Proceedings of the Annual Conference Companion on Genetic and Evolutionary Computation pp ACM Nobile Cazzaniga Besozzi Colombo Mauri Pasi Fuzzy PSO algorithm for global optimization Swarm Evol Comput Nobile Cazzaniga Besozzi Pescini Mauri cuTauLeaping stochastic simulator for massive parallel analyses of biological systems PLoS ONE Nobile Mauri Accelerated analysis of biological parameters space using GPUs In Malyshkin V ed PaCT LNCS vol pp Springer Cham https Nobile et Computational intelligence for parameter estimation of chemical systems In IEEE Congress on Evolutionary Computation CEC pp IEEE NVIDIA CUDA C Programming Guide September http Accessed Nov NVIDIA What is Computing April http Accessed Nov Driscoll et HBLAST parallelised sequence Hadoop MapReducable basic local alignment search tool Biomed Inform https http Opara Arabas Diﬀerential evolution survey of theoretical analyses Swarm Evol Comput https OpenMP OpenMP Speciﬁcations July http Accessed Mar Padua Encyclopedia of Parallel Computing Springer Heidelberg Piotrowski Review of diﬀerential evolution population size Swarm Evol Comput Piotrowski optimization algorithms with tia Inf Sci Piotrowski Napiorkowski Some metaheuristics should be simpliﬁed Inf Sci Piotrowski Napiorkowski improvement of JADE and algorithms success or failure Swarm Evol Comput https Poli Kennedy Blackwell Particle swarm optimization Swarm Intell Poovathingal Gunawan Global parameter estimation methods for stochastic biochemical systems BMC Bioinform Provost Bastin Metabolic ﬂux analysis an approach for solving stationary underdetermined systems In Proceedings MATHMOD Conference Paper vol Citeseer Qin Huang Suganthan Diﬀerential evolution algorithm with strategy adaptation for global numerical optimization IEEE Trans Evol put Qu Suganthan Liang Diﬀerential evolution with neighborhood mutation for multimodal optimization IEEE Trans Evol Comput Spolaor et Saastamoinen Ketola Turunen Deﬁning athlete s anaerobic and obic thresholds by using similarity measures and diﬀerential evolution In IEEE International Conference on Systems Man and Cybernetics vol pp IEEE Sadasivam Baktavatchalam novel approach to multiple sequence alignment using Hadoop data grids In Proceedings of the Workshop on Massive Data Analytics on the Cloud MDAC pp ACM New York https Sanders Integrated frameworks for and eling In Proceedings International Workshop on Petri Nets and Performance Models Cat pp September https Sanders Courtney Deavours Daly Derisavi Lam and modeling frameworks the approach Schilling Letscher Palsson Theory for the systemic deﬁnition of metabolic pathways and their use in interpreting metabolic function from perspective Theor Biol Senkerik Pluhacek Oplatkova Davendra Zelinka gation on the diﬀerential evolution driven by selected six chaotic systems in the task of reactor geometry optimization In IEEE Congress on Evolutionary Computation CEC pp IEEE Senkerik Viktorin Pluhacek Janostik Davendra On the inﬂuence of diﬀerent randomization and complex network analysis for diﬀerential evolution In IEEE Congress on Evolutionary Computation CEC pp IEEE Senkerik Viktorin Pluhacek Kadavy Oplatkova Diﬀerential evolution and chaotic series In International Conference on Systems Signals and Image Processing IWSSIP pp IEEE Senkerik Zelinka Pluhacek Davendra Kominkova Chaos enhanced diﬀerential evolution in the task of evolutionary control of selected set of discrete chaotic systems Sci World J Skanderova Fabian Diﬀerential evolution dynamics analysis by complex networks Soft Comput Skanderova Fabian Zelinka Diﬀerential evolution dynamics modeled by longitudinal social network Intell Syst Skanderova Fabian Zelinka Analysis of changes of diﬀusion speed in temporal networks generated on the basis of diﬀerential evolution dynamics Swarm Evol Comput Sneddon Faeder Emonet Eﬃcient modeling simulation and of biological complexity with NFsim Nat Methods Spark Apache Spark project https Accessed Nov Stegle Teichmann Marioni Computational and analytical lenges in transcriptomics Nat Rev Genet Stelling Mathematical models in microbial systems biology Curr Opin biol Stone Gohara Shi OpenCL parallel programming standard for heterogeneous computing systems Comput Sci Eng Towards Human Cell Simulation Storn Price Diﬀerential simple and eﬃcient heuristic for global optimization over continuous spaces Glob Optim from reconstruction to model of human Swainston et Recon metabolism Metabolomics Szallasi Stelling Periwal System Modeling in Cellular Biology From Concepts to Nuts and Bolts The MIT Press Cambridge Talay Numerical Solution of Stochastic Diﬀerential Equations Taylor cis Milton Park Tanabe Fukunaga based parameter adaptation for ential evolution In IEEE Congress on Evolutionary Computation CEC pp IEEE Tanabe Fukunaga Improving the search performance of SHADE using linear population size reduction In IEEE Congress on Evolutionary putation CEC pp IEEE Tanabe Fukunaga How far are we from an optimal adaptive In International Conference on Parallel Problem Solving from Nature PPSN XIV IEEE accepted Tangherloni Nobile Besozzi Mauri Cazzaniga LASSIE simulating models of biochemical systems on GPUs BMC Bioinform Teijeiro Pardo Penas Banga Doallo enhanced diﬀerential evolution algorithm for parameter estimation problems in computational systems biology Clust Comput Trivedi SHARPE symbolic hierarchical automated reliability and performance evaluator In Proceedings International Conference on Dependable Systems and Networks June https Turunen Mathematics Behind Fuzzy Logic Heidelberg Turunen Using GUHA data mining method in analyzing road traﬃc accidents occurred in the years in Finland Data Sci Eng Vazquez Liu Zhou Oltvai Catabolic eﬃciency of aerobic colysis the Warburg eﬀect revisited BMC Syst Biol Viebke Pllana The potential of the Intel R Xeon Phi for supervised deep learning In IEEE International Conference on High Performance Computing and Communications pp August https Viktorin Pluhacek Senkerik Network based linear population size reduction in shade In International Conference on Intelligent Networking and Collaborative Systems INCoS pp IEEE Viktorin Senkerik Pluhacek Kadavy Towards better population sizing for diﬀerential evolution through active population analysis with complex network In Barolli Terzo eds CISIS AISC vol pp Springer Cham https Viktorin Senkerik Pluhacek Kadavy Zamuda Distance based parameter adaptation for based diﬀerential evolution Swarm Evol Comput https Accessed Nov Viktorin Senkerik Pluhacek Zamuda Steady success clusters in diﬀerential evolution In IEEE Symposium Series on Computational ligence SSCI pp IEEE Spolaor et Vitabile Conti Lanza Cusumano Sorbello Metabolic networks robustness theory simulations and results Interconnect Netw Vitabile Conti Lanza Cusumano Sorbello Topological mation ﬂux balance analysis and extreme pathways extraction for metabolic networks behaviour investigation In Workshop on Italian Neural Network vol pp IOS Press Vitello Alongi Conti Vitabile cognitive agent for autonomous urban vehicles routing optimization IEEE Trans Cogn Dev Syst Wiback Palsson Extreme pathway analysis of human red blood cell metabolism Biophys J Wienke Springer Terboven an Mey experiences with applications In Kaklamanis Papatheodorou Spirakis eds LNCS vol pp Springer Heidelberg https Wolkenhauer Why model Front Physiol Wu Shen Li Chen Lin Suganthan Ensemble of ential evolution variants Inf Sci Wynn Consul Merajver Schnell models in tems biology predictive and network analysis method Integr Biol Zamuda Brest Environmental framework to visualize emergent artiﬁcial forest ecosystems Inf Sci https Zamuda Brest Vectorized procedural models for animated trees struction using diﬀerential evolution Inf Sci Zamuda Brest control parameters randomization frequency and propagations in diﬀerential evolution Swarm Evol Comput Zamuda Sosa Diﬀerential evolution and underwater glider path planning applied to the opportunistic sampling of dynamic mesoscale ocean structures Appl Soft Comput Zamuda Nicolau Zarges discrete optimization marking pipeline survey taxonomy evaluation and ranking In ceedings of the Genetic and Evolutionary Computation Conference Companion GECCO pp Zamuda Sosa Adler Constrained diﬀerential evolution tion for underwater glider path planning in eddy sampling Appl Soft Comput Zamuda Sosa Success history applied to expert system for underwater glider path planning using diﬀerential evolution Expert Syst Appl Zhang Sanderson JADE adaptive diﬀerential evolution with optional external archive IEEE Trans Evol Comput Zhou Liepe Sheng Stumpf Barnes GPU accelerated chemical network simulation Bioinformatics Towards Human Cell Simulation Open Access This chapter is licensed under the terms of the Creative Commons Attribution International License http which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give appropriate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this chapter are included in the chapter s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the chapter s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder High Throughput Virtual Screening in Novel Drug Discovery Abdurrahman Aslı and Steffen Simla Laboratory of Molecular Modeling Evias Pharmaceutical R D Ankara Turkey aolgac Department of Pharmaceutical Chemistry Faculty of Pharmacy Gazi University Ankara Turkey Department of Pharmaceutical Chemistry Faculty of Pharmacy Marmara University Istanbul Turkey Department of Biochemistry Faculty of Pharmacy Gazi University Ankara Turkey csimla Institute for Biostatistics and Informatics in Medicine and Ageing Research IBIMA Rostock University Medical Center Rostock Germany Abstract Drug discovery and development requires the integration of multiple scientiﬁc and technological disciplines in chemistry biology and extensive use of information technology Computer Aided Drug Discovery CADD methods are being used in this work area with several different workﬂows Virtual screening VS is one of the most often applied CADD methods used in rational drug design which may be applied in early stages of drug discovery pipeline The increasing number of modular and scalable computational platforms can assist the needs in VS studies Such platforms are being developed to try to help researchers with various types of applications to prepare and guide the drug discovery and development pipeline They are designed to perform VS efﬁciently aimed to identify commercially available and compounds to be acquired and tested Chemical datasets can be built libraries can be analyzed and or VS studies can be formed with cloud technologies Such platforms could also be adapted to be included in different stages of the pharmaceutical R D process to rationalize the needs to repurpose drugs with various computational scalability options This chapter introduces basic concepts and tools by outlining the general workﬂows of VS and their integration to the cloud platforms This may be seed for further development of VS to be applied by drug hunters Keywords Drug discovery Virtual screening High performance computing Cloud computing The Author s Kołodziej and Eds cHiPSet LNCS pp https High Throughput Virtual Screening in Novel Drug Discovery Introduction Pharmaceutical drug discovery is and costly process spanning over to years and costing about billion US Dollars The process to identify new active pharmaceutical ingredients API starts with target identiﬁcation and dation steps and follows hit identiﬁcation lead discovery and lead optimization to acquire safe and effective new drug molecules at the preclinical stage Biological screening is used to identify possible target of hit molecule as developable candidate as the ﬁrst step in drug discovery Advances in systematic biological screening have generated automated parallel biological screening technologies called high throughput screening HTS Virtual screening VS is widely applied computational approach which is performed as hit identiﬁcation method in early stages of drug discovery pipeline VS protocols involve searching chemical libraries to identify hit compounds with putative afﬁnity for speciﬁc biological target enzyme or receptor for further development CADD methods in conjunction with VS studies emerged as valuable tools to speed up this long process and limit the cost expansion of R D Such studies demand strong combination of computational resources and skills biochemical understanding and medicinal motivation Effects of drugs in the human body are investigated with two main parameters namely pharmacokinetics and pharmacodynamics While pharmacokinetic studies investigate the fate of drug substances during absorption distribution metabolism and elimination ADME processes pharmacodynamics determines the required tration of drug to be delivered at the site of action and the biochemical and logical effect which may be responsible for the targeted biological response Fig Drug discovery pipeline presented together with some of the computational approaches which are used to rationalize the process Olğaç et CADD implementations deal with computational calculations of both cokinetics and pharmacodynamics parameters Therefore ADME and even toxicity Tox properties of given compound can be predicted with computational chemistry programs prior to any experimental studies Furthermore in silico approaches can also be applied to determine the putative interactions between ligand and receptor Fig In this chapter we brieﬂy address challenges and applications of biochemical and discovery and development approaches and their transformation in VS to be applied in cloud platforms Background Drug Discovery and Development Until the century the drug discovery and development process was based on the trial and error learning approach for diagnosis and curing the diseases The therapeutic effect was completely produced with natural products NPs These drugs were obtained from the whole or fraction of the NPs that contain the active pharmaceutical ingredient Natural compounds are an important source for drugs helped with improved sensitivity and better means for their biochemical separation Starting with the ﬁrst successful in vitro organic synthesis in laboratory by Wöhler it was clear that organic compounds could be produced out of the bodies of the living organisms However to synthesize chemically different organic compounds structural information had to be explained in more efﬁcient way In Kekulé proposed structural theories which successfully followed by different theories from the different scientists leading to the discoveries of new ﬁndings Research Paradigms in Classical Terms of Drug Discovery Mendelian Inheritance Gregor Mendel stated that at least one dominant or two recessive gene pair are required for mutations causing properties diseases Fig Fig Mendelian genetics presumed phenotypic traits to depend on single genetic variation Magic Bullet The term of magic bullet was created by Paul Ehrlich who started the ﬁrst systematic pharmaceutical screening and won the Nobel Prize in Physiology or Medicine in found out that chemical compounds can directly be delivered and bound only to its biological target called chemoreceptors In light of this information biological targets are determined to play role in the ﬁrst step of the drug discovery depending on the developing technologies and knowledge Fig High Throughput Virtual Screening in Novel Drug Discovery Fig Summary of the biological understanding after the magic bullet approach Research Paradigms in Modern Terms of Drug Discovery Polygenicity It been shown that many common diseases can occur as result of mutations on multiple genes such as diabetes asthma and heart disease Such diseases may develop as result of an interplay of genetical inheritance mutations on multiple genes or contribution of environmental factors Fig Fig Common diseases are polygenic and this is one of the biggest challenges in the ﬁeld Magic Shotgun The term of magic shotgun emerged as result of the detection of secondary effect properties such as adverse effects associated with the ability of the drug to affect multiple targets This concept is related to the development of drugs with high selectivity and to predict the potential adverse effects prior to clinical trials Fig Fig Single compound may affect multiple targets Such may be positive or undesired Olğaç et Several in silico approaches have been developed to predict and to repurpose drugs that are already on the market Individualized Medicine The genetic polymorphisms of drug targets metabolizing enzymes or transporters may explain differences in the molecular pathophysiology of patients even of those who are assigned with the same diagnosis The aim is to ﬁnd drugs to compensate genetic deﬁciencies to ﬁght the disease at its roots As of today with the transcriptome and the genetic parameters obtained from patient s tissue or blood one can be informed about their contributions to disorders For instance pharmacogenetics investigates how genetic variation affects the binding site of drug That may suggest higher dosage because of disturbed molecular recognition Target Fishing It can be regarded as the inverse screening wherein the ligand is proﬁled against wide array of biological targets to elucidate its molecular mechanism of action by experimental or computational means Drug Repositioning An approach to identify new medicinal applications for approved drugs to treat other diseases because the drugs may bind to other receptors Polypharmacology Special ligand design approach to exert an effect on multiple biological targets Molecular Recognition Theories Key and Lock In Emil Fischer proposed the model of key and lock for the molecules bearing potential effects on biological systems According to this model it is assumed that ligand binds to the active site of its target which behaves like key that ﬁts its lock Fig Fig Key and lock model Induced Fit In it was proposed by Daniel Koshland to consider the mational changes during the interaction between the ligand and its biological target which wasn t considered in the theory of Fischer In this model binding is occurred by small but expected conformational ges during the interaction between the ligand and the biological target Later this speciﬁc conformation of the ligand been referenced as the bioactive mation Fig High Throughput Virtual Screening in Novel Drug Discovery Fig Induced ﬁt model Thermodynamics in Binding During the binding process of ligands to their biological targets it is known that conformational arrangements and desolvation occur with the help of speciﬁc physicochemical interactions Fig Fig Desolvation effect and thermodynamics in binding The most important strategy in the process of developing novel drug candidates is to increase the afﬁnity of binding of the ligand to its target The energy change after the binding process is quantitatively expressed by logarithm of Kd or Ki values These values are related to Gibbs free energy of binding DGbinding RTlnKd RTlnKi In this equation R is the gas constant T is the absolute temperature Kd is the equilibrium constant and Ki is the inhibitory constant that exist in the equation In thermodynamic binding process binding energy change is occurred and two thermodynamic quantities the enthalpy change DH and entropy change determine the sign and magnitude of the binding free energy DGbinding DHbinding TDSbinding Olğaç et Chemical Space In Regine Bohacek generated predictions about possible chemical compound types that might be chemically accessed Her estimation was pointing to chemical compounds making up chemical space that was virtually identiﬁed by using carbon oxygen or nitrogen atoms and by considering linear molecules with up to atoms While making the predictions Bohacek regarded chemical groups to be stable and chemical branches ring structures and stereochemical possibilities were taken into account In later studies the limits of the chemical space was drawn to be between according to the results of the analyses by different methods and descriptors Although there are many reports about this range to be accepted as Bohacek deﬁned But it is expected that this number will continuously increase by the discovery of new chemical skeletons Additionally the number of organic compounds accessed experimentally is according to CAS and Beilstein databases which contain records obtained from the scientiﬁc papers those have been published by the scientiﬁc munity since Rational Drug Design The increasing scientiﬁc knowledge for novel drug discovery opened new horizons and generated useful technological developments for the researchers in the ﬁeld When these new tools are wisely brought together with recent knowledge they would provide many advantages in drug design and development studies Moreover the available theoretical and experimental knowledge about drug safety and the idea of being appropriate for human use generate extra difﬁculties for drug design and development It is known that not all candidate molecules with high potency can reach to drug status due to several reasons such as inefﬁcient systemic exposure unwanted side effects and effects Also drug may not be right for every patient due to the genetic variations and binding This also effects drugs that are already on the market Table However molecular reasoning may give second chances for drugs that once failed in late clinical studies at great expense or that have been retracted from clinical use With an improved molecular understanding and with hindsight from the now feasible pharmacogenetics and these compounds have chance to ﬁnd their niche for reentry Conventional drug design and development approaches are associated with high afﬁnity binding of ligand to its target In rational drug discovery studies evaluating the ligands by only approach lost its validity and assessing their binding to the target related diseases is insufﬁcient By taking control of some parameters like binding and bioavailability properties of the molecules that have appropriate binding properties should also be discovered for the drug candidates Therefore revising the classical strategies with rational approaches become substantial This process is called reverse pharmacology or drug discovery In this recent approach the questions about Which chemotypes will be worked on and Why should also be considered during drug High Throughput Virtual Screening in Novel Drug Discovery design and development studies before directly evaluating the target binding properties of the molecules Table Deﬁnition of the key terms related with informatic approaches used in early stage drug discovery Bioinformatics The discipline responsible for biological data recording and interpretation using information technologies Some of its applications on drug discovery include the tection of protein binding pocket prediction of interactions occurrence of mutations analysis of the biological sequences of macromolecules similarity searches and fingerprint matches estimation of structures of biological macromolecules Cheminformatics The discipline which accumulates and processes the chemistry related data using information technologies Some of its applications on drug discovery include construction of the and structures storing and searching the chemical data building chemical compound databases and datasets QSAR QSPR estimation of erties It can also be used to map the chemical space of compound libraries Pharmacoinformatics The discipline which combines the cheminformatics and ics tools for processes Computer Aided Drug Design CADD Development of mathematical formulas to calculate the potential and kinetic energies of biomolecular systems made possible the implementation of such complex culations with computers CADD is applicable for hit molecule discovery for new different chemotypes and for designing new derivatives CADD processes may be divided into molecular mechanical methods and quantum mechanical methods In both techniques the results are obtained through calculations Molecular mechanics deals with the calculations at the molecular level that can be performed on an atomic basis while quantum mechanics involves electron related complex calculations performed at the quantum level During existence of the obscurity in drug discovery studies it is hard to reach the desired target But the physicochemical parameter as factor can be useful about this topic by measuring the ADME properties Also the should be bound to its target with high afﬁnity In relation to that drug design processes are carried out within the framework of selected strategies with the acquisition of dimensional bioactive conformation of the ligands CADD is used for identifying and designing biologically active compounds and this ﬁeld can be synergistically integrated with all other medicinal chemistry related ﬁelds like pharmacology biology and chemistry Drug Discovery It is possible to store the chemical and biological information in special databases to preserve the molecular features and their biological effects obtained from series of Olğaç et assays If there is molecular level structural data about the biological target viously obtained data can be used in CADD studies to ﬁnd and analyze the trend between the known compounds and their activity retrospectively then to design and discover new chemical entities prospectively main workﬂow of studies addresses the generation of model by training on series of compounds and subsequent testing stage of the model with test series of compounds Later the generated model s can be validated with an external series of compounds which were not used during model generation Then the model can be used to virtually screen chemical databases within the applicability domain of the model Fig Fig Basic workﬂow of modeling applications Quantitative Structure Relationships In an approach about the relationship between chemical structure and biological activity was proposed by Cros After that in and Fraser evaluated this correlation in molecular level and described following equation to computationally predict the biological response U to be identiﬁed as the function of the chemical compound C U f ðCÞ In the course of the history many researchers have conducted studies to relate physicochemical properties and biological effects of compounds with different approaches Through these studies the necessity to consider molecular substitutions emerged to try to explain biological effect or physicochemical property of chemical structure In Louis Hammett compared ionization rates of benzene derivatives substituted in various positions The ﬁrst quantitative parameter was determined as sigma r the potential electronic contribution value which is deﬁned by calculating electronic substituent constant values This was the ﬁrst identiﬁed quantitative parameter Then the ﬁrst steric parameter for ester hydrolysis was determined by Taft as constant Many other parameters have been continued to be developed for being used in QSAR studies Later parametric formula was presented by Corwin Hansch that brought these parameters together The formula was designed to calculate minimal concentration needed to mathematically formulate the biological activity as logarithm of concentration and was measured with several independent factors in different cases such as partition cient log P aromatic substituent constant p electronic substituent constant r High Throughput Virtual Screening in Novel Drug Discovery steric parameter Mixed Approach Based on Hansch and Analysis and Kubinyi Bilinear Analysis Method are some of the ﬁrst generated models used in QSAR analysis When all the steps are evaluated it can be observed that QSAR different applications Structural activity or physicochemical property studies performed with one independent variable are named calculations and the studies with multivariate equations are called In such equations log P and Hammett constant can be used as independent variables Studies that take into account the molecular descriptors and ﬁngerprints containing information about structural and bonding characteristics resulting from the molecular presentation are called if extra structural information chirality is included in studies these studies are named as QSAR The efforts to explain the binding orientations of the ligands have emerged QSAR together with the help of the developments in molecular understanding through the experimental methods methods and improved molecular visualization technologies Implementation of this approach is accomplished by forming equally divided grids in the coordinate system and the surface interaction areas of the chemical compounds are determined by the following analysis methods Table Table Summary of some of the approaches Comparative Molecular Field Analysis CoMFA CoMFA is subtype of the study which includes steric and electronic fields and Coulomb potentials generated on the bioactive conformation of the compounds in the training set In CoMFA studies it is only possible to analyze the effects of enthalpic properties on binding Comparative Molecular Similarity Index Analysis CoMSIA To explain the structure ty relationships by considering major contributions obtained by steric electrostatic phobic and hydrogen bond donors HBD or hydrogen bond acceptors HBA properties are added into CoMFA and this generated CoMSIA analysis QSAR In this study model which is carried out using combination of and approaches bioactive conformation of the ligands is tained by methods Afterwards studies are conducted through this bioactive conformation Pharmacophore Modeling pharmacophore aggregates functional groups of ical compounds which are responsible for the biological response and which exhibit appropriate interaction with biological target The term pharmacophore modeling refers to the identiﬁcation and display of important pharmacophore groups to illustrate the basic interactions between the ligand and the receptor Pharmacophore modeling is generally applied to determine common structural features within series of similar or diverse molecules by subtracting maps Once determined the generated cophore hypotheses may be used to virtually screen and to predict the biological activity Olğaç et of other molecules The model is generally obtained from the information belonging to the ligands However it is also possible to generate pharmacophore model from the receptor itself or with combined approach as well After the formation of possible bioactive conformers of the compounds cophore model can be generated in by aligning the structures and mapping the binding properties More than one pharmacophore hypothesis can be generated and the most suitable one s can be identiﬁed by enrichment factor within their applicability domain While generating the model s the optimum volume of each pharmacophore property takes the major interest The aim of pharmacophore modeling is to determine the optimum volume for the identiﬁed properties If identiﬁed volumes are larger than required the selectivity of active compounds by this model decreases and active and inactive compounds can be found together by using this model Conversely if the ﬁelds are smaller than they need to be the active compounds can not be identiﬁed by pharmacophore screening While creating pharmacophore model HBA and HBD features hydrophobic aliphatic or aromatic properties and charges can be used Besides it is possible to identify the regions or features without any speciﬁcity Most pharmacophore modeling programs create these properties according to mized interactions The ideas behind pharmacophores are also applied to specify new compounds that aggregate as many pharmacophores from series of in silico ligand docking experiments or after ﬁrst in vitro validation The assembly of new compounds can be iterative growing from existing binders or starting novo design of novel drugs Machine Learning ML Relations between structural properties and biochemical effects are found by statistical models With the advent of computational chemistry and advances in structural biology an increasing number of features may be identiﬁed in silico for any given ligand and receptor which may affect virtual screening The integration of all these data sources and tools allows for the formulation of many models ML covers the development of models or rules and their ation The use of ML techniques for drug discovery been increasing recently both in and studies to ﬁnd rules from set of molecular tures and to predict speciﬁc property Estimation of properties by the use of physicochemical descriptors generation of hit or lead molecules with the studies on prediction of biological activity development of homology models mination of bioactive conformation by the help of docking studies or pharmacophore modeling are some of the examples of ML applications in drug discovery ML can be applied as regression or classiﬁcation models In the regression model quantitative models are formed automatically Statistically the most appropriate model is selected from the generated ones Classiﬁers utilize such models to cluster the data The learning process is carried out by known characteristics of the molecules to predict their In the classiﬁcation model the branches are formed on classiﬁcation tree Biological and chemical data are placed on the leaves of the High Throughput Virtual Screening in Novel Drug Discovery branches of the tree It can be generated and used for various statistical making scenarios Artiﬁcial neural networks support vector machines decision trees and random forest techniques are some of the most applied ML techniques used in drug discovery studies Drug Design Experimental studies on elucidation of the structures of biological macromolecules are generally performed by crystallography and NMR methods Fig Obtained structural information about the macromolecules is stored in regional protein databases There are three major regional databases Protein Data Bank PDB for storing the crystal structures of these macromolecules such as RCSB PDB USA PDBe Europe PDBj Japan Likewise the structures of relatively small biological macromolecules obtained by NMR techniques are stored in the Biological Magnetic Resonance Data Bank BMRB Records stored in these databases are synchronized via the Worldwide PDB wwPDB organization Fig Obtaining the protein structure Homology Modeling Homology modeling techniques are used to predict sentations of biomolecular structures The model generation is done with the sequence of monomers nucleotides or amino acids The applied algorithm transfers spatial structures of other phylogenetically arrangements biological structures from crystal Protein sequence information is stored in reference databases such as UniProt where the information on the protein sequence and its functions are collected in one place The UniProt database offers other web services such as BLAST and Clustal Omega where sequence similarity and alignment can be retrieved sensus annotation across protein families across species is aggregated in PFAM with functional annotation Fig Olğaç et Fig Basic workﬂow for homology model generation interactions between ligands and their biological Docking simulation method which is used to predict the binding targets modes and molecular Table This technique is based on the thermodynamic calculations by following the determination of the possible binding orientation of the ligands and the proteins with appropriate interactions in the active site The docking scores are determined based on these interactions and conformational rearrangement costs which help evaluating the generated results Fig Table Docking concepts and their limitations Technology Rigid Docking Receptor structure is treated as rigid body and the ligands are prepared with conformational sample s then ﬁt into the active site of the protein Docking Flexibility of the active site residues is considered for the protein and these ﬂexible active site residues ﬂexibly adapt to accommodate the ligand Covalent Docking In this method binding region of the receptor that ligands bind covalently is identiﬁed and the docking procedure is performed in this speciﬁc condition Peptide Docking Peptide docking method is used to determine binding modes of peptide structures in the active site of its biological target Docking It is the general name of the docking method that is used to predict or interactions those are taking place biologically Reverse Docking It can be applied for target ﬁshing the drugs or active substances or to become aware of Limitations The overall protein structure and its active site residues are ﬂexible this affects the binding orientations of the ligands in docking results It is computationally expensive approach that requires careful arrangement of the docking parameters and cuts off the values prior to docking simulation This method may generate chemically wrong binding pattern and incorrect binding pose but lowers the computational search costs of docking Peptides or fractions of proteins are large and ﬂexible ligands and hence difﬁcult to parameterize and computationally expensive compared to the small molecules This is the most computationally demanding approach to simulate the interactions due to the size and complexity of the macromolecules Conceptionally this approach requires catalog of conﬁgurations for screening many targets by docking High Throughput Virtual Screening in Novel Drug Discovery Fig Basic workﬂow for docking simulations Molecular Dynamics MD MD is technique to simulate the dynamics of nected atoms in joint molecular system of ligands and their biological targets by considering environmental factors temperature solvents ions membrane pressure temperature The simulation may be run within speciﬁed time interval all depending on the ambient conditions set by the The most preferred binding orientation of the conformational samples that emerge during the simulation can be obtained by analysing the energy of the whole or portion of the simulation and by visualizing and analyzing the interactions Fig Fig General workﬂow for MD simulations Application of Virtual Screening VS is chemical search to match ligands with their biological targets The process can be applied by either or approaches Related to the ciples of the method used to ﬁnd hit molecule VS covers various concepts Regardless of the CADD approach the process starts with chemical database to be prepared for each molecular entry This can be followed with in silico predictions of the pharmacokinetic and pharmacodynamic proﬁles of the molecules and ﬁltering of molecules within the database to obtain statistically signiﬁcant and manageable clusters of the chemical space Fig Olğaç et Fig General VS workﬂow Accessing Compound Databases Chemical compound databases represent signiﬁcant chemical catalogues for the covery of hit molecules with biological activity which compile large numbers of chemically different entities from either synthetic or natural origin They can be generated with data combination of compound databases previously tested or untested screening compound series or in silico synthesis with building blocks Compound databases of curated biological activity data commercial sources of the molecules and pharmaceutical reference databases of the marketed drugs can be seen in Table High Throughput Virtual Screening in Novel Drug Discovery BindingDB chEMBL ChemSpider www DrugBank www DrugPort eMolecules Molport MMsINC Open PHACTS PubChem ZINC Table Compound databases Comment Around million binding afﬁnity data points are available which are obtained from patents and scientiﬁc articles More than million biological activity results are available curated from scientiﬁc papers and patents Contains over million structural data obtained from different data sources Database of drug entries linked to their biological targets Structural representation of drugs with their receptors with data from DrugBank and PDB Database comprises over million screening compounds and million building blocks which are ready to order Comprising around million ready to be synthetized molecules over million screening compounds and around million building blocks which are ready to order Helps to search subsets of the database containing over million unique compounds with tautomeric and ionic states at physiological conditions as well as possible stable conformer for each molecular entry Large of data integration for compounds their receptors and pathways Contains around million compounds and million bioactivity data records Contains around million purchasable molecules in and million purchasable molecules in Preparing Ligands for Virtual Screening It is important to assign the correct stereoisomeric or tautomeric forms and protonation states of each ligand at speciﬁed pH to avoid changing physicochemical and formational behavior of the molecules Parameterization during this stage must be done very carefully for VS study because the molecules are generally stored in or within the databases For example chiral entry within the database may be in single enantiomer or racemic mixture or molecule may not be neutral as it is stored in the database and may be at different protonation states at different pH values Conformational ﬂexibility of ligands is also important and computationally more expensive Every additional rotation increases the number of conformations that needs to be generated and this results with computationally more expensive VS process Olğaç et Compound Filtering The main idea of compound ﬁltering is to exclude the molecules which are not carrying suitable pharmacokinetic or pharmacodynamic properties The aim is to prevent sible risks in preclinical or clinical phases of the studies This also helps to keep down computational costs in VS studies Clustering by Chemical Diversity It is possible to estimate the chemical similarities of the compounds by calculations This lets identiﬁcation of similar or diverse subsets of the libraries Many different techniques are used for this purpose The basic approach is to use similarity algorithms based on mathematical parameters obtained from chemical descriptors Such calculations are generally done by calculating molecular ﬁngerprints which are way of encoding the structure of molecule usually as binary digits used to symbolize the presence or absence of chemical substructure within the molecule allowing for similarity search in chemical database The most common similarity and classiﬁcation algorithm is moto s similarity estimation and classiﬁcation practice that regulate mathematical classiﬁcation studies Filtering with Physicochemical Parameters One of the major examples of ﬁltering by pharmacokinetics related approaches is based on the scientiﬁc analysis of the drugs Most drugs in the market can be used orally and it is found that there is generally applicable rule to determine whether biologically active compound chemical properties that would make it an orally administrable drug in humans In it is formulated by Christopher Lipinski and called Lipinski s rule of ﬁve It is designed to evaluate of compound to be used in drug discovery and development studies The properties of depend on molecular weight less than Dalton an partition coefﬁcient log P less than equal or less than HBD groups equal or less than HBA Drug candidates which have properties of the tend to have more success in clinical trials so have more chance to reach to the market Such approaches let the identiﬁcation of chemical space to fall within the biologically relevant subspace with better macokinetic proﬁle of vast unbounded chemical space Filtering with or Screening of Chemical ies approaches focus on model generation with binding afﬁnity data of small molecules against biological targets These approaches are used with calculated descriptors to predict the characteristics within the molecular database Though approaches don t necessarily rely on existing data and try to place the molecules in the binding sites of the target and evaluate their potential afﬁnity by binding scores within the complex biomolecular structures Such computationally expensive calculations are preferred to run on distributed compute platforms to speed up the in silico screening process Ties with Research The search for new biologically relevant compounds is not possible without laboratory result in which theories are conﬁrmed reﬁned or disproved Considerable time and effort are needed to be invested into the development of such tests Eventually it is needed to transfer the ﬁndings to human tissues High Throughput Virtual Screening in Novel Drug Discovery VS may support the selection of the ligands for testing once drug target is tiﬁed This process is long and demands deep insights into the molecular understanding of the pathology of disease The selection of target is likely to be supported from genetic studies an observed association of DNA variation with disease phenotypes hints the gene to target experiments are performed in the laboratories to conﬁrm the molecular mechanism that trigger the disease Many researchers on rare diseases do not have industrial partners and drugs may be marketed for such orphan diseases There are many rare diseases and quentially many patients To ﬁnd new drugs the availability of in silico approaches may be signiﬁcant guidance for the researchers for repositioning drugs already on the market The same approach can be applied to identify the biological targets of traditional medicines obtained from NPs Ties with Individualized Medicine Individualized medicine identiﬁes genetic tions that are causative for disorder for every patient and adjusts therapy accordingly Once compound binds to its receptor this yields detailed molecular understanding how the drug works Such insights are of interest whenever genetic variation may affect the binding of drug to its receptor Some drugs address cells with high mutation rate like virus or tumor cell Mutations appear at random but when the beneﬁts from it because of lower binding afﬁnity to the drug it will continue to divide and pass that genetic information on Thus the pathogen or tumor evades the therapy For that reason The FightAIDS Home project searched for compounds with strong binding afﬁnities to many sequence variations of the viral protein target Today cancer patients are monitored by DNA sequencing to learn the changes in relative frequencies of genetic variations of the The adoption of sequencing technologies is recent development It will indicate the frequency of escape routes that may be closed with new drugs for the same target possibly in the same binding pocket It is with the same modeling technology that is at the root of in silico screening that this data can be interpreted Without molecular interpretation clinicians would need to wait for insights from observations of how nuclear variation and subsequent the clinical outcome molecular reasoning in contrast is possible for every patient individually therapy decisions affect Automation Several tools are available to prepare receptors and ligands Glue guage can be used to integrate different processes of such calculations In addition there are workﬂow environments for CADD in particular like OPAL or KNIME The users can easily develop any integration of CADD workﬂow that needs to be automated Recent works integrate formal descriptions in databases like for execution of the tools Experimental Evaluation and Validation For biological research in silico produced ﬁndings are needed to be evaluated with in vitro or in vivo biological assays It is preferable to know the statistical strength of the model to evaluate the functionality of the model with retrospective studies prior to biological testing That test is likely to be performed in several stages and can be Olğaç et exempliﬁed as given below It should be noted that this stage of the work strictly depends on the nature of the target Test of binding afﬁnities calorimetry SPR b c cell line tissue sample d Effect on disease models animal trials Induced structural changes NMR These tests are expensive may cost the lives of animals If the wrong compounds are chosen for testing then this delays the treatment of humans Testing diverse sets of compounds is preferred with respect to their binding mode and chemical structure Infrastructures and Computational Needs Virtual screening is data parallel process The throughput of ligands tested in the computer almost linearly scales with the number of processors contributing second concern is the number of applications contributing to the screening itself The availability of computing HPC infrastructure is needed to speed up the in silico screening studies In addition accessing stronger HPC facilities direct impact on covering bigger chemical and conformational space of compounds in VS studies or testing higher numbers of druggable targets to identify biomolecular targets of the compounds in target ﬁshing studies When it is applied on thousands or millions of series of chemical compounds the total VS simulation time exceeds the limits of single workstations For example molecular VS run covering chemical space of millions of compounds may take years of computation time on single workstation The overall compute time is ﬁxed also with multiple machines contributing But by distributing the load the effective time can be lowered The same VS run can be taken to scale by accessing hundreds of central processing unit CPU cores with an small or medium scale HPC infrastructure or by accessing thousands of CPU cores with supercomputers or cloud resources Recent technical advancements to employ graphical processing unit GPU for parallel computing have fostered the concept to apply on data deep learning or molecular dynamics We here discuss alternative sources for computational infrastructures and single out unique properties of the cloud environments Local Compute Cluster Many computers are integrated in local network and combined with grid software to distribute compute jobs The concept of batch system is familiar to IT specialists who maintain such hardware Larger clusters with special hardware to allow for fast are referred to as HPC environments The VS is mostly data parallel computation with individual compute nodes which do not need to communicate with each other during the computations High Throughput Virtual Screening in Novel Drug Discovery Volunteer Computing Volunteer computing refers to the concept of having program running in the ground addressing scientiﬁc problem If several thousand individuals contribute then one compute power or storage resource that can compete with the large putational clusters It should be noted that one needs to interact with the community to keep the momentum Several in silico screening runs have been performed voluntarily One of the most was the search for an HIV protease inhibitor in FightAIDS Home with the underlying BOINC technology Because of the enormous compute power acquired by devices that are not operated by humans the decentralization of computing is strategically important route Tapping into these resources for computing and to the periphery is called as edge generally support computing the interaction of devices at Cloud Computing Cloud technologies are means to integrate different workﬂows and tools on dynamically allocated resources These instances share respective services in their speciﬁcation of CADD studies and cloud computing is an emerging solution for VS needs The infancy of cloud solutions was in IaaS dynamically conﬁgured and started machines that were paid on use Today IaaS services are provided by many compute centers at very competitive price levels Setups are easy to use for individuals familiar with remote access with command line interface CLI or graphical user interfaces GUI Researchers in both academia and industry are likely to have access to local compute cluster But this is costly when the expenses of hardware upgrades electricity and maintenance are added Remote sources are highly competitive in pricing This is meant for individuals who could use local resources if they had them There is technology allowing these remote resources to behave much like an extension of local cluster The most known cloud service providers are Amazon Web Services and Google Cloud but there are increasing numbers of open source cloud solutions that may be of interest The OpenStack middleware evolved into facto standard to set up public or private cloud environment When software is accessed via web interface the implementation becomes hidden look to the side how payments are performed online with an interplay of many service providers tell what clouds also mean An interplay of services that scale Anticipating variants of VS principles to be employed in different and barely predictable ways are outlined in the next section One may expect layer of molecular modeling and genomics to emerge But there are as Service yet Table Olğaç et Table Classiﬁcation of cloud services IaaS Infrastructure as Service is service model which to provide remote access on mand to compute environments Besides the typical virtualized setups with multiple users sharing single processor for HPC also bare metal setups are available CADD tools may be readily usable deployed as cloud image or be installed posteriori to running image PaaS Platform as Service is technical concept for an aggregation of services to facilitate software running in support of IaaS to scale in environments SaaS Software as Service runs remotely and stars copies of itself on multiple nodes in an IaaS environment to scale to whatever size of problem or to as many users as there may be The software is typically presented as web interface but the concept found broad adoption from business applications to the gaming industry FaaS Function as Service Element of SaaS that may be invoked without context fered by all major cloud vendors an open source implementation is Open Whisk This technology offers route for services to integrate into larger workflows Solutions Recent reviews and the catalog give vivid expression on describing many different sources for many partial solutions available as automated web services or as instances that may be started at one s independent disposal How exactly transfer of information between these tools should be formed or how these tools should be run to generate redundant ﬁndings with then higher conﬁdence all this is yet to be clariﬁed both semantically and technically In silico approaches have been integrated in drug discovery pipeline and big pharmaceutical companies are likely to perform these often Cloud environments are used for their advantages to reduce setup costs for professionals Also for creative new approaches in tackling disease the information on which selected target is already crucial not to lose competitive advantage This may not be acceptable to be computed in extra mural facilities However for common targets the interception of individual VS results from volunteer computing project may not be considered critical Table shows remote services that may use cloud services internally or that are offered for integration with workﬂow Most services are agnostic with respect to the source of request However some may fall back to the cloud provider s infrastructure for monetary compensation Thus the cloud becomes an integral part of the VS tool s deployment High Throughput Virtual Screening in Novel Drug Discovery Table solutions for virtual screening and target ﬁshing The table lists services that describe features of ligands or their receptors and services Services integrating multiple tools are tagged as workﬂow F stands for Feature R for L for and W for Workﬂows Service Properties F R L W X X URL Comment X Collaboration environment for Molecular docking and VS AceCloud X Achilles X X achilles BindScope X X X X DINC WebServer DOCK Blaster DockingServer Evias Cloud Virtual Screening Tool HDOCK HADDOCK Web Server idock iScreen LigandScout Remote X X X X X X X X BindScope X idock X X X X X mCule X X X X researchers to exchange opinions on interactions Cloud image to run MD simulations with CLI on Amazon Cloud can be used for MD Blind docking server with web interface binding prediction tool web service for large ligands Automated molecular based VS web service service Integrated with chemical library management system as scalable HPC platform for VS service as web service and nucleic acid docking server biomolecular structure docking service Flexible VS tool with web interface web server which started with docking of traditional Chinese medicine desktop application letting access to the HPC for VS studies Integrates VS tools with purchasable chemical space continued Table continued URL Comment X MTiOpenScreen web server for predicting binding site and generating docking calculations Integration of AutoDock and MTiOpenScreen in bioinformatics Mobyle environment Fully automated rigid docking server based on Monte Carlo search technique for generating docking simulations with geometry and shape complementarity principles web server letting target prediction for the ligands PatchDock X X X X Olğaç et Service MEDock Properties F R L W X X MTiOpenScreen ParDOCK PatchDock Polypharmacology Browser ProBiS CHARMMing X X X X X SwissDock X X X analysis tool for binding site identiﬁcation In addition to ProBis it is possible to do energy minimization on complexes platform that allows generation and validation of models docking service to predict binding VS web server using shape recognition techniques ZincPharmer X X Online VS tool for screening the purchasable subset of the ZINC or Molport databases Conclusions The goal of VS is to identify novel hit molecules within the vast chemical space to ameliorate symptoms of disease Hereto the compound interacts with target protein and changes its action in pathological condition None of the in silico molecular modeling techniques can generate perfect models for all kinds of biochemical cesses However large variety of tools is provided by the scientiﬁc community High Throughput Virtual Screening in Novel Drug Discovery Alone or in combination there are available VS technologies which suite the problem at hand With clouds as HPC resources complete workﬂows have been established to directly address the needs of medicinal chemists The same technology also supports collaborative efforts with computational chemists to adjust workﬂows to emerging preclinical demands Another problem to address is the demand for HPC facilities for VS projects These are indirect costs when using workﬂows already prepared as cloud service Clouds are attractive both for scaling with computational demands and for the enormous diversity of tools one can integrate To minimize the costs and to complete the putation as quickly as possible workﬂows should be well considered to select which tool to use while performing VS studies VS projects demand the investment of considerable time prior to the computation for their preparation and afterwards for in vitro analyses For any isolated project it is likely to be more cost effective to share the compute facilities and expertise by using solutions The authors of this text hope to have helped with an initial team building for interdisciplinary projects There is lot of good work to be done algorithmically or on data at hand but few groups can develop and use these techniques all on their own Acknowledgement This work was partially supported by the Scientiﬁc and Technological Research Council of Turkey Technology and Innovation Funding Programmes Directorate Grant and Academic Research Funding Program Directorate Grant and EU ﬁnancial support received through the cHiPSet COST Action References Food and Drug Administration FDA The Drug Development Process https Accessed Dec Kopp Deﬁnition of active pharmaceutical ingredient revised pp Hughes Rees Kalindjian Philpott Principles of early drug discovery Br Pharmacol Cronk screening In Drug Discovery and Development pp Elsevier Amsterdam Introduction to pharmacokinetics and pharmacodynamics In Concepts in Clinical Pharmacokinetics pp ASHP Olğaç Orhan Banoglu The potential role of in silico approaches to identify novel bioactive molecules from natural resources Future Med Chem Kinghorn Pan Fletcher Chai The relevance of higher plants in lead compound discovery Nat Prod Taylor Successful but often unconventional the continued and contribution of natural products to healthcare Wöhler Ueber künstliche Bildung des Harnstoffs Ann Phys Hafner Gmewue Angewandte Chemie Hoffmann Natural product synthesis changes over time Angewandte Chemie Int Ed Mendel Versuche über Plﬂanzenhybriden des naturforschenden Vereines Brünn Bd IV für das Jahr Abhandlungen pp Olğaç et Kaufmann Paul Ehrlich founder of chemotherapy Nat Rev Drug Discov Strebhardt Ullrich Paul Ehrlich s magic bullet concept years of progress Nat Rev Cancer Burghes Vaessin Chapelle The land between mendelian and multifactorial inheritance Science Roth Sheggler Kroeze Magic shotguns versus magic bullets selectively drugs for mood disorders and schizophrenia Nat Rev Drug Discov Ma Lu Pharmacogenetics pharmacogenomics and individualized medicine Pharmacol Rev Cariaso Lennon SNPedia wiki supporting personal genome annotation interpretation and analysis Nucleic Acids Res Fischer Ueber die optischen Isomeren des Traubenzuckers der Gluconsaure und der Zuckersaure Berichte der Dtsch Chem Gesellschaft Fischer Einﬂuss der Conﬁguration auf die Wirkung der Enzyme Berichte der Dtsch Chem Gesellschaft Ferenczy Keseru Thermodynamics guided lead discovery and optimization Drug Discov Today Bohacek McMartin Guida The art and practice of drug design molecular modeling perspective Med Res Rev Kirkpatrick New horizons in chemical space Nat Rev Drug Discov Kirkpatrick Ellis Chemical space Nature Dobson Chemical space and biology Nature Lipinski Hopkins Navigating chemical space for biology and medicine Nature Chemical Abstracts Service CAS CAS Databases https databases Heller The Beilstein online database In ACS Symposium Series vol pp Huggins Sherman Tidor Rational approaches to improve selectivity in drug design Med Chem Oprea Davis Teague Leeson Is there difference between leads and drugs historical perspective Chem Inf Comput Sci Villoutreix Lagorce Labbe Sperandio Miteva One hundred thousand mouse clicks down the road selected online resources supporting drug discovery collected over decade Drug Discov Today Jelliffe Tahani Pharmacoinformatics equations for serum drug assay error patterns implications for therapeutic drug monitoring and dosage In Proceedings of the Annual Symposium on Computer Application in Medical Care pp Olgaç Carotti Pharmacoinformatics in drug R D process In GPSS Levitt The birth of computational structural biology Nat Struct Biol Hopkins Groom Alex Ligand efﬁciency useful metric for lead selection Drug Discov Today Guedes Serra Salvador Guedes Computational approaches for the discovery of human proteasome inhibitors an overview Molecules Petterson Balle Liljefors Ligand based drug design In Textbook of Drug Design and Discovery pp Yalcin Kantitatif İlişkileri Analizleri QSAR Ankara Üniversitesi Eczacılık Fakültesi Yayınları High Throughput Virtual Screening in Novel Drug Discovery Fraser On the connection between chemical constitution and physiological action Part II on the physiological action of the ammonium bases derived from Atropia and Conia Trans Soc Edinburgh Hammett Effect of structure upon the reactions of organic compounds Benzene derivatives J Am Chem Soc Taft Steric Effects in Organic Chemistry Free Wilson mathematical contribution to studies Med Chem Fujita Ban study of phenethylamines as substrates of biosynthetic enzymes of sympathetic transmitters Med Chem Kubinyi Quantitative relationships mixed approach based on Hansch and analysis Med Chem Kubinyi Quantitative relationships The bilinear model new model for nonlinear dependence of biological activity on hydrophobic character Med Chem Polanski Receptor dependent multidimensional QSAR for modeling drug receptor interactions Curr Med Chem Sippl Applications recent advances and limitations In Recent Advances in QSAR Studies pp Klebe Abraham Mietzner Molecular similarity indices in comparative analysis CoMSIA of drug molecules to correlate and predict their biological activity Med Chem Lima Philot Trossini Scott Maltarollo Honorio Use of machine learning approaches for novel drug discovery Expert Opin Drug Discov van der Kamp Shaw Woods Mulholland Biomolecular simulation and modelling status progress and prospects Soc Interface Suppl Berman et The Protein Data Bank Nucleic Acids Res Rose et The RCSB Protein Data Bank views of structural biology for basic and applied research and education Nucleic Acids Res RCSB Protein Data Bank http Accessed Dec Berman Henrick Nakamura Markley The worldwide Protein Data Bank wwPDB ensuring single uniform archive of PDB data Nucleic Acids Res Protein Data Bank in Europe http Accessed Dec Protein Data Bank Japan https Accessed Dec Biological Magnetic Resonance Data Bank http Jorgensen Kastrup Biostructure based modeling In Textbook of Drug Design and Discovery pp Bateman et UniProt hub for protein information Nucleic Acids Res Lopez Silventoinen Robinson Kibria Gish server at the European Bioinformatics Institute Nucleic Acids Res Altschul Gapped BLAST and new generation of protein database search programs Nucleic Acids Res Goujon et new bioinformatics analysis tools framework at Nucleic Acids Res Finn et Pfam the protein families database Nucleic Acids Res Olğaç et Fenu Lewis Good Bodkin Essex Scoring functions In Jhoti Leach eds Drug Discovery pp Springer Dordrecht https Forli Huey Pique Sanner Goodsell Olson Computational docking and virtual drug screening with the AutoDock suite Nat Protoc Schomburg Bietz Briem Henzler Urbaczek Rarey Facing the challenges of target prediction by inverse virtual screening Chem Inf Model Rognan approaches to target ﬁshing and ligand proﬁling Mol Inform Moura Barbosa Rio Freely accessible databases of commercial compounds for virtual screenings Curr Top Med Chem Liu Lin Wen Jorissen Gilson BindingDB database of experimentally determined protein ligand binding afﬁnities Nucleic Acids Res Gaulton et The ChEMBL database in Nucleic Acids Res Pence Williams ChemSpider an online chemical information resource Chem Educ Wishart et DrugBank comprehensive resource for in silico drug discovery and exploration Nucleic Acids Res Wishart et DrugBank knowledgebase for drugs drug actions and drug targets Nucleic Acids Res Knox et DrugBank comprehensive resource for Omics research on drugs Nucleic Acids Res Law et DrugBank shedding new light on drug metabolism Nucleic Acids Res Xue Fang Zhang Yi Wen Shi TCMID traditional Chinese medicine integrative database for herb molecular mechanism analysis Nucleic Acids Res Masciocchi et MMsINC chemoinformatics database Nucleic Acids Res Williams et Open PHACTS Semantic interoperability for drug discovery Drug Discov Today Kim et PubChem substance and compound databases Nucleic Acids Res Irwin Sterling Mysinger Bolstad Coleman ZINC free tool to discover chemistry for biology Chem Inf Model Koutsoukas et From in silico target prediction to drug design current databases methods and applications Tanimoto An elementary mathematical theory of classiﬁcation and prediction International Business Machines Corporation Lipinski Lombardo Dominy Feeney Experimental and tional approaches to estimate solubility and permeability in drug discovery and development settings Adv Drug Deliv Rev Lipinski and compounds the revolution Chang Lindstrom Olson Belew Analysis of HIV and mutant structures via in silico docking against diverse ligand libraries Chem Inf Model Xue Wilcox Changing paradigm of cancer therapy precision medicine by generation sequencing Cancer Biol Med High Throughput Virtual Screening in Novel Drug Discovery Möller et Robust workﬂows how technical and scientiﬁc communities collaborate to develop test and share best practices for data analysis Data Sci Eng Ren Williams Clementi Krishnan Li Opal web services for biomedical applications Nucleic Acids Res Berthold et KNIME the Konstanz information miner SIGKDD Explor Ison et Tools and data services registry community effort to document bioinformatics resources Nucleic Acids Res Palmblad Lamprecht Ison Schwämmle Automated workﬂow composition in mass spectrometry based proteomics Bioinformatics https Balan Malinauskas Prins Möller molecular docking now in reach for wider biochemical community In Euromicro International Conference on Parallel Distributed and Processing pp Amazon Web Services https Accessed Dec Google Cloud https Accessed Dec Open Stack https Accessed Dec Open Whisk https Accessed Dec et Advances in distributed computing with modern drug discovery Expert Opin Drug Discov Potemkin Grishina Potemkin Internet resources for drug discovery and design Curr Top Med Chem https Catalog https Accessed Dec Harvey Fabritiis AceCloud molecular dynamics simulations in the cloud Chem Inf Model Doerr Harvey Noé Fabritiis HTMD molecular dynamics for molecular discovery Chem Theory Comput Cecilia García parallel blind virtual screening using BINDSURF BMC Bioinform Skalic Jimenez Fabritiis PlayMolecule BindScope large scale virtual screening on the web Bioinformatics Antunes Moll Devaurs Jackson Kavraki Liz DINC new protein peptide docking webserver using an incremental approach pp Irwin et Automated docking screens feasibility study Bikadi Hazai Application of the method to modeling proteins enhances docking accuracy of AutoDock Cheminform Olgac Budak Cobanoglu Nuti Carotti Banoglu Evias web services drug discovery platform In EuroQSAR Yan Zhang Zhou Li Huang HDOCK web server for protein and docking based on hybrid strategy Nucleic Acids Res Zundert et The web server integrative modeling of biomolecular complexes Mol Biol Li Leung Wong idock multithreaded virtual screening tool for ﬂexible ligand docking In IEEE Symposium on Computational Intelligence in matics and Computational Biology CIBCB pp Olğaç et Tsai Chang Chen iScreen world s ﬁrst web server for virtual screening and novo drug design based on TCM database Taiwan Comput Aided Mol Des Kainrad Hunold Seidel Langer LigandScout remote new interface for HPC and cloud resources Kiss Sandor Szalai http public web service for drug discovery Cheminform Chang Oyang Lin MEDock web server for efﬁcient prediction of ligand binding sites based on novel optimization algorithm Nucleic Acids Res Labbe et MTiOpenScreen web server for virtual screening Nucleic Acids Res Gupta Gandhimathi Sharma Jayaram ParDOCK an all atom energy based Monte Carlo docking protocol for complexes Protein Pept Lett Inbar Nussinov Wolfson PatchDock and SymmDock servers for rigid and symmetric docking Nucleic Acids Res Duhovny Nussinov Wolfson Efﬁcient unbound docking of rigid molecules Awale Reymond The polypharmacology browser target prediction combining nearest neighbors with machine learning Chem Inf Model https Konc Janezic ProBiS web server for detection of structurally similar protein binding sites Nucleic Acids Res Konc et web interface for prediction and optimization of ligands in protein binding sites Chem Inf Model Grosdidier Zoete Michielin SwissDock molecule docking web service based on EADock DSS Nucleic Acids Res Li Leung Wong Ballester web server for prospective virtual screening using ultrafast shape recognition techniques Nucleic Acids Res Koes Camacho ZINCPharmer pharmacophore search of the ZINC database Nucleic Acids Res Open Access This chapter is licensed under the terms of the Creative Commons Attribution International License http which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give priate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this chapter are included in the chapter s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the chapter s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder Ultra Wide Band Body Area Networks Design and Integration with Computational Clouds Joanna Ko lodziej B Daniel Grzonka Adrian Wid lak and Pawe l Kisielewicz Department of Computer Science Cracow University of Technology ul Warszawska Cracow Poland jokolodziej BANs connect Abstract Body Area Networks together nodes attached to human body and transfer the data to an external tructure The wireless communication channel and variety of ture sensor devices have lead to many useful applications of BANs such as healthcare monitoring military and emergency coordination rescue services sports and entertainment The Ultra Wide Band UWB munication model is widely used in wireless body area networks UWB Radio Frequency RF technology provides robust and energy eﬃcient transmission of data and signals through wireless networks This chapter surveys recent models applications and research challenges for future generation UWB RF technology for BANs The chapter also discusses the art in the support for data storage and ysis in mobile health monitoring Security issues for BANs in general and mobile health monitoring are addressed as key aspect of the recent developments in the domain Keywords Cloud computing Body area networks Sensor networks Ultra wideband communication Medical services Wireless communications Introduction Based on the IEEE standards Body Area Network BAN can be deﬁned as communication standard optimized for low power devices and operation on in or around the human body but not limited to humans to serve variety of applications including medical consumer entertainment and others REcently technological advancements in ultra Radio Frequency RF technology integrated circuits energy harvesting and storage and wireless communications have lead to the design of lightweight ligent and medical devices and sensors With all these achievements the thought of widespread deployment of pervasive wireless BANs for diverse c The Author s Ko lodziej and Eds cHiPSet LNCS pp https Ko lodziej et applications such as health monitoring and military coordination seems to be reality of the near future Typical BAN connects nodes sensors artiﬁcial skin etc attached to the surface of body or implanted in the body The focus in BANs been on medical and quality of life applications In such applications the network s nodes can be implanted medical devices such as gram ECG electrodes activity sensors and actuators or devices for storage of the medical data The Wireless Body Area Network WBAN sensors and devices may be classiﬁed based on their location into the following two main categories nodes area network nodes Figure presents the generic model of typical WBAN and nodes distribution with the wrist node as local central base station CENTRAL NODE SENSOR NODE Drug Delivery Blood Pressure WBAN Wrist Watch Hearing Aids ECG Capsule Endoscopy Total Knee Replacement Fig Generic sample model of wireless BAN The key requirements for BANs are i small network scale ii long battery life iii short range communications and iv security awareness Many powerful sensor platforms have been developed for various applications in the past decade In the case of transceivers decreasing the amount of energy required to transmit bit of data means reducing the turn times and power during transients and b duty cycling the radio operation The Ultra Wide Band UWB is the key technology for data transmission in WBANs through the transmission and duty cycle In Ultra Wide Band Body Area Networks the human tissues are not aﬀected by closely placed WBAN devices transmission and Ultra Wide Band Body Area Networks duty cycle also enable lower power consumption in the network nodes and long battery time which is the basic requirement for such systems UWB technology also conforms to the size and cost constraints of BAN devices From the implementation perspective the major advantages of using UWB radios are low implementation complexity b low duty cycling operation and c bit rate scalability UWB BANs have been surveyed already from the diﬀerent aspects such as communication channel models transmitter design antenna design and performance evaluation This chapter summarizes all the above surveys and gives the general and wider view on the design architecture and technological issues It addresses the main challenges related to BANs and and presents the recently developed els based on the cloud computing support for the patient s health monitoring patients data storage analysis and processing with the security criterion for inter and communication and data access The rest of the chapter is organized as follows BANs application areas are discussed in Sect Section presents the classiﬁcation of the wireless technologies used in WBANs In Sect some practical issues and relations between diﬀerent requirements of BANs are discussed Sections and deﬁne the research challenges and art in UWB BANs The concept of support for WBANs is presented in Sect Security aspects related to the cloud support in mobile health applications based on are discussed in Sect The chapter ends with the conclusions and highlights of the future research directions in the domain speciﬁed in Sect BAN s Applications and Challenges Various types of the main BANs components namely vital sign monitoring sors motion detectors through accelerometers and communication protocols for data processing and system access make these networks very useful in viding numerous services in healthcare research lifestyle emergency sports and military In this section we ﬁrst discuss the major application areas for BANs present the candidate technologies for wireless communication and data transmission and then highlight the most important practical and research challenges regarding WBAN technologies Applications of BANs The major areas of applications of BANs can be speciﬁed as follows Healthcare BANs are used to connect sensor devices with remote ing and assistance systems for physically disabled and elderly persons Mobile devices can be used to send and receive data to hospitals from the ambulances carrying patients to alert the concerned authorities and to get information about providing ﬁrst aid to save people s lives BANs also enable continuous itoring of patient s condition by sensing and transmitting vital signs such as Ko lodziej et heart rate ECG body temperature respiratory rate chest sounds blood sure etc It makes BANs an important tool for diagnosis and treatment of patients with chronic diseases such as hypertension diabetes etc BANs are also beneﬁcial to hospital patients who are monitored pervasively less of their location Pervasive monitoring through implanted devices enables medical staﬀ to predict diagnose and start treatment before the patient reaches an adverse stage of the disease BANs are also highly beneﬁcial for monitoring and assistance of elderly people as more and more people demand better quality of life Work and Emergency Services BANs are used in emergency services like rescue systems ﬁreﬁghting etc Using miniature sensors the condition and location of ﬁreﬁghters is monitored These sensors are also used to transfer mation to person commander who is not present at the site Based on the information provided by sensors the commander can coordinate emergencies with the team carrying out the rescue operation Lifestyle and Sports BANs are also beneﬁcial in providing quality of life and sport healthcare monitoring Such applications may include wearable entertainment systems car navigation systems and tour guides In sports BANs are used to monitor player vital signs such as the heart rate of marathon runner Military BANs are also used in war and combat scenarios by the military and police personnel Wearable sensors are integrated into the uniforms of soldiers This network can then be used to connect diﬀerent devices such as health sors surveillance cameras and Personal Digital Assistants PDAs installed on diﬀerent personnel body parts It is envisioned that wireless body area networks will become the heart of the future Internet and will play vital role in access and exchange of information in order to provide better facilities in healthcare education and lifestyle time patients vital signs monitoring emergency and battleﬁeld coordination may be read as part of science ﬁction and movies But one such ambitious project is underway at University of Washington to display information gathered from diﬀerent body sensors through LEDs built in contact lenses Candidate Wireless Technologies In this section various wireless technologies that are leading competitors in the upcoming market of BANs are discussed Communication between sensor devices is called communication while BAN communication with other works is called communication Selection of appropriate ogy for communication is research issue where mance of candidate wireless technologies is determined by the complete protocol stack including Physical layer PHY and upper protocol layers Bluetooth Classic Bluetooth is short range wireless communication dard that deﬁnes the link and application layers to support data and voice Ultra Wide Band Body Area Networks applications Piconet is short range network made up of up to eight tooth devices Bluetooth Special Interest Group developed the Bluetooth Health Device Proﬁle HDP that deﬁnes the requirements for qualiﬁed tooth healthcare and ﬁtness device implementations Bluetooth is widely adopted wireless technology for communication in WBANs Bluetooth Low Energy Bluetooth Low Energy BTLE is another candidate wireless technology for BANs It provides simple device discovery and reliable data transfer with encryption power save functionalities and ultra low power idle mode operation The key advantages of BTLE are the strength of the Bluetooth brand the promise of interoperability with Bluetooth radios in mobile phones and operations ZigBee ZigBee deﬁnes network security and application layer protocol suite on top of the PHY and Media Access Control MAC layers deﬁned by the IEEE Wireless Personal Area Network WPAN standard The PHY layer exploits the direct sequence spread spectrum technique for interference tolerance and MAC layer exploits carrier sense multiple access with collision avoidance for channel access ZigBee also provides support for sonal health devices complying with the IEEE standard including cardiographs glucometers pulse oximeters blood pressure monitors ters weight scales and respirometers ANT ANT is proprietary technology designed for general purpose wireless personal area network applications ANT features low latency simple design ability to trade oﬀ data rate against power consumption and net data rate of data rate is Sensium Sensium is proprietary transceiver platform tom designed for healthcare and lifestyle management applications The work adopts architecture where joining and leaving the network is managed centrally and all communications are Zarlink Zarlink developed an RF transceiver for medical implantable applications It uses coding scheme together with cyclic redundancy check CRC in order to achieve highly reliable link The key features of Zarlink are extremely low power consumption power wakeup circuit MedRadio compliance and security Other Technologies Proprietary RF technologies such as BodyLAN and Wave are also emerging on the horizon Inductive coupling IC and body coupled communications BCC technologies are also promising The data rate of IC is limited and it can not initiate communication from inside the body BCC transceivers are capacitively coupled to the skin and use the human body as channel to exchange data BCC is energy eﬃcient and alleviates interference and coexistence issues BCC can also be used for user identiﬁcation and automatic formation of BANs Ko lodziej et Practical Challenges BANs face several important challenging issues regarding their eﬃciency cal deployment and social adoption These issues constrain the solution space and need to be considered carefully when designing mechanisms for data security and privacy in WBANs Conﬂict Between Security and Eﬃciency High eﬃciency is strongly demanded for data security in WBANs because of the resource constraints Wearable sensors are often extremely small and have insuﬃcient power plies which render them inferior in computation and storage capabilities Thus the cryptographic primitives used by the sensor nodes should be as lightweight as possible in terms of both fast computation and low storage overhead Conﬂict Between Security and Safety Whether the data can be obtained whenever needed or not can be very crucial for patients safety Too strict and inﬂexible data access control may prevent the medical information from being accessed on time by legitimate medical staﬀ especially in emergency scenarios where the patient may be unconscious and unable to respond On the other hand loose access control scheme opens back doors to malintent users It is hard to ensure strong data security and privacy while allowing ﬂexible access Conﬂict Between Security and Usability The devices should be easy to use and foolproof since the users might be patients As the setup process of the data security procedures is it should involve as few actions with humans as possible For instance to bootstrap initial secure munication between all the nodes in WBAN for secure data communication device pairing techniques can be adopted The devices should oﬀer rity services during the phase of data collecting and processing by the advanced data processing centers like Cloud or Big Data processing systems but not disturbing their daily usage Requirement for Device Interoperability Patients can buy sensor devices from diﬀerent manufacturers it may be diﬃcult to any cryptographic materials among them It would be hardly possible to establish data security procedures that require minimum settings and eﬀorts and can work with wide range of devices Research Challenges In this section we summarize diﬀerent research challenges associated with BANs These issues need to be addressed for the widespread ment of BANs BANs brings forward number of research issues that need to be considered in the design of RF wireless systems Users can carry several BAN devices globally hence BAN radios are required to operate worldwide There is an abundance of high power technologies in Industrial Scientiﬁc and ical radio bands ISM bands that can cast performance degradation on the BAN devices which thus makes them less appealing for high ﬁdelity Ultra Wide Band Body Area Networks medical applications Wireless Medical Telemetry Service WMTS bands are heavily used but their use is restricted to healthcare facilities in the United States UWB can be exploited for wearable applications but it raises the issue of coexistence with multimedia applications The rules for MedRadio wing band are very strict and limiting These issues have voked the Federal Communication Commission FCC to think about opening up MHz range for medical BANs This is planned to hold up band entrenched devices that can serve as an artiﬁcial nervous system to reinstate sensation mobility and function to paralyzed limbs and organs Another research issue in BANs is channel model design The channel model plays vital role in the design of PHY technologies Experimental channel modeling for embedded and wearable devices is diﬃcult because humans and healthcare facilities are involved and both are governed by regulations An antenna design for body area networks is yet another challenging issue due to limitations on the size stuﬀ and form of the antenna Only caustic and biocompatible material such as titanium or platinum can be used for implants which results in degraded performance when compared with the antennas made of copper Organ and location of antenna decides its shape and size which further restricts the choice of designer physical layer protocol design requires reducing power consumption out aﬀecting reliability Flawless connectivity should be maintained in dynamic environments without the slightest possible performance degradation in terms of throughput data loss and latency Rapid turnaround time from transmission to reception and speedy wakeup from sleep mode can add signiﬁcance to power savings Energy eﬃcient hardware is also an issue existing wireless technologies draw relatively high peak current and mainly rely on duty cycling the radio between sleep and active modes to minimize the average current drawn Researchers have been exploring several promising techniques such as listening and radios which are intended to minimize power consumed by idle listening Coexistence of multiple BANs in crowded places such as hospitals needs robust MAC protocol The MAC protocol should be able to cope with topology changes caused by movement of nodes Channel migration protocols need to be developed to be able to migrate to quiet channel when serious hindrance is noticed Medical devices are subject to strict regulations to promote the safety and welfare of users Compliance to applicable regulations set forth by the FCC Food and Drug Administration FDA European Telecommunications dards Institute ETSI and other regulatory agencies is essential UWB Solutions for BANs Communication and eﬃcient data transmission are the crucial issues in the design and management of modern WBANs The Federal Communication mission urges the use of WMTS for medical applications However the people Ko lodziej et Table Main properties of communication and data transmission bands for BANs and WBANs Frequency MHz Acronym Scalability to BAN applications Merits Demerits MedRadio Worldwide availability good propagation characteristics quiet channel medical only General Telemetry Good propagation characteristics WMTS Good propagation characteristics medical only antenna size reasonable General Telemetry ISM Good propagation characteristics excellent building penetration characteristics Good propagation characteristics Secondary usage applications not allowed in MHz core band large antenna size limited bandwidth stringent rules Not internationally agreed only crowded spectrum large antenna limited bandwidth Licensed secondary use limited to healthcare providers inside healthcare facilities in US limited spectrum heavily used EU only limited spectrum heavily used only crowded spectrum used by low powered unlicensed devices harmful interference ISM ISM UWB Worldwide availability small antenna large bandwidth Crowded spectrum many standards and technologies Worldwide availability small antenna large bandwidth Existing standards and technologies severe attenuation Worldwide availability short range low power huge bandwidth does not interfere largely Coexistence with high data rate multimedia applications severe attenuation Ultra Wide Band Body Area Networks who are authorized to use this band are physicians and trained technicians Other technologies include unlicensed ISM Medical Implant Communications Service MICS and UWB ISM band is usually utilized to protect adjacent channel interference and additionally it is used by other classical Wireless sor Network WSN technologies In this context UWB technology seems to be the most promising band candidate for the future generation WBANs In Table we compare the main properties of the communication and data transmission bands in BAN and WBAN networks In this section we ﬁrst deﬁne the main concept of UWB technology and then present possible beneﬁts of the deployment of this mechanism in WBANs We focus on the aspects of the whole system and transmitter design communication channel modeling and the project of the physical layer of WBAN The models are presented along with short survey of the most notable and projects proposed in the literature UWB Technology UWB RF technology allows robust and energy eﬀective data and signals mission through wireless networks By using wide bandwidth UWB oﬀers an eﬀective and combination of data and energy management systems by utilizing bands within the frequency range of GHz UWB signals have an inherent behavior due to their extremely low maximum eﬀective isotropically radiated power This makes them diﬃcult to detect which is crucial in the medical applications where security aspects are very important There are various implementations of UWB technology which diﬀer in frequency band and signal characteristics The most common UWB model is based on speciﬁcation of the WiMedia Alliance presented in Fig Applications Additional Protocols Bluetooth etc UWB Media Access Control MAC Layer UWB Based Physical PHY Layer I D R N M M C M R F T L P Fig WiMedia UWB protocol see also Ko lodziej et Two main layers of the WiMedia Alliance protocol are UWB based Physical PHY and Media Access Control MAC layer which together form the mon radio platform in the protocol stack In PHY layer the frequency spectrum is divided into bands and band groups as shown in Fig BAND GROUP D N B z H M D N B z H M D N B z H M D N B z H M D N B z H M D N B z H M D N B z H M D N B z H M D N B z H M D N B z H M D N B z H M D N B z H M D N B z H M D N B z H M BAND GROUP BAND GROUP BAND GROUP BAND GROUP BAND GROUP Fig UWB frequencies and band groups Additional technical speciﬁcations for UWB include onal Frequency Division Multiplexing with over per nel MHz bandwidth bandwidth b channel bandwidth of MHz and c low broadcast power that allows tence with the other close band devices such as and Bluetooth radios The MAC protocol deﬁnes structure with sequentially executed beacon and data periods see Fig FRAME K FRAME BEACONS DATA BEACONS DATA BEACON PERIOD DATA PERIOD BEACON PERIOD DATA PERIOD Fig UWB MAC frames Beacons are transmitted by the UWB device s in order to provide the timing and resource access information which is passed on and transmitted during the data phase The PHY and MAC layers can be additionally combined with the other wireless protocols such as or Bluetooth as an additional option for integration into mobile devices Ultra Wide Band Body Area Networks UWB BAN System Model There is standardized architecture or system design for UWB BANs but research is being done in this area and number of works have been proposed with diﬀerent designs for the architecture of BAN system In Roy et have proposed promising architecture for body area networks architecture In this approach every sensor carries an antenna and the sink is placed in closed vicinity of body This sink is the central device and it supports an antenna array Most of the computational complexity lies in the sink Authors have proposed complete analytical channel model for the body diﬀracted waves mechanism IEEE group developed low cost low complexity low power and low range physical layer based on UWB technology The under discussion architecture is also built on the channel model of IEEE UWB is chosen because its low power spectral density and large bandwidth oﬀers many advantages such as low interference low sensibility to fading and accurate positioning This architecture is expected to exploit the array gain and in turn increase the Signal to Noise Ratio SNR Since power saving is critical issue in such systems the architecture minimizes the transmission power requirement and increases the battery lifetime of sensors Diﬀerent measurements were taken to test the proposed architecture in diﬀerent scenarios Issues like path loss tapped delay time and correlation aspects are discussed in detail Tap mean amplitude decays with the delay and is described by power law Parameters depend on the locations of transmitting and receiving antennas This work only considers the ﬁrst waves interfering with the human body More research needs to be done on the delayed waves which are coming after reﬂection through the surroundings In Kim et have proposed an UWB system employing binary zero correlation duration ZCD code for wireless BANs As diﬀerent devices are mounted on human body so there are chances of multiple access interference MAI in order to avoid this and to enhance the performance binary ZCD is used as spreading code for ultra wide band This work shows that system performance can be enhanced considerably without adding much complexity by applying ZCD code In Rajagopal et have proposed chaotic based UWB system The system meets the ultra low power requirements of body area networks while providing kbps to Mbps data rates thus enabling wide range of cations Chaotic UWB modulation scheme is used to attain spectrum ﬂexibility at minimum cost and power In Chen et have explored the application of cooperative nications in UWB body area networks Cooperative communications make use of available user terminals as relays that cooperate together to form virtual antenna array This work focuses on the system deployment scenario when the subject is in sitting posture Vital channel parameters such as path loss power delay proﬁle power variation and eﬀective received power correlation are investigated The main feature of this work is that it utilizes the directional antennas in such way so as to minimize human exposure to Ko lodziej et electromagnetic radiation keeping health concerns in mind This work also vides view of channel through apparatus The method used calculates the system diversity levels and it is expected to serve as standard for assessing the performance of various sity based techniques in body centric scenarios UWB Transmitter Design for BANs The power saving requirements of BANs impose some constraints on the transceiver architecture and UWB signaling strategy Implementation of UWB transceivers is still in the phase of research and conventional trends can not be followed because of these constraints So the logical alternative is to look for analog front end implementations and simplest possible transceiver architectures Low power UWB implementation for BANs have to meet the following design guidelines Low duty cycle signaling architecture at the receiver Pulse generator consuming low power and In order to comply with regulations increase spreading factor in case of short pulse waveforms That is the average power concentrated in pulse duration is spread into the spreading factor systems use pulse position modulation PPM to transmit mation with the MHz clock frequency of modulator In Ryckaert et highlight the potential of UWB for communication Most of the UWB transmitter building blocks are switched oﬀ between the pulses which allows to minimize the energy consumption Authors have evaluated the overall power consumption of the transmitter through link budget analysis in less BAN context taking into account explicit channel models comparison of implementations with UWB shows great reduction in power sumption In Kohno et have presented an ultra low power transceiver design for BANs Transmitters and detectors are also described System is lyzed in terms of packet error ratio link budget and power consumption This can be done by adopting the IEEE standard If research maintains its current momentum in this area further ments are expected both at circuit and algorithmic level which will make UWB promising and key technology for communications WBANs Antennas for UWB Frequencies Another important issue of WBANs is the antenna design which should meet the size limit and biocompatibility requirements The antenna for wireless body area network faces numerous RF challenges Several antenna designs have been proposed in the literature few of them are discussed below In Yazdandoost et have presented an antenna design which operates in UWB frequency from to GHz Two layers of substrate are used in the Ultra Wide Band Body Area Networks antenna to cancel the eﬀects of human body on performance of the antenna Performance is then analyzed in free space as well as close to the human body Experiments in diﬀerent scenarios are performed without making any changes to the antenna design The antennas become directional when placed close to the human body as it aﬀects the radiation patterns of antenna Similarly human body absorbs large amount of output power which aﬀects performance The proposed antenna design provides better performance when placed close to the human body due to the second substrate layer In Attari et have presented small size ultra wide band antenna for BANs The proposed antenna works in the range of GHz in free space which is then modiﬁed by using PEC perfect electric conducting plane PEC is used as reﬂector to enhance performance when antenna is in close proximity to the human body Simulation results show that the modiﬁed antenna better performance in frequency and time domain The proposed antenna design reduces the eﬀect of the body on the antenna thus increasing the gain of antenna when placed close to the human body by using PEC plane In Almpanis et have presented an inverted truncated annular tric resonator antenna for BANs The antenna operates at GHz The properties of antenna are discussed theoretically and practically Performance is analyzed in free space and close to human body and demonstrates good results in frequency as well as time domain Although the design is presented ically for BANs the design concepts are general and it can be used for other applications with minor modiﬁcations Using UWB for Transmission Channel Modeling in WBANs The high capacity transmission channel of the UWB systems is one of the most important advantage due to which UWB is employed in and medical monitoring systems Research been done on the study of path loss for various onbody and devices As data rate for BANs is very low and the range is limited UWB is being considered as most suitable lot of work been done on path loss model but the propagation channel not been discussed in detail so far When two sensors are communicating on the human body receiver can receive the signal in ways propagating through the body b diﬀracting around the body and c reﬂections oﬀ nearby scatters and then back toward the body The human body itself is quite complex environment and one studied it explicitly from the perspective of wireless communication Several channel models are presented in In Takizawa et have proposed stochastic channel models on power delay proﬁle and path loss All the models are derived from CTF channel fer functions in hospital room Path loss model is presented for several bands while power delay proﬁle is only presented for UWB In Aoyagi et have provided channel models for BANs in UWB quency band The statistical model and parameters are extracted directly from measured channel CFTs Channel models are derived on both path loss and power delay proﬁle In Betancur et have proposed similar statistical channel model Ko lodziej et UWB BAN Physical Layer According to the technical requirement of the WBAN task group many panies and research institutes have proposed physical layer architectures to vide fundamental technologies for WBAN communication systems Since there are various service scenarios for or applications the cal layer proposals include UWB and narrowband techniques The physical layer is responsible for activation and deactivation of the radio transceiver b Clear Channel Assessment CCA within the current channel and c data transmission and reception In Choi et have proposed the design of PHY simulator for wireless BAN system in IEEE As there are varying scenarios for body area works including and scenarios depending on the application the physical layer proposals include UWB as well as narrowband techniques In the WBAN PHY case the modulation method is adjusted according to frequency band and data rate Several works on performance evaluation based on diﬀerent channel models for UWB BANs are present in the literature In Chang et have investigated the performance of for BAN channel ity through extensive measurements for both spatial and polar antenna arrays Channel frequency responses were measured in GHz frequency range that covers the whole UWB band Eﬀects of bandwidth array spacing antenna ization and propagation conditions from the measured channels were analyzed on the channel capacity MIMO channels are formed by ing number of single input single output channel responses It was observed that the MIMO channel capacity decreases with bandwidth and frequency thermore the receiving power decreases when frequency band is increased High frequency components have low power so they don t contribute much to the channel capacity Similarly when the receiving array spacing is increased the channel capacity and the spatial correlation are decreased in most of cases and the maximum value of power diﬀerence between any two is increased In order to achieve maximum channel capacity array spacing of the spatial array should be lower than one wavelength Device compactness can be achieved without sacriﬁcing channel capacity in case of polar array in contrast to spatial arrays in of sight NLOS conditions in line of sight conditions the maximum achievable capacity of spatial array is higher than that of the polar array In Sangodoyin et have investigate on the impact of body mass index BMI on the propagation channels They provided detailed description of measurement campaign for wireless propagation channels Measurements were done in an anechoic chamber to clearly work out the impact of the body Channel frequency responses were measured in GHz frequency range In Takizawa et have presented performance evaluation of UWB less BANs which shows that the modulation scheme to pay much penalty if it uses receiver as compared to coherent detection In Domenicali et have analyzed the performance of BAN composed of IEEE Ultra Wide Band Body Area Networks UWB sensors The parameters used for performance evaluation are BER bit error rate network lifetime and throughput The work presents unique case by analyzing performance in the presence of an external interfering network BAN performance can be improved by using an optimized time hopping code assignment strategy In and performance evaluation method is presented which observes the waveform distribution along the signal path Performance of transceiver is evaluated in terms of BER Results show that the human body casts more eﬀect on performance than the environment especially in case of NLOS path for propagation channel Selection of suitable pulse shape depends upon the modulation schemes used Optimal Locations for Sensors in UWB BANs Knowledge of precise locations of sensors is necessary for successful data mission in many medical applications In WBAN these locations must be mated with very high precision Localization in WSNs been actively studied during the last few years Many methodologies tive in the standard wireless architectures such as the indicator RSSI technique and TOF measurement nique can be successfully applied in WBANs and other medical tions In Abbasi et have investigated the optimum locations for placing sors on human body Experiments were performed on more than transceiver locations for and NLOS scenarios Path loss models were developed and formance was evaluated for diﬀerent scenarios An orthogonal multiplexing OFDM based UWB system was used and calculations were formed on the basis of BER values see Table on selected locations Best possible locations identiﬁed by this work are legs and arms for and NLOS scenarios respectively Table BER values for diﬀerent parts of human body Body part Trunk Front side NLOS Back side BER Left arm Right arm Left leg Right leg Head BER Ko lodziej et of localization systems have been surveyed in In the authors present methodology of the transmission of ps Gaussian pulse which is modulated by an GHz carrier signal as direct support to ization system for indoor WBANs that provides accuracy in the mm range and seems to be one of the most promising solutions for the future generation positioning techniques Cloud Support to Mobile Systems and WBANs system was considered as solution for the presented model because it is the most powerful and it allows to interact remotely between patients and professionals Due to providing data from WBAN to this system these can help to solve many of the patient s medical problems Although communication tocols and eﬃcient data transmission between sensors network body local base node and the external network base station signal receivers are crucial in the design of modern WBANs data processing and replication remote patient monitoring by medical personnel and fast and secure access to the data still remain challenging and complex issues in today s health care systems Conventional medical systems are still built on workﬂows that consist of paper medical records duplicated test results handwritten notes The ment of Health and Human Services published in the Health mation Technology for Economic and Clinical Health HITECH Act which contains format known as an Electronic Medical Record EMR for tizing the patients data and medical tests to make them transparent to patients together with health plan costs covered services and health insurance cessful practical implementation of such digitization idea and its extension to be global system needs scalable IT infrastructure that allows fast remote access of patients and doctors to the system and data This system must be easily adaptable to various patients and other users needs tal policies and organizational sizes and security and data protection must be the paramount system s characteristics Recently cloud computing seems to be good candidate for supporting health medical systems as it successfully oﬀers multiple beneﬁts for enterprise computing environments Cloud computing enables convenient and access to shared ﬁgurable computing resources of various types namely physical infrastructures servers datacenters networks virtual servers and services to tributed users All these resources are provisioned by the service providers and local resource providers with possible minimal cost and management eﬀorts and interactions Major characteristics of cloud computing which are tant in medical data storage data analysis and processing can be deﬁned as follows Ultra Wide Band Body Area Networks each cloud user can deploy and conﬁgure the cloud servers and services himself interactions with service providers are sary multitenancy the resources and costs are shared across large community of cloud users scalability an eﬃcient and conﬁguration and assignment of system resources according to consumer demand easy system access cloud resources and services are accessed through dard Internet protocols using standard web browsers regardless of their location and platform smart phones meeting security demand dedicated services for higher security ments Model described above is supported by architecture Data from body sensors in WBANs are received by Cloud Client and then they are sent directly to the cloud which consist with the providers of the service such as Software as Service SaaS that is cloud computing model in which the application is running on the service provider s computers and it is available to users via the Internet Platform as Service PaaS which provides the runtime environment and the system on which the application runs Infrastructure as Service Iaas which provides the appropriate equipment to implement the described model and allows to its scalability at any time These services lect received from Cloud Client data from sensors in Data center with access to Cloud Storage Facilities The general architecture of WBAN support infrastructure is presented in Fig Body Sensors Cloud Client SaaS PaaS IaaS Data center Cloud Storage Facilities Cloud Architecture Fig support to WBANs In this model patient equipped with BAN sensors and mobile device such as smart phone or tablet communicates to the cloud system through cloud web client or specially designed standalone application The base sor module in WBAN collects the patient s data and transmits it to the mobile device often via Bluetooth without the patient s intervention The cloud client cloud platform interface installed at the mobile device forwards the patient s Ko lodziej et data to the appropriate web service installed at the cloud platform The collected data is stored analyzed and processed by software module The results of this analysis and possible diagnosis are disseminated to the patient his doctor who can be another mobile cloud user emergency services if necessary tal at speciﬁed time intervals or just single time period point Patient s historical medical reports are available for retrieval from the cloud after user authentication Although many popular cloud computing platforms either free iCloud and DropBox or commercial GoGrid Amazon AWS Google Cloud Platform are already available for pervasive agement of large volumes of user s data they may not be suitable for many health care applications Moreover cloud computing for health care needs est level of availability security and privacy for the patients data in order to gain acceptance in the marketplace Development of high security and ity of cloud computing services and personalization of processed patients data are contentious issues in the design of scalable systems supporting WBANs Art The integration of WBANs and cloud service providers to provide mobile health facilities transformed from vision of future to nowday facility Numerous renowned hospitals in Europe and US provide services based on tronic Health Records EHR with cloud integration In the following lines we discuss the art research proposals regarding ubiquitous health based on integration In the authors present the support for the analysis of the electrocardiogram ECG data The proposed system model consist of the three traditional cloud layers that is Software b Platform and c ture The software layer provides data upload storage and analysis facilities The platform layer consists of three modules container scaling manager b workﬂow engine and c Aneka scheduling environment The workﬂow engine is installed inside the container and manages the execution of the ﬂow process in ECG analysis The ECG data processed in this workﬂow can be either numerical or graphical ECG and are calculated and compared with the patients historical data and standard values The container scaling manager creates new containers according to the number of incoming requests tasks The tasks composed in the workﬂow module are submitted to the Aneka scheduling environment This scheduling system plays the role of workﬂow distribution and management platform based on the conventional model adopted in cloud systems The Aneka master is installed on the platform cloud layer while the slaves are task executors installed on the infrastructure entities virtual machines of the cloud For the security of data tion between diﬀerent layers third party Public Key Infrastructure PKI is proposed This model is good example of the utilization of the public cloud environment for monitoring the patient s health condition Ultra Wide Band Body Area Networks Similar idea is presented as the HealthCloud project in The cloud vice client is designed as web application for the Android consisting of Patient Health Record module which retrieves patient records form the cloud b Medical Imaging module that decodes and displays the image ing to the Digital Imaging and Communications in Medicine DICOM and compression standard The Amazon s cloud service been utilized for the evaluation of the developed model in WLAN and works with diﬀerent compression parameters SSL data encryption is proposed to secure data communication similar model based on Internet of Things IoT communication framework for mobile health care is described in Doherty et have proposed physiological lifelogged data framework that stores lifetime events of person over cloud While the lifelogged data can help people remember what they were doing at particular moment it can also be analyzed to ﬁnd out potential reasons behind medical condition The proposed SmartLogger system provides three interfaces to view the ical data The query interface allows user to search data based on temporal queries The chart interface provides graphical images of physiological activities such as the heart rate measured by ECG sensor The context interface provides additional information gathered from various such as the location of the user images of the surroundings and audio data that might be helpful in classifying temporal event Research also been done on integration to build centric information sensing and dispensing applications that work on the basis of publication subscription model broker that delivers information to users via cloud service application consists of sensor data stream monitoring and processing component b registry ponent that saves information about user subscriptions to diﬀerent applications c an analyzer component that maps data streams to corresponding tions and d disseminator component that sends data to subscribed user using an event matching algorithm integration for mobile health care systems is novel idea with beneﬁts of computational and storage oﬄoading However researchers have opposed cloud computing approach to ubiquitous healthcare due to the municational overhead such system incurs delays experienced and continuous connectivity requirement with the cloud Instead ubiquitous healthcare system based on resource provisioning framework that harnesses the tational and storage capabilities of computing devices lying in close vicinity is proposed The authors of have proposed wireless communication model to transmit patient data and to prioritize data streams based on severity of patient s condition The model is hierarchical architecture consisting of inter and communications Inside each BAN cluster head CH is selected based on higher computational and communicational power to act as gateway for all sensor nodes The CH may himself act as ker or oﬄoad the tasks to base station The broker of the proposed system is Ko lodziej et composed of workﬂow manager and The workﬂow ager receives service requests from sensors The optimizer identiﬁes the ate service providers in the vicinity for service distribution The service provides volunteer themselves or they are identiﬁed by messages These messages contain the computational CPU cycles and storage bytes facility available at service provider In this manner ubiquitous health can be provided without the requirement of cloud connectivity Security Challenges and Solutions BANs are meant to support medical applications mainly Hence privacy rity Quality of Service QoS and reliability are important factors besides energy eﬃciency Traditional security and privacy techniques are not appropriate for BANs due to bounded processing power memory and energy as well as the lack of user interface unskilled users and global roaming Hence novel lightweight and methods have to be developed for BANs Global ing over heterogeneous infrastructure networks further complicates security provisions Cloud integration of mobile health applications given rise to new security concerns such as trustworthiness of the cloud client and secure communication Security and privacy of patients data when it is stored in and retrieved from the cloud user authentication and authorization for access to the data and internal management of data by the cloud provider are major concerns in the mobile health applications Researchers have proposed the use of protocol to secure communication between the cloud and the patient The trust between the cloud and the patient is still major research issue to be dwelled upon The process of authentication between user and service provider for the identiﬁcation of proper authorization level is based on trusted third party certiﬁcation authority in many cloud based applications The trusted third party certiﬁcation authority is responsible for the tation of the authentication and authorization system based on the PKI Cloud support provides the opportunity to oﬄoad computationally intensive jobs form the mobile devices to the cloud In the following lines we present art research proposals for security challenges in WBAN and WBAN cloud tion Nkosi and Mekuria propose security service provision framework based on the cloud services as solution for mobile health concerns Security is vided as service to the mobile devices like other services provided by the cloud In this model the mobile devices are cloned over the cloud infrastructure When the mobile device faces computationally intensive task it transfers the task to its clone over the cloud The task is executed in the cloud and the output is sent back to the mobile device The only concern about Security As Service SasS model is the security of the communication channel between the cloud and the mobile device This threat is mitigated by implementation of secure tion protocols Similar task scheme been presented in Task Ultra Wide Band Body Area Networks oﬄoading techniques help the resource constrained mobile devices and sensors to execute their task over the cloud but they also incur communication overhead Wang et have proposed distributed dynamic integrity check and data storage scheme for WSNs The data to be encrypted is divided into n blocks Each data block is encrypted using secret key Each encrypted data block is distributed among n neighbors thus providing secure data storage When an integrity check is required for the data each node having share of the n blocks of the data computes and broadcasts its integrity checksum Thus the distributed storage and computation reduces the memory overhead on single sensor node To secure medical data from determined adversary Pietro et have proposed data survivability scheme based on continuous movement of data from one sensor node to another It is assumed that the adversary knows the origins of the target data and can compromise subset of nodes to capture that data The continuous data communication replication and encryption applied in this scheme makes it impractical for energy constrained WBANs Access privileges to medical data in WBANs can be maintained by menting Access Control RBAC model The participating WBAN entities are assigned groups according to their roles such as physicians and patients Each user role is mapped to set in one to many pings RBAC is simplistic and well suited for the requirements of WBANs An RBAC scheme based on secret key encryption for medical WBANs is proposed in The proposed scheme utilizes the random key predistribution methods deﬁned in By predistributing encryption keys user can establish wise keys with all entities in the WBAN Each pairwise key is mapped to role thus access privileges are assigned to each pair of keys The predistribution and pairwise key management make this scheme impractical for large WBANs conﬁgurations In secure data communication model for scalable cloud computing applications is presented The model adds Security Manager entity to the generic cloud model The security manager is trusted third extension of the cloud provider The security manager generates public and private keys of each user regenerates keys after expiry generates session keys and maintains access control list for each data set data scheme proposed in is used to oﬀer conﬁdential data storage that is even private to cloud provider user encrypts the data before sending it to the cloud where the security manager the data before it is stored in the cloud The authors have also provided variation of the model in which data encryption is oﬄoaded to the security manger while data is performed by the cloud provider In researchers have presented novel domain based trust model trust value is associated with each client and domain entity When an entity wants to communicate with an entity B it compares the entity B s trust value with its trust threshold If the trust value is greater than the trust threshold it will continue with the transaction Otherwise it will abort Ko lodziej et the operation Trust values are updated on the basis of completed transactions which have not violated trust relationship between two entities Sorber et have proposed novel mHealth model based on watchlike wearable device Amulet An Amulet provides many advantages over mobile and PDA based mHealth modes such as Amulet is tightly coupled with the patient s body unlike mobile devices and PDA s which can be out of transmission range of the sensor devices b Amulet uses hardware to support secure mHealth applications Unlike mobile devices the Amulet provides secure storage of encryption keys c the Amulet provides interoperability between sensor and mobile devices by implementing multiple radios supporting diverse wireless technologies and d Amulet is able to authenticate the wearer by various physiological parameters such as pulse and galvanic skin response GSR Researchers have proposed approach to secure medical data sensing and processing in mobile phones based on smart cards that provide trusted computing platform The smart card provides processing and storage of encrypted data The smart card thus exchanges keys with medical sensors and the cloud for sending and receiving encrypted data The test model includes JavaCard applet and two Android phones with card integration The test model implementation shows energy consumption overhead in worst case increase in latency time and communication overhead while ensuring data conﬁdentiality and integrity The proposals discussed in the above section ensure data security which is critical in the context of medical health records but each proposal also incurs computational and communicational overheads while ensuring data security lightweight universally accepted security framework for WBANs providing uitous health facilities is an area with great research potential Conclusions and Future Directions The WBAN is an emerging and promising technology that is expected to change people s daily life and healthcare experiences Data security safety eﬃciency and privacy in WBANs are the key research issues and number of considerable challenges still remain to overcome The research in this area is still in its infancy but it is believed it will draw huge amount of interest in next few years This chapter highlighted many research and practical issues related to WBANs supported by the UWB wireless technologies UWB wireless technology for communications is one of the most promising methodology for the eﬀective data transmission in today s medical applications However some early results on the application of the other technologies such as Bluetooth and ZigBee are also available but standard been developed so far for WBANs There is big potential in designing physical layer for WBANs Previous work includes some UWB and narrow band techniques channel modeling ﬁeld is also in research phase and the complex environment of the human Ultra Wide Band Body Area Networks body poses many challenges to researchers in this regard There is standard antenna design for wireless BAN devices Antenna needs to be comfortable and compatible with the human body The issues of directionality antenna gain needs to be investigated in detail when working around the human body Neither transmitter design not been ﬁnalized yet Diﬀerent transmitter designs have been proposed with the focus on transmitters Apart from these technical details there are lot more issues to be resolved for social acceptance of BANs These include conﬂicts between diﬀerent requirements such as security and eﬃciency Cloud computing infrastructure provides cost eﬀective and easy to use data sharing and access platform which is ideal for the IT enabled healthcare systems Requirements of such cloud supported health system is high availability of data for healthcare personnel and security of the patients data Authentication and authorization solutions provided by third parties have been proposed as solution for data conﬁdentiality requirements Catering all these challenges is most likely to require new methods and cols for MAC and PHY and antenna design transmitter design and lightweight security protocols UWB is being considered as most promising candidate for BANs The in system design channel modeling performance evaluation transmitter design and antenna design of UWB BANs is also presented in this chapter The integration and related rity challenges have also been thoroughly surveyed Researchers engineers and practitioners from various disciplines must come together to overcome technical roadblocks in order to bring the vision of ubiquitous body area network to reality References Abbasi Khan Alomainy Hao Characterization and modelling of ultra wideband radio links for optimum performance of body area network in health care applications In International Workshop on Antenna Technology iWAT pp Almpanis Fumeaux Vahldieck truncated conical tric resonator antenna for network applications IEEE Antennas Wirel Propag Lett Drude Requirements and application scenarios for body area networks In Mobile and Wireless Communications Summit Amazon Web Services IEEE Ayar UWB Wireless Video Transmission Technology in Medical Applications NDS Surgical Imaging NDSsi White Bahoura Hassani Hubin DSP implementation of wavelet transform for real time ECG wave forms detection and heart rate analysis Comput Methods Programs Biomed Betancur Cardona Navarro Traver statistical channel model for on body area networks in ultra wide band communications In Proceedings of the IEEE Radio and Wireless Symposium vol pp Ko lodziej et Basu Sarkar Nagaraj Chinara survey on ultra wideband and ultrasonic communication for body area networks Int Ultra Wideband mun Syst Baunach Kolla Mvihlberger Beyond theory development of real world localization application as low power WSN In Proceedings of IEEE ference on Local Computer Networks Dublin Ireland October pp Buyya Broberg Goscinski eds Cloud Computing Principles and Paradigms Wiley Hoboken Cao Leung Chow Chan Enabling technologies for wireless body area networks survey and outlook IEEE Commun Mag Chang Tarng Peng on UWB MIMO performance for body area network applications IEEE Antennas Wirel Propag Lett Cwalina S lawomir Rajchowski An narrowband and band channel model for body area networks in ferry environment In Proceedings of the IRACON MC Meeting and Technical Meeting pp Teo et Cooperative communications in wireless body area networks channel modeling and system diversity analysis IEEE Sel Areas Commun Hoyt Reifman Coster Buller Combat medical informatics present and future In Proceedings of the AMIA Annual Symposium San Antonio TX pp Choi Kim Lee Wang Kim Chung Narrowband physical layer design for WBAN system In Proceedings of the First International Conference on Pervasive Computing Signal Processing and Applications pp Contaldo Banerjee Ruﬃeux Chabloz Roux Enz transceiver for wireless body area networks IEEE Trans Biomed Circuits Syst The Digital Imaging and Communications in Medicine DICOM Standard Mare Sorber Shin Cornelius Kotz Adaptive security and privacy for mHealth sensing In Proceedings of the USENIX Conference on Health Security and Privacy HealthSec pp USENIX Association Berkeley Merli Skrivervik Design and measurement considerations for implantable antennas for telemetry applications In Proceedings of European Conference on Antennas and Propagation EuCAP pp Santis Feliziani Maradei Safety assessment of UWB radio systems for body area network by the method IEEE Trans Magn Domenicali Nardis Di Benedetto UWB body area network coexistence by interference mitigation In Proceedings of IEEE International ference on ICUWB Vancouver Canada September pp Doukas Pliakas Maglogiannis Mobile healthcare information agement utilizing cloud computing and android In Proceedings of the Annual International Conference of the IEEE EMBS Buenos Aires Argentina September pp Ultra Wide Band Body Area Networks DropBox Drude Requirements and application scenarios for body area networks In IST Mobile And Wireless Communications Summit pp Gerrits Farserotu Long communications IEEE Trans Circuits Syst GoGrid Storage Services Google Cloud Platform Grzonka The analysis of OpenStack cloud computing platform features and performance Telecommun Inf Technol Grzonka Szczygiel Bernasiewicz Wilczynski Liszka Short analysis of implementation and resource utilization for the OpenStack cloud puting platform In Proceedings of European Conference on Modelling and Simulation ECMS pp Hanson et Body area sensor networks challenges and opportunities Computer HITECH Hernandez Kohno Ultra low power UWB transceiver design for body area networks In International Symposium on Applied Sciences in Biomedical and Communication Technologies Israel pp Hjortland Lande CTBV integrated impulse radio design for biomedical applications IEEE Trans Biomed Circuits Syst iCloud Val et Mobihealth mobile health services based on body area networks In Istepanian Laxminarayan Pattichis eds Topics in Biomedical Engineering pp Springer Boston https Karimabadi Attari Gain enhancement of small size UWB antenna for wireless body area network applications In IEEE Proceedings of ICEE May pp Kim Park Cha Kim Improved performance of UWB system for wireless body area networks IEEE Trans Consum Electron Kuhn Zhang Mahfouz Fathy UWB indoor tioning system with millimeter dynamic accuracy In Proceedings of IEEE Antennas and Propagation Society International Symposium Charleston CS June Braem Moerman Blondia Demeester survey on wireless body area networks Wireless Netw Meier Terzis Lindenmeier robust high precision radio location system In Proceedings of International Microwave Symposium Honolulu HI June pp Nkosi Mekuria Cloud computing for enhanced mobile health tions In Proceedings of the IEEE International Conference on Cloud puting Technology and Science CloudCom pp Open Clinical Knowledge Managemnt for Medical Care Pandey Voorsluys Rahman Buyya Dobson Chiu grid workﬂow environment for brain imaging analysis on distributed systems Concurr Comput Pract Exp Ko lodziej et Pandey Voorsluys Niu Khandoker Buyya An autonomic cloud environment for hosting ECG data analysis services Future Gener Comput Syst Chen Gonzalez Vasilakos Cao Leung Body area networks survey Mob Netw Appl Bui Zorzi Health care applications solution based on the Internet of Things In Proceedings of the International Symposium on Applied Sciences in Biomedical and Communication Technologies ISABEL ACM New York Article p Viswanathan Chen Pompili Research challenges in computation munication and context awareness for ubiquitous healthcare IEEE Commun Mag Hassan Song Huh framework of integration opportunities and challenges In Proceedings of the International Conference on Ubiquitous Information Management and Communication ICUIMC pp ACM New York Sorber Shin Peterson Kotz practical trusted ing for mhealth In Proceedings of the International Conference on Mobile Systems Applications and Services MobiSys pp ACM New York Ateniese Fu Green Hohenberger Improved proxy schemes with applications to secure distributed storage ACM Trans Inf Syst Secur Wenliang Deng Varshney Katz Khalili pairwise key predistribution scheme for wireless sensor networks ACM Trans Inf Syst Secur Li Lou Ren Data security and privacy in wireless body area networks Wireless Commun Chun Maniatis Augmented smartphone applications through clone cloud execution In Proceedings of the Conference on Hot Topics in Operating Systems HotOS USENIX Association Berkeley Li Ping Trust model to enhance security and interoperability of cloud environment In Jaatun Zhao Rong C eds CloudCom LNCS vol pp Springer Heidelberg https Wang et Dependable and secure sensor data storage with dynamic integrity assurance In Proceedings of the IEEE INFOCOM pp Weiss Health and biomedical informatics netWorker Kurschl Beer Combining cloud computing and wireless sensor networks In Proceedings of the International Conference on Information Integration and Applications and Services iiWAS pp ACM New York Doherty Tolle Smeaton Utilising contextual memory retrieval cues and the ubiquity of the cell phone to review lifelogged physiological activities In Proceedings of the International Workshop on Interactive Multimedia for Consumer Electronics IMCE pp ACM New York Sorber et An amulet for trustworthy wearable mHealth In Proceedings of the Twelfth Workshop on Mobile Computing Systems and Applications HotMobile ACM New York Article Ultra Wide Band Body Area Networks Di Pietro et Catch if you can data survival in unattended sor networks In Sixth Annual IEEE International Conference on Pervasive Computing and Communications PerCom pp Morchon Baldus Eﬃcient distributed security for wireless medical sor networks In International Conference on Intelligent Sensors Sensor works and Information Processing pp Istepanian Laxminarayan Pattichis Emerging Mobile Health Systems Springer New York https Venkatasubramanian Gupta Security solutions for pervasive care In Xiao ed Security in Distributed Grid Mobile and Pervasive puting pp Auerbach Boston Patel Wang Applications challenges and prospective in emerging body area networking technologies IEEE Wirel Commun Qiu et Cloud technologies for bioinformatics applications In Proceedings of the Workshop on Computing on Grids and Supercomputers pp ACM Rajagopal Kang Park Bynam Cho Won Chaotic UWB based system design for ultra low power body area networks IEEE Trans Microw Theory Tech Roy Oestges Horlin Doncker comprehensive channel model for UWB multisensor multiantenna body area networks IEEE Trans Antennas Propag Ryckaert et transmitter for wireless body area networks IEEE Trans Circuits Syst I Sangodoyin Molisch Experimental investigation of the impact of BMI on ultrawideband MIMO networks In Proceedings of the IEEE Vehicular Technology Conference VTC Spring pp Takizawa et Channel models for wireless body area networks In ings of the Annual Conference of IEEE Engineering in Medicine and Biology Society pp Takizawa Aoyagi Kohno Channel modeling and performance tion of wireless body area networks In IEEE ICC Tysowski Hasan key management towards secure and scalable mobile applications in clouds In IACR Cryptology ePrint Archive pp Lorincz et Mercury wearable sensor network platform for motion analysis In Proceedings of the ACM Conference on Embedded worked Sensor Systems SenSys pp ACM New York Grzonka Palmieri security driven meta scheduler for distributed cloud organizations Simul Model Pract Theory Varshney Pervasive healthcare and wireless health monitoring Mob Netw Appl Big data security In Pop Ko lodziej Di Martino B eds Resource Management for Big Data Platforms Algorithms Modelling and Performance Computing Techniques CCN pp Springer Cham https Vecchiola Chu Buyya Aneka software platform for could computing In High Speed and Large Scale Scientiﬁc Computing pp IOS Press Ko lodziej et Wang Park Modeling and analysis of failures in wireless body area networks with model IEEE Commun Lett Xia Redﬁeld Chiang Experimental characterization of UWB channel for body area networks EURASIP Wirel Commun Netw Article ID https Yan Soh Vandenbosch Wearable ultrawideband review of ultrawideband antennas propagation channels and applications in less body area networks IEEE Access Yang ed Body Sensor Networks Springer London https Yazdandoost Kohno UWB antenna for wireless body area network In Proceedings of the Microwave Conference APMC pp Zasowski Wittneben Performance of UWB receivers with partial CSI using simple body area network channel model IEEE Sel Areas Commun Open Access This chapter is licensed under the terms of the Creative Commons Attribution International License http which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give appropriate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this chapter are included in the chapter s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the chapter s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder Survey on Multimodal Methods for Emotion Detection Catherine Krzysztof Lamine Dariusz Piotr Corinne and Katarzyna Allianstic Research Laboratory Efrei Paris Villejuif France Institute of Mechanics and Applied Computer Science Kazimierz Wielki University Bydgoszcz Poland dmikolaj krzysiekkt piotrek Neurocognitive Laboratory Centre for Modern Interdisciplinary Technologies Nicolaus Copernicus University Toruń Poland MINES ParisTech PSL University CRI Paris France Abstract Automatic emotion recognition constitutes one of the great lenges providing new tools for more objective and quicker diagnosis nication and research Quick and accurate emotion recognition may increase possibilities of computers robots and integrated environments to recognize human emotions and response accordingly to them social rules The purpose of this paper is to investigate the possibility of automated emotion tion recognition and prediction its and main directions for ther research We focus on the impact of emotion analysis and state of the arts of multimodal emotion detection We present existing works possibilities and existing methods to analyze emotion in text sound image video and logical signals We also emphasize the most important features for all available emotion recognition modes Finally we present the available platform and outlines the existing projects which deal with multimodal emotion analysis Keywords Affective computing Emotion detection Automatic data processing Data collection Expressed emotion Big data Artiﬁcial intelligence Introduction Affective Computing AC been popular area of research for several years Many research been conducted to enable machines detect and understand human affective states such as emotions interests and the behavior It attempts to bridge the munication gap between human users and computers with soulless and emotionless The Author s Kołodziej and Eds cHiPSet LNCS pp https Marechal et feeling The inability of today s systems to recognize express and feel emotions limits their ability to act intelligently and interact naturally with humans To become more and effective systems need to become sensitive to human emotions Verbal and nonverbal information is very important because it complements the verbal message and provides better interpretation of the message It is very useful to design and develop systems that can measure the emotional state of person based on for example gestures body movements and postures facial expression acoustic characteristics and emotions expressed in the text In the practical case body signals and facial expressions recorded in by sensors and cameras can be associated with predetermined emotions It is interesting to merge the multimodal information retrieved by these devices with information from analysis of emotions and intensity in text For example the ultimate goal may be to improve the system s decisions so that they can react accordingly to recognized emotions which will allow better interaction Conventional wisdom says that of communication between humans is nonverbal The studies conducted by Albert Mehrabian in established the rule also known as the rule of the communication is verbal of the communication is vocal and of the communication is visual This justiﬁes the interest and importance of nonverbal communication The ﬁrst study in the ﬁeld of emotion detection was born during the The most prominent example is that of mood rings The principle is simple rings contain thermotropic liquid crystals that react with body temperature When person stressed his mood ring take on darker color The scientiﬁc publications of Rosalind Picard MIT have introduced great gress in this ﬁeld since the nineties is one of the pioneers of affective computing In his book Affective Computing Picard proposed that emotion can be modeled using the nonlinear sigmoid function Over the last years the development of technology allowed the implementation of relatively good system market and efﬁcient such as ambient intelligence AMI virtual reality VR and augmented reality AR Nowadays in the automotive ﬁeld for example an computer that is able to detect confusion interest or fatigue can increase safety The AutoEmotive MIT Media Lab is prototype equipped with sensors and camera placed on the steering wheel This vehicle measures the level of stress and fatigue of the driver When the need arises puts background music changes the temperature and light in the vehicle interior or still proposes to follow less stressful journey We have structured the remainder of the paper as follows Section describes in general existing works dealing with the emotion detection and multimodal emotion analysis problem Section presents the basic modalities and methods used for tion analysis We have presented the related existing projects dealt with this problem in Sect We summarize the survey and introduce some directions for future research in Sect Survey on Multimodal Methods for Emotion Detection Multimodal Emotion Analysis Traditional emotion recognition and classiﬁcation associate the value to at least several basic emotions such as happiness joy neutrality surprise sadness disgust anger or fear Multimodal emotion analysis becomes very popular research domain Started from the classic deﬁnition of sentiment analysis was extended to modal with other relevant modalities video audio sensor data Different niques and methods are combined to achieve it very often they are based on big data methods semantic rules and machine learning novel multimodal implicit emotion recognition system can be built upon an based model designed to extract information on the emotion from different devices To feed such model video data captured by the camera embedded in the user s device laptop desktop tablet etc an audio signals collected from microphones embedded in mobile devices and motion signals generated by sensors in wearable devices can be used Several multimodal datasets include sentiment annotations Zadeh et introduced the ﬁrst multimodal dataset MOSI with sentiment intensity annotations and studying the prototypical interaction patterns between facial gestures and spoken words when inferring sentiment intensity They proposed new computational resentation called multimodal dictionary based on study and evaluated the proposed approach in model for sentiment intensity prediction For other examples of data sets we can cite and MOUD datasets One of the most challenges in multimodal emotion analysis is to model the actions between language visual and acoustic behaviors that change the observation of the expressed emotion named the dynamics second challenge in multimodal emotion analysis named dynamics is to efﬁciently explore emotion not only on one but on highly expressive nature modality ex spoken guage where proper language structure is often ignored video and acoustic modalities which are expressed through both space and time To solve this problem Zadeh introduced the Tensor Fusion Network TFN which combine the and models dynamics is modeled with new multimodal fusion approach named Tensor Fusion which explicitly aggregates unimodal bimodal and trimodal interactions dynamics is modeled through three Modality Embedding Subnetworks for language visual and acoustic modalities respectively Interesting work is realized by Poria et who developed network to extract contextual features from the video for multimodal sentiment analysis Additionally they presented multimodal sentiment analysis framework which includes sets of relevant features for text and visual data as well as simple technique for fusing the features extracted from different modalities The previous presented methods combine and analyze data from various types of modality the basis for all these analyzes is appropriate detection of emotion on each modality input At the same time we must point out that for each modality individually Marechal et we can also perform emotion analysis In the next chapter we will describe the methods of emotion analysis for each modality separately Basic Modalities for Emotion Analysis Emotion in Text Opinion Mining OM and Sentiments Analysis SA consists in identifying orientation or intensity of sentiments and opinion in pieces of texts blogs forums user comments review websites community websites It enables determining whether sentence or document expresses positive negative or neutral sentiment towards some object or more In addition it allows for classiﬁcation of according to intensity degrees Deﬁnition An opinion is quadruple F H S where is target object F is set of features of the object H is set of opinion s holders S is the set of value of the opinion s holder on feature fi F of the object According to Liu sentiment analysis is the ﬁeld of study that analyses ple s opinions sentiments evaluations appraisals attitudes and emotions toward entities such as products services organizations and their attributes It represents large problem space There are also many names and slightly different tasks sentiment analysis opinion mining opinion extraction sentiment mining subjectivity analysis affect analysis emotion analysis review mining Sentiments analysis is complex technique Sentiments and opinions can often be expressed in subtle manner that creates difﬁculty in the identiﬁcation of their tional value Moreover opinions and sentiments are highly sensitive to the context and the ﬁeld in which they are used the same string might be positive in one context and negative in another In addition on the Internet everyone uses his or her own style and vocabulary what adds extra difﬁculty to the task It is not yet possible to ﬁnd out an ideal case to marking the opinion in text written by different users because the text does not follow the rules Therefore it is impossible to schedule every possible case Moreover very often the same phrase can be considered as positive for one person and negative for another one Text analysis and social network analysis have gained importance with growing interest in Big Data Both deal with large amounts of data largely unstructured and the Big Data beneﬁt comes from the application of these two methods of data analysis We will begin by examining some of the ways in which text analysis can be applied to sentiment analysis before moving on to social network analysis Text Analysis Sentiment Analysis SA is computational study of how opinions attitudes emoticons and perspectives are expressed in language Sentiment Detection or in its simpliﬁed form Polarity Classiﬁcation is tedious and complex task Contextual changes of polarity indicating words such as negation sarcasm as well as weak syntactic structures make it troublesome for both machines and humans to safely determine polarity of messages Survey on Multimodal Methods for Emotion Detection Sentiment analysis methods involve building system to collect and categorize opinions about product This consists in examining natural language conversations happening around certain product for tracking the mood of the public The analysis is performed on large collections of texts including web pages online news Internet discussion groups online reviews web blogs and social media Opinion Mining aims to determine polarity and intensity of given text whether it is positive negative or neutral and to what extent To classify the intensity of opinions we can use methods introduced in Social Networks Social Networks are indisputably popular nowadays and show sign of slowdown According to the Kepios study the number of active users of social networks increased by in to reach billion users in April For example Facebook attracts more than billion users month Penetrating ever more aspects of our daily life they become not only considerable threat to our privacy but also an encompassing tool for analyzing opinions habits trends and some would even say thoughts In the current growth of artiﬁcial intelligence machine learning and natural guage processing driven by new technological possibilities it is possible to automate the analysis of vast amounts of publicly published data Text Mining and Social Network Analysis have become necessity for analyzing not only information but also the connections across them The main objective is to identify the necessary information as efﬁciently as possible ﬁnding the relationships between available information by applying algorithmic statistical and data ment methods on the knowledge The automation of sentiment detection on these social networks gained attention for various purposes Twitter is social network that allows the user to freely publish short messages called Tweets via the Internet instant messaging or SMS These messages are limited to characters more exactly NFC normalized code points With about million monthly active users as of Twitter Twitter is leading social network which is known for its ease of use for mobile devices of users access the social network via mobile devices Twitter known by the diversity of content as well as its comprehensive list of APIs offered to developers With an average of million messages sent per day the platform seems ideal for tracking opinions on various subjects Furthermore the very short format messages facilitate classiﬁcation since short messages rarely discuss more than one topic However automated interpretation is complicated by embedded links tions and misspellings Facing these challenges is becoming increasingly important for Economic and Market Intelligence in order to successfully recognize trends and threats The frequent expression of negative emotion words on social media been linked to depression The aim of was to report on the associations between depression severity and the variability and instability in emotion word expression on Facebook and Twitter across status updates Several works on depression have emerged They are based on social networks Twitter and Facebook Marechal et Several authors have been interested in the use of emoticons to complete the sentiment analysis Authors in utilize Twitter API to get training data that contain emoticons like and They use these emoticons as noisy labels Tweets with are thought to be positive training data and tweets with are thought to be negative training data In authors present the ESLAM Emoticon Smoothed LAnguage Models which combine fully supervised methods and distantly supervised methods Although many TSA Twitter Sentiment Analysis methods have been presented The authors in explored the inﬂuence of emoticons on TSA Emotion Detection in the Sound Automatic emotion recognition based on utterance level prosodic features may play an important role within emotion recognition The recognition of emotions based on the voice been studied for decades However most of the data collected in controlled environment in which the data are clean without signiﬁcant noise and directly well segmented In addition the majority of such system are In the real world the process is much more plex There are many factors such as background noise and not speech voice like laugh whimper cry sigh which greatly aggravate the results obtained in controlled environment These factors will make the real emotion recognition trained on the data from the controlled environment unsuccessful The author in focused on systems with speech as only input channel proposed FAU Aibo Emotion Corpus which is speech corpus with naturally occurring states FAU Aibo is corpus of spontaneous emotionally colored speech of German children at the age of to years interacting with the Sony Robot Aibo Eleven states are labeled on the word level Best results have been obtained on the chunk level where classwise averaged recognition rate of almost for the problem anger emphatic neutral and motherese been achieved emotion recognition system relying on audio input low ments for hardware Especially the recent emerging of artiﬁcial assistant Google Home Amazon Alexa provides the platform for emotion recognition system Audio Analysis Background Nowadays human speech recognition HSR and automatic speech recognition ASR systems have very wide application This aspect is also referred to the recognition of emotion state Emotion recognition by research human speech is nected with two research areas First is related to synthetic approach which allows generating artiﬁcial speech samples ﬁlled speciﬁc emotions The second issue concerns machine recognition of the speaker s emotions In order to possible machine IT system to learn human emotions expressed out of speech like speech intonation articulation etc By the recognizing emotions from human speech we can notice which kind of emotion is dominant during conversation Possible to recognize emotions state like sadness fear embarrassment terror etc Connected with this machines could help people in making right decisions by recognizing emotions especially in irrational Survey on Multimodal Methods for Emotion Detection situations where decisions have to be made faster than rational performing mind Sometimes it is valuable to artiﬁcially inﬂuence mental and emotional states to get better individual performance in occupations and prevent mental ders from happening Recent research shown that under certain circumstances multimodal emotion recognition is possible even in real time Feature Extraction Sound signals including human speech is one of the main mediums of communication and it can be processed to recognize the speaker or even emotion This diagnosis is possible through signal decomposition and analysis During this analysis key physical features are found which are clearly able to describe the tional background of the speaker Analyzing speech as sound and not the meaning of spoken words is possible eliminate the language barrier by focusing only on the emotional message This can be obtained by calculating the values of descriptors be able to describe such features as the ratio of amplitudes in particular parts of the signal the shape of the signal waveform the frequency distribution etc The basic principle behind emotion recognition lies with analyzing the acoustic difference that occurs when talking about the same thing under different emotional situations The accurate selection of descriptors and their combination allows to determine which emotions are dominant in conversation that is their worth is very important for classiﬁcation It is therefore necessary to index the speech signal to evaluate the speaker s emotions Acquiring features is possible by investigating time domain and frequency domain speech signals This way it is practicable to obtain feature vector which can be able to automatic objects classiﬁcation it means automatic classiﬁcation of emotion state too During research is necessary concentrate on insightful research frequency domain not forgetting the time domain descriptors either There are some physical features applied for indexing speech like spectrum irregularity wide and narrow band spectrograms speech signals ﬁltering and processing enhancement and manipulation of speciﬁc frequency regions segmentation and labeling of words lables and individual phonemes Moreover the Cepstral cients MFCC is widely used in speech classiﬁcation experiments due to its good performance It extracts and represents features of speech signal including kinds of emotions The takes spectral shape with important data about the quality of voice and production effects To calculate these coefﬁcients the inverse cosine transform of decimal logarithm of the Mel ﬁltered spectrum of energy must be done The purpose of improving results of experiment and searching effective spectral feature vector constant spectral resolution is used For the reduction of leakage effect the Hamming window is implemented This is necessary for increasing the efﬁciency of frequency in human speech There is MPEG standard which gives many descriptors deﬁnitions for the physical features of sound including human speech These descriptors are deﬁned on the base of analysis of digital signals and index of most important their factors The MPEG Audio standard contains descriptors and description schemes that can be divided into two classes generic tools and tools MPEG descriptors are being very helpful for assessment features of human speech in Marechal et general sense They can be used to assess the emotions because each emotional state expressed in speech contains speciﬁc numeric values of MPEG descriptors After the extraction of features the machine learning part of the generic system should analyze features and ﬁnd statistical relationships between particular features and emotional states This part of the system is also called classiﬁer Most commonly used classiﬁcation algorithms are Artiﬁcial Neural Networks ANN Neighbor and Support Vector Machines SVM decision trees Furthermore probabilistic models such as the Gaussian mixture model GMM or stochastic models such as Hidden Markov Model HMM can be applied Accuracy Emotion analysis of speech is possible however it highly depends of the language Automatic and universal emotion analysis is very challenging Analyze of emotion is more efﬁcient when perform for one dedicated language Study by Chaspari et showed that emotion classiﬁcation in speech Greek language achieved accuracy up to Similar study by Arruti et showed mean accuracy of emotion recognition rate in Basque and in Spanish Emotion in Image and Video Nonverbal behavior constitutes useful means of communication in addition to spoken language It allows to communicate even complex social concepts in various contexts thus may be regarded not only supplementary but also as basic way of emotion recognition It is possible to easily reading the affects emotions and intentions from face expressions and gestures Indeed Ekman one of the pioneers in the study of facial expressions and emotions identiﬁes at least six characteristics from posed facial actions that enable emotion recognition morphology symmetry duration speed of onset coordination of apexes and ballistic trajectory They are common to all humans conﬁrming Darwin s evolutionary thesis Therefore an emotional recognition tools based on facial video is universal However automatic and either automatic semi recognition of the meaning of face expressions and gesture still constitutes true challenge We will focus on facial expressions Automatic Facial Emotion The human face as modality for emotion detection takes dominant position in the study of affect It is possible to register the facial information automatically in without requiring any specialized equipment except simple video camera Thus facial expressions as noninvasive method are used in behavioral science and in clinical Therefore automatic recognition of facial expressions is an important component for emotion detection and modeling the natural interfaces The face is equipped with large number of independent muscles which can be activated at different levels of intensity Consequently the face is capable of generating high number of complex dynamic facial expression patterns There are three main dimensions of facial variation morphology complexion and dynamics As ﬁrst two Survey on Multimodal Methods for Emotion Detection dimensions morphology and complexion deals with static dimensions there are very useful for facial recognition but their role for emotion analyze is not signiﬁcant The third one the dynamics play the most important role for emotion face analysis Although mapping of facial expressions to affective states is possible the automatic recognizing of humans emotion from the facial expressions without effort or delay is still challenge Automatic detection of emotions from facial expressions are not simple and their interpretation is largely To reduce the complexity of automatic affective inference measurement and interpretation of facial expressions Ekman and Friesen developed in special system for objectively measuring facial movement the Facial Action Coding System FACS FACS based on system originally developed by Swedish anatomist named Hjortsjö became the standard for identifying any movement of the face Later Ekman and Sejnowski studied also computer based facial measurements FACS is common standard to systematically categorize and to index the physical expression of emotions The basic values of FACE are Action Units AUs AUs are the fundamental actions of individual muscles or groups of muscles The AUs are grouped in several categories and identiﬁed by number main codes head movement codes eye movement codes visibility codes and gross behavior codes The intensities of AUs are annotated in ﬁve categories by appending letters to the AUs for minimal B for slight intensity C for marked D for severe for maximum intensity For example signiﬁes the weakest trace of and is the maximum intensity possible of for the individual person The eyes and mouth have high importance to emotion detection therefore to successfully recognize an emotion the observations mostly rely on the eye and mouth regions Furthermore the actions of eyes and mouth allowed grouping the expressions in continuous space ranging from sadness and fear reliance on the eyes to disgust and happiness mouth Combining these observations with facial AUs increase knowledge about the areas involved in displaying each emotion For example happy denoted by comprises Cheek Raiser and Lip Corner Puller whereas sad comprises Inner Brow Raiser Brow Lowerer and Lip Corner Depressor The computer algorithm for facial coding extracts the main features of the face mouth eyebrows etc and analyzes movement shape and texture composition of these regions to identify facial action units AUs Therefore it is possible to track tiny movements of facial muscles in individuals face and translate them into universal facial expressions like happiness surprise sadness anger and others Very important is also the way how the emotional representation is created Afzal et performed studies differences between human raters judgments of emotional expressions and three automatically generated representations in video clips First the display was created from the output of commercial automatic on black background Second one stick ﬁgure models which connecting the automatically tracked using straight lines and sketching eyes using typical shape The last one XFace Animation was animated facial expressions and displays created using XFace open source toolkit Their experiences conﬁrmed that Marechal et the human judgement is still the best one with the highest recognition rates followed by models displays and then XFace animations Existing Tools for Automatic Facial Emotion Recognition Facial emotion recognition is one of the most important methods for nonverbal emotion detection Several popular commercial packages offer speciﬁc facial image analysis tasks including facial expression recognition facial attribute analysis and face tracking We cite below few examples IntraFace IF publicly available software package offering automated facial feature tracking head pose estimation facial attribute recognition facial expression analysis from video unsupervised synchrony detection to discover correlated facial behavior between two or more persons It also measure an audience reaction to talk given or synchrony for smiling in videos of interaction The Emotion API Microsoft Azure Microsoft Azure proposes API to recognize emotions in images and in videos The Emotion API permits input data directly as an image or as bounding box from Face API In the output it returns the conﬁdence across set of eight emotions anger contempt disgust fear happiness neutral sadness and surprise Emotion API is able to track how person or crowd responds to your content over time Emotion API provide the interface for C cURL Java JavaScript PHP Python Ruby It is also possible to implement these API in R Micro Expressions Training Tool Ekman created several training tools to enhance understanding of emotions and relationships Micro Expressions Training Tool METT and intensive MEITT to teach how to detect and interpret micro expressions Subtle Expressions Training Tool SETT to learn how to see emotions as they develop SETT provides foundational knowledge of how emotions emerge in just one region on the face Micro Expressions Proﬁle Training Tool MEPT you to identify micro expressions from different angles Emotion Detected by the Physiological and Motor Signals Recently the physiological and motor data are accessible by IoT technology People are interested in purchasing connected objects in order to monitor their healthy like heart rate blood pressure number of burned calories and analyze their movements We can ﬁnd lot work for healthcare applications but at the same time this technology can be Survey on Multimodal Methods for Emotion Detection used to emotion detection As an example we can cite work done by Amira et as good example of using the emotion analyze for healthcare purpose This work takes into consideration the emotional state of the peoples stress happiness sadness among and analyze that using the appropriate AI tools to detect emotion to categorize it and then analyze its impact on cardiovascular disease Physiological Signals Automatic emotion recognition based on physiological signals is key topic for many advanced applications safe driving security mHealth Main analyzed logical signals useful for emotion detection and classiﬁcation are electromyogram EMG recording of the electrical activity produced by skeletal muscles galvanic skin response GSR reﬂecting skin resistance which varies with the state of sweat glands in the skin controlled by the sympathetic nervous system where conductance is an indication of psychological or physiological state respiratory volume RV referring to the volume of air associated with different phases of the respiratory cycle skin temperature SKT referring to the ﬂuctuations of normal human body temperature blood volume pulse BVP measures the heart rate heart rate HR electrooculogram EOG measuring the standing potential between the front and the back of the human eye photoplethysmography PPG measuring blood volume pulse BVP which is the phasic change in blood volume with each heartbeat etc The recognition of emotions based on physiological signals covers different aspects emotional models methods for generating emotions common emotional data sets characteristics used and choices of classiﬁers The whole framework of emotion recognition based on physiological signals recently been described by weight fusion strategy for emotion recognition in multichannel physiological signals using database been recently described Various classiﬁcation tools may be used including artiﬁcial neural networks ANN support vector machine SVM neighbors KNN and many more More advanced emotion representation models including clustering of responses are created for purposes of further studies and increased recognition accuracy Özerdem and Polat used EEG signal discrete wavelet transform and machine learning techniques multilayer perceptron neural network MLPNN and neighbors kNN algorithms Research by Jang et used ECG EDA SKT and more sophistical machine learning ML algorithms linear discriminant analysis LDA ﬁnds linear combination of features that characterizes or separates two or more classes of objects or events classiﬁcation and regression trees CART uses decision tree as predictive model to go from observations about an item represented in the branches to clusions about the item s target value represented in the leaves Marechal et map SOM type of artiﬁcial neural network that is trained using unsupervised learning to produce discretized representation of the input space of the training samples called map Naïve Bayes algorithm based on applying Bayes theorem with strong independence assumptions between the features Support Vector Machine SVM supervised learning models with associated learning algorithms analyzing data used for classiﬁcation and regression analysis Kortelainen et showed results of combining physiological signals heart rate ability parameters respiration frequency and facial expressions were studied Motor Signals Body posture and movement is one of the most expressive modalities for humans Researchers have recently started to exploit the possibilities for emotion recognition based on different motor signals Zachartos et present well done analyze of emerging techniques and modalities related to automated emotion recognition based on body movement and describes application areas and notation systems and explains the importance of movement segmentation At the same time this work outlines that this ﬁeld still requires further studies To recognize the emotion different postural kinematic and geometrical feature are used Tsui et use the keystroke typing patterns usually on standard keyboard for automatic recognizing of emotional state Li et proposed another way to solve this problem by analyzing the human pattern of movement of the limbs gait recorded by Microsoft Kinect The gait pattern for every subject was extracted from coordinates of main body joints using Fourier transformation and Principal Component Analysis PCA To classify signals features four ML algorithms were trained and evaluated Naive Bayes described above Random Forests an ensemble learning method for classiﬁcation regression and other tasks that operate by constructing multitude of decision trees at training time and outputting the class that is the mode of the classes classiﬁcation or mean prediction regression of the individual trees LibSVM an integrated software for support vector classiﬁcation regression and distribution estimation Sequential Minimal Optimization SMO is an algorithm for solving the quadratic programming problem arising during the training of support vector machines They showed that human gait reﬂects the walker s emotional state Accuracy Accuracy of the emotion recognition and classiﬁcation based on physiological signals identiﬁcation have improved signiﬁcantly Research by Goshvarpour et using HRV and PRV and fusion rules feature level decision level showed classiﬁcation rates improved up to sensitivity speciﬁcity Previous studies have provided evidence for general recognition rate from to for different emotional states that more higher set of analyzed signals and more recognized tions provides better results recognition accuracy was also in research by Jang et Survey on Multimodal Methods for Emotion Detection Özerdem and Polat used EEG signal resultant average overall accuracy was using MLPNN and using kNN The highest accuracy achieved by Li et was Accuracy for combining heart rate variability respiration frequency and facial expressions was relatively low Results of the Medical Natural Language Processing Challenge showed that the best of participants teams achieved accuracy of It should be noted that all algorithm must be personalized to each person in order to be reliable Related Project openSMILE Speech Music Interpretation by Extraction Originally created in the scope of the European research project SEMAINE http OpenSMILE is modular and ﬂexible feature extractor for signal processing and machine learning applications However due to their high degree of abstraction openSMILE components can also be used to analyze signals from other modalities such as physiological signals visual signals and other physical sensors given suitable input components openEAR Emotion and Affect Recognition It consists of three major components the core component is the SMILE Speech and Music Interpretation by Extraction signal processing and feature extraction tool which is capable generating k features in Factor RTF either from live audio input or from ofﬂine media The advantages of emotion recognition framework providing an extensible platform independent feature extractor implemented in models on six databases which are for emotion and affect recognition and supporting scripts for model building evaluation and visualization This framework is compatible with related such as HTK and WEKA by supporting their this solution are Interactive Emotion Games for Children with Autism Understand and express emotions through facial expressions vocal intonation and body gestures This project aims to create an platform that will assist children with Autism Spectrum Condition ASC to improve their communication skills The project will attend both the recognition and the expression of cues aiming to provide an where to give scores on the prototypically and on the naturalness of child s expressions It will combine several technologies in one comprehensive virtual world environment bining voice face and body gesture analysis providing corrective feedback regarding the appropriateness of the child s expressions Marechal et INTERSPEECH Computational Paralinguistics ChallengE ComParE In this Challenge authors introduced four paralinguistic tasks which are important for the realm of affective interaction yet some of them go beyond the traditional tasks of emotion recognition Thus as milestone ComParE laid the foundation for successful series of ComParEs to date exploring more and more the paralinguistic facets of human speech in tomorrow s information communication and entertainment systems Conclusion In this paper we presented existing multimodal approaches and methods of emotion detection and analysis Our study includes emotion analyze in text in sound in and physiological signals We showed that automated emotion analysis is possible and can be very useful for improving the exactness of the reaction and making possible to anticipate the emotional state of the interlocutor more quickly Not only basic emotions can be analyzed but also the cognitive assessment terpretation of stimuli in the environment and their physiological response Such an approach may cause quicker development of more systems and ronments supporting everyday life and work Automatic emotion analysis requires advanced recognition and modeling very often based on artiﬁcial intelligence systems Presented approach may be successful but the limitations of the current knowledge and experience still concern tools for automatic emotion measurement and analysis We should be aware that among requirements on automatic emotion recognition key might constitute portability and low price Novel intelligent systems may be friendlier preventing the computers from acting inappropriately In this state of the arts we have addressed of course only some most important issues doubt there is need for further effort of scientists and engineers toward more advanced automatic emotion recognition systems Further research may constitute an important part of future technological clinical and scientiﬁc progress Acknowledgement COST PHC Polonium This state of the art was published in ation with the COST Action Modelling and Simulation for Big Data Applications cHiPSet supported by COST European Cooperation in Science and Technology This article is based upon the work done under the project PHC POLONIUM JECT realized between the AlliansTIC Research Laboratory of Efrei Paris Engineering School France and the Institute of Mechanics and Applied Computer Science Kazimierz Wielki University in Bydgoszcz Poland Survey on Multimodal Methods for Emotion Detection References Mehrabian Ferris Inference of attitudes from nonverbal communication in two channels J Consult Psychol Mood Ring Monitors Your State of Mind Chicago Tribune October at Ring Buyers Warm Up to Quartz Jewelry That Is Said to Reﬂect Their Emotions The Wall Street Journal October at and Ring Around the Mood Market The Washington Post November at Picard Affective Computing MIT Press Cambridge Picard Vyzas Healey Toward machine emotional intelligence analysis of affective physiological state IEEE Trans Pattern Anal Mach Intell Hernandez et AutoEmotive bringing empathy to the driving experience to manage stress In DIS June Vancouver BC Canada ACM http Zadeh Zellers Pincus Morency Multimodal sentiment intensity analysis in videos facial gestures and verbal messages IEEE Intell Syst https Wöllmer et YouTube movie reviews sentiment analysis in an context IEEE Intell Syst Mihalcea Morency multimodal sentiment analysis In ACL vol pp Zadeh Chen Poria Cambria Morency Tensor fusion network for multimodal sentiment analysis In Proceedings of the Conference on Empirical Methods in Natural Language Processing September Copenhagen Denmark pp Association for Computational Linguistics Poria Cambria Hazarika Majumder Zadeh Morency dependent sentiment analysis in videos In Proceedings of the Annual Meeting of the Association for Computational Linguistics vol pp Poria Cambria Howard Huang Hussain Fusing audio visual and textual clues for sentiment analysis from multimodal content Neurocomputing Part https ISSN Liu Sentiment analysis and opinion mining Synth Lect Hum Lang Technol Pang Lee Opinion mining and sentiment analysis J Found Trends Inf Retrieval Dziczkowski RRSS rating reviews support system purpose built for movies recommendation In Szczepaniak eds Advances in Intelligent Web Mastering Advances in Soft Computing vol pp Springer Berlin https Dziczkowski An autonomous system designed for automatic detection and rating of ﬁlm Extraction and linguistic analysis of sentiments In Proceedings of WIC Sydney Dziczkowski Tool of the intelligence economic recognition function of reviews critics In ICSOFT Proceedings INSTICC Press Kepios Digital in essential insights into internet social media mobile and ecommerce use around the world April https Ghiassi Skinner Zimbra Twitter brand sentiment analysis hybrid system using analysis and dynamic artiﬁcial neural network Expert Syst Appl Marechal et Zhou Tao Yong Yang Sentiment analysis on tweets for social events In Proceedings of the IEEE International Conference on Computer Supported Cooperative Work in Design CSCWD June pp Salathé Vu Khandelwal Hunter The dynamics of health behavior sentiments on large online social network EPJ Data Sci https Sriram Fuhry Demir Ferhatosmanoglu Demirbas Short text classiﬁcation in Twitter to improve information ﬁltering In Proceedings of the in Information International ACM SIGIR Conference on Research and Development Retrieval July pp http Seabrook Kern Fulcher Rickard Predicting depression from emotion dynamics longitudinal analysis of Facebook and Twitter status updates Med Internet Res https Wang Hernandez Newman Bian Twitter analysis studying US weekly trends in work stress and emotion Appl Psychol Reece Reagan Lix Dodds Danforth Langer Forecasting the onset and course of mental illness with Twitter data Unpublished manuscript https Park Lee Shablack et When perceptions defy reality the relationships between depression and actual and perceived Facebook social support J Affect Disord Burke Develin Once more with feeling supportive responses to social sharing on the ACM Conference on Computer Supported In Proceedings of Facebook Cooperative Work pp Go Bhayani Huang Twitter sentiment classiﬁcation using distant supervision Proj Stanford Liu Li Guo Emoticon smoothed language models for Twitter sentiment analysis In AAAI Bougueroua Yu Zhong Explore the effects of emoticons on Twitter sentiment analysis In Proceedings of Third International Conference on Computer Science Engineering CSEN August Dubai UAE Bitouk Verma Nenkova spectral features for emotion recognition Speech Commun Busso et Analysis of emotion recognition using facial expressions speech and multimodal information In Sixth International Conference on Multimodal Interfaces ICMI October State College PA pp ACM Press Dellaert Polzin Waibel Recognizing emotion in speech In International Conference on Spoken Language ICSLP October Philadelphia PA USA vol pp Lee et Emotion recognition based on phoneme classes In International Conference on Spoken Language Processing ICSLP October Jeju Island Korea pp Deng Xu Zhang Frühholz Grandjean Schuller Fisher kernels on features for speech emotion recognition In Jokinen Wilcock G eds Dialogues with Social Robots LNEE vol pp Springer Singapore https Steidl Automatic classiﬁcation of user states in spontaneous children s speech thesis Erlangen Lugovic Horvat Dunder Techniques and applications of emotion recognition in speech In MIPRO Survey on Multimodal Methods for Emotion Detection Kukolja Popović Horvat Kovač Ćosić Comparative analysis of emotion estimation methods based on physiological measurements for applications Int Stud Davletcharova Sugathan Abraham James Detection and analysis of emotion from speech signals Procedia Comput Sci Tyburek Prokopowicz Kotlarz Fuzzy system for the classiﬁcation of sounds of birds based on the audio descriptors In Rutkowski Korytkowski Scherer Tadeusiewicz Zadeh Zurada eds ICAISC LNCS LNAI vol pp Springer Cham https Tyburek Prokopowicz Kotlarz Michal Comparison of the efﬁciency of time and frequency descriptors based on different classiﬁcation conceptions In Rutkowski Korytkowski Scherer Tadeusiewicz Zadeh Zurada eds ICAISC LNCS LNAI vol pp Springer Cham https Chaspari Soldatos Maragos The development of the Athens Emotional States Inventory AESI collection validation and automatic processing of emotionally loaded sentences World Biol Psychiatry Arruti Cearreta Alvarez Lazkano Sierra Feature selection for speech emotion recognition in Spanish and Basque on the use of machine learning to improve interaction PLoS ONE Ekman Facial expression and emotion Am Psychol Jack Schyns The human face as dynamic tool for social communication Curr Biol Rev https Ekman Friesen Hager Facial action coding system Research Nexus Network Research Information Salt Lake City Hjorztsjö Man s face and mimic language https Ekman Huang Sejnowski et Final report to NSF of the planning workshop on facial expression understanding vol Human Interaction Laboratory University of California San Francisco Afzal Sezgin Gao Robinson Perception of emotional expressions in different representations using facial feature points IEEE http Licensed from Google Torre Chu Xiong Vicente Ding Cohn IntraFace In IEEE International Conference on Automatic Face and Gesture Recognition Workshops https https http https Amira Dan et Monitoring chronic disease at home using connected devices In Annual Conference on System of Systems Engineering SoSE pp IEEE Shu et review of emotion recognition using physiological signals Sensors Basel Wei Jia Feng Chen Emotion recognition based on weighted fusion strategy of multichannel physiological signals Comput Intell Neurosci Özerdem Polat Emotion recognition based on EEG features in movie clips with channel selection Brain Inform Jang Park Park Kim Sohn Analysis of physiological signals for recognition of boredom pain and surprise emotions Physiol Anthropol Marechal et Kortelainen Tiinanen Huang Li Laukka Pietikäinen Seppänen Multimodal emotion recognition by combining physiological signals and facial expressions preliminary study In Conference Proceeding of the IEEE Engineering in Medicine and Biology Society vol pp Zacharatos Gatzoulis Chrysanthou Automatic emotion recognition based on body movement analysis survey IEEE Comput Graph Appl Tsui Lee Hsiao The effect of emotion on keystroke an experimental study using facial feedback hypothesis In Conference Proceedings of the IEEE Engineering in Medicine and Biology Society pp Li Cui Zhu Li Zhao Zhu Emotion recognition using Kinect motion capture data of human gaits PeerJ Goshvarpour Abbasi Goshvarpour Fusion of heart rate variability and pulse rate variability for emotion recognition using lagged poincare plots Australas Phys Eng Sci Med Khezri Firoozabadi Sharafat Reliable emotion recognition system based on dynamic adaptive fusion of forehead biopotentials and physiological signals Comput Methods Programs Biomed Gouizi Bereksi Reguig Maaoui Emotion recognition from physiological signals Med Eng Technol Verma Tiwary Multimodal fusion framework multiresolution approach for emotion classiﬁcation and recognition from physiological signals Neuroimage Part Yang Willis Roeck Nuseibeh hybrid model for automatic emotion recognition in suicide notes Biomed Inform Insights Suppl Eyben Weninger Wöllmer Shuller Media Interpretation by Large Extraction November openSMILE by audFERING Eyben Wöllmer Shuller openEAR introducing the munich emotion and affect recognition toolkit In International Conference on Affective Computing and Intelligent Interaction and Workshops https Reilly et The stimulus set validation study Behav Res https Psychonomic Society Schuller et Affective and behavioural computing lessons learnt from the ﬁrst computational paralinguistics challenge Comput Speech Lang Elsevier ScienceDirect Open Access This chapter is licensed under the terms of the Creative Commons Attribution International License http which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give priate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this chapter are included in the chapter s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the chapter s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder Forecasting Cryptocurrency Value by Sentiment Analysis An Survey of the in the Cloud Aleˇs B Joana Matos Vincenzo Katarzyna Horacio Roman Juan Imen Claudia Tudor Ioan and Andrea Faculty of Electrical Engineering and Computer Science University of Maribor Koroˇska cesta Maribor Slovenia Department of Computing Science and Mathematics University of Stirling Stirling UK abb Department of Telematic Engineering University of Vigo Campus Universitario Vigo Spain Faculty of Economics University of Coimbra Av Dias da Silva Coimbra Portugal joana Allianstic Research Laboratory EFREI Paris Ecole d Informatique et technologies du Avenue Villejuif France Cloud Competency Centre National College of Ireland Mayor Street Lower IFSC Dublin Ireland horacio Faculty of Applied Informatics Tomas Bata University in Zlin Masaryka Zlin Czech Republic senkerik Computer Science Department Technical University of Baritiu Street Romania Abstract This chapter surveys the in forecasting tocurrency value by Sentiment Analysis Key compounding perspectives of current challenges are addressed including blockchains data tion annotation and ﬁltering and sentiment analysis metrics using data streams and cloud platforms We have explored the domain based on this metric perspective as technical analysis ing and estimation using standardized technology The envisioned tools based on forecasting are then suggested ranking c The Author s Ko lodziej and Eds cHiPSet LNCS pp https Zamuda et Initial Coin Oﬀering ICO values for incoming cryptocurrencies ing strategies employing the new Sentiment Analysis metrics and risk aversion in cryptocurrencies trading through portfolio selection Our perspective is rationalized on the perspective on elastic demand of computational resources for cloud infrastructures Keywords Cryptocurrency Blockchain Sentiment Analysis Forecasting ICO CSAI Cloud computing Introduction This chapter presents position survey on the overall objective and speciﬁc challenges encompassing the state of the art in forecasting cryptocurrency value by Sentiment Analysis The compounding perspectives of current challenges are addressed such as the blockchain technologies underlying data collection of items from social media the annotation and ﬁltering of such items and the Sentiment Analysis as resulting metric of the observed data streams We describe Cryptocurrency Sentiment Analysis Indicator CSAI and identify its required inner workings compounding perspectives Further ities are then explored based on this new metric perspective such as technical analysis forecasting and beyond The envisioned tools based on forecasting are then suggested ranking Initial Coin Oﬀering ICO values for incoming tocurrencies trading strategies employing the new Sentiment Analysis metrics and risk aversion in cryptocurrencies trading through portfolio selection Since the introduction of Bitcoin and rise of algorithms and technologies there been signiﬁcant increase in their recognition and analysis In this chapter we focus on the speciﬁc aspect of value for the blockchain projects related to cryptocurrencies While Computing HPC and Cloud Computing are not sine qua non for cryptocurrencies their use become pervasive in their action veriﬁcation mining Cryptocurrencies rely on powerful computational nodes to verify transactions convert them into groups blocks and add them to the blockchain Such veriﬁcation is based on complex cryptology algorithms ergo the term cryptocurrencies which assure user anonymity and payment untraceability With the convergence of HPC and clouds elastic computational resource utilization become commonplace in diﬀerent domains including of course Cryptocurrencies Moreover recent survey on open challenges and trends in cloud computing recognizes blockchain as disruptive inﬂuencer of the ﬁeld In the following section related work is provided introducing sentiment ysis cryptocurrencies and their value forecasting Section highlights for the interested reader more about cryptocurrencies and blockchain in general and lists an example blockchain application for energy markets After Sect ciﬁc challenges are addressed listing the speciﬁc perspectives Section covers inﬂuence pertaining to social media Sect social media data annotation and Forecasting Cryptocurrency Value by Sentiment Analysis sentiment dictionary Sect Filtering of Tweets Sect the perspective on core resulting Sentiment Analysis for cryptocurrencies Sect technical ysis Sect ranking of ICOs Sect portfolio selection using optimization and Sect investment approaches Then the Conclusion section summarizes the surveyed perspectives Related Work As this chapter focuses on the objective of Sentiment Analysis for cies the more recent related work leading to the formation of this objective is initially presented in this section Namely when analyzing the impact of tocurrencies there are diﬀerent possibilities on which to focus like prediction of value or some other underlying principles and features of technologies enabling these cryptocurrencies We ﬁrst focus on the Bitcoin cryptocurrency and the trading volumes of coin as introduced using Google Search Engine as the media feed In search engine query trends are predicted for Bitcoin connection between coin search queries on Google Trends and Wikipedia is established in where their relationship is also studied An attempt to explain Bitcoin prices and tion rates using Google Search is made in Returns and volatility tive when using Bitcoin volume analysis is discussed in text mining from an online forum which analyzes user opinions and aims to predict value tions for Bitcoin is reported in survey of methods and models for textual sentiment in ﬁnance is presented in Dynamic topic modeling for rency community forums is introduced in crowdsourced perspective is presented in listing future directions in international ﬁnancial integration research For Bitcoin value formation model based on an empirical study is given in In high frequency volatility is forecasted for cies Some of the underlying determinants including technology and economic factors inﬂuencing exchange rates for cryptocurrencies are presented for the case of Bitcoin in cryptocurrency price prediction using news and social media sentiment was ﬁrst introduced in followed shortly by the predicting of cryptocurrency price bubbles using social media data and epidemic ing in The popular modern techniques from Artiﬁcial Intelligence and general softcomputing paradigms have been utilized in recent work where price prediction based on historical price values was compared over machine learning with diﬀerent types of neural networks and further in where the prediction of alternative cryptocurrency price ﬂuctuations using gradient boosting tree model is given In of tocurrency market returns and social media topics is investigated whereas the paper reports recent study considering the impact of social media on coin value Finally study measuring the interaction between media sentiment based on news articles as well as blog posts and the Bitcoin price is given in Another broader analysis used to study the general ships between Bitcoin prices and fundamental economic variables technological Zamuda et factors and measurements of collective mood derived from Twitter feeds was presented in An example type of media data feeds content inﬂuencing cryptocurrency sentiment are social media channels posts about ransomware and hacker attacks mostly inﬂuencing the sentiment as negative sentiment posts The cybersecurity threads computer malware ransomware virus worm trojan horse retrovirus botnet etc development and their impact on society and ﬁnancial markets are signiﬁcant topic today Hackers enjoy anonymity under blockchain technology that may have high social cost For example the paper shows that substantial amount of Bitcoin blockchain operations and addresses are involved in ransomware money processing Ransomware represents type of software where the hackers take advantage of victims operating system vulnerabilities to deploy dedicated ransomware code Once it happened the major harmful activity lies in the encryption of ﬁles with tain extensions that are expected to be highly personally important documents photos The victim still can access boot the operating system usually ting message specifying the amount of ransom in cryptocurrency the hacker s wallet address and the time left to pay the ransom in exchange to get the tion key On the other hand the hacker attacks are more frequent and easier to perform The victim usually receives an email stating some mix of technical info and blackmailing based description of situation that the hacker takes over the control of web browser email and social networks applications remotely and will send some private information to all gathered contacts unless certain amount in cryptocurrency is paid within given time slot Both of the cybersecurity threads mentioned above are inﬂuencing the blockchain value and should be taken into consideration in any future developed forecasting sentiment aggregator to achieve better accuracy The reasons and relations are given in an original work where the author investigates how the cybersecurity shocks aﬀect the demand for blockchain settlement transaction fees mining reward and cryptocurrency exchanges That paper also explores in detail the theory given in and their ﬁndings show that sudden shocks to the exogenous demand for blockchain settlement are resulting in an increase in transaction fees and reduction in the endogenous demand Following the ongoing research and published works listed in this section the perspectives compounding the presented research topic of this chapter are ﬁed in Sects as speciﬁc challenges in sentiment analysis for cryptocurrency value forecasting Background Cryptocurrencies and Blockchain Widely considered immutable data structures blockchains ment networks where participants can verify interactions rently using decentralized consensus protocols As an emerging nology trend diﬀerent research and industrial perspectives are being assembled to document its potential disruptive impact Forecasting Cryptocurrency Value by Sentiment Analysis Blockchains have ﬁve unique characteristics namely communication without central authority Transparent transaction processing with ownership Decentralized transaction history veriﬁable by all participants Immutability of records assuring chronological sequence and accessibility processing to trigger algorithms and events The aforementioned characteristics have made blockchain particularly able to manage cryptocurrencies Electronic cash systems administered via consensus Indeed the most widely known for cryptocurrency the coin remains something like the gold Standard for ﬁnancial blockchain cations Nonetheless while blockchains have been used extensively in ﬁnancial entities their decentralized immutability characteristics have made them ticularly suitable for applications in other domains as diverse as Law Food Traceability and Software Management To highlight the importance of blockchain technologies evaluation an example in surrounding blockchain technologies for Energy markets follows in the next subsection Blockchain Technologies for Energy Markets The energy grid is moving to new shifting from centralized like energy systems to decentralized smart energy systems by incorporating large number of Distributed Energy Prosumers DEP The advent of intermittent decentralized renewable energy sources is completely changing the way in which electricity grids are managed supporting the shift to more decentralized smart energy systems Variations in energy production either plus or deﬁcit may threaten the security of supply leading to energy tion systems overload and culminating in power outages or service disruptions forcing the DEPs to shed or shift their energy demand to deal with peak load periods The centralized energy systems take limited account of local conditions and are diﬃcult to be optimized and oﬀer incentives for consumers to manage and adjust their consumption according the generation proﬁles In this context the eDREAM project contributes to the transformation of the ditional energy systems into novel decentralized and ones by leveraging on blockchain technology to exploit local capacities and constraints fully at level to preserve the continuity and security of supply at aﬀordable costs at smart grid level The grid is modeled as collection of DEPs resources able to coordinate through blockchain based infrastructure to port fully decentralized management control and stable grid operation The eDREAM project presents blockchain decentralized management relying on the implementation of distributed ledger for storing DEPs energy data at the level All monitored energy data recorded at the level of DEP are registered and stored as immutable transactions Therefore the individual energy production or Zamuda et energy consumption values are aggregated in blocks which are then replicated in the ledger set of smart contracts for decentralized energy management and control Through speciﬁc mechanisms these contracts enable the of energy among DEPs and matching and tralized coordinated control for energy stakeholders such as the DSO tribution System Operator The contracts are able to assess and trace the share of contracted ﬂexibility services actually activated in by the aggregators from their enrolled prosumers Consensus based validation for transactions validation and ﬁnancial ment The project oﬀers the solution of novel validation that goes in the direction of increased reliability of the smart grid system operation better energy incentives for DEPs and increased usage of able energy Three types of smart management scenarios are supported with smart contracts i The provisioning of energy ﬂexibility services by the DSO leveraging on aggregators ii The implementation of decentralized green energy market at the level promoting the consumption of renewable energy where it is produced and iii The creation of community based tion of prosumers allowing them to participate in the national energy or capacity market The Provisioning of Flexibility Services supposes that prosumers are able to oﬀer and trade their ﬂexibility in terms of loads modulation They are involved via enabling aggregators or directly with the DSO via direct Demand Response and control of DEP s energy assets Using smart contracts the DSO is able to assess and trace the share of contracted ﬂexibility services actually activated in by the aggregators from their enrolled prosumers at the grid level At the same time the smart contracts act as decentralized control mechanism used to manage the levels of energy ﬂexibility from aggregators and enrolled prosumers on one side and from aggregators to the DSO on the other side associating incentive and penalties rates If relevant deviations between the expected energy ﬂexibility request and the actual delivered ﬂexibility are detected by smart contracts speciﬁc actions are taken to rebalance the energy demand with the energy production The Decentralized Green Energy Market designed at the level enacts any prosumer to participate and trade energy directly The market acts as management mechanism by rewarding the consumption of renewable energy when it is available leveraging on green energy tokens making sure that the potential energy imbalances at the level are addressed locally and not exported to higher smart grid management levels The fungible tokens are generated at rate proportional with the forecast renewable energy production transforming the energy in transactable digital asset The producers and consumers use the generated tokens to participate in the tricity market sessions and leverage on smart contracts to submit energy and transact energy in fashion Forecasting Cryptocurrency Value by Sentiment Analysis The Creation of Virtual Power Plants VPP addresses the increasing need to optimize the output from multiple local generation assets small hydro photovoltaic generators etc that serve primarily local communities and have export connections at the power distribution network The beneﬁts behind creating such coalitions are that mix of diﬀerent energy generation resources which have diﬀerent energy generation models and scale may be interested to cooperate in convenient way with view to achieving smart grid sustainability objectives The VPP aims at maximizing the utilization and revenues from RES and classic generation sources through accessing diﬀerent markets as an aggregated portfolio bringing its capacity to the optimum paying service at any time The DEPs can ultimately participate on other energy markets such as ﬂexibility service provider to TSO Transmission System Operator or wholesale capacity provider on the wholesale or capacity market The adoption of the above presented blockchain based management approaches will transform the smart grid into democratic community that longer relies on central authority but can take any decision through smart contracts rules enforced and veriﬁed by each DEP of the grid At the same time it is in line with the European strategic vision of creating secure and able energy system by and reducing the greenhouse gas emissions by at least Speciﬁc Challenge Inﬂuence on Social Media Social networks are enabling people to interact and are their human relations to the virtual world People utilize these media platforms for diﬀerent activities to express their opinions and their sentiments to share their experiences to react to another person We can observe in this space diﬀerent human interactions and we can deﬁne many roles related to relations between diﬀerent users We can observe among others inﬂuential trusted or popular individuals These roles of users in social networks are signiﬁcant and useful in various ciplines such as Economy Finance Marketing political and social campaigns and recommendations Deﬁning distinguishing and measuring the strength of those relations becomes challenging both in the theoretical and practical ﬁelds The roles of trusted or inﬂuential users users with high reputation and popular users can be used in various ways An interesting work to deﬁne classify and present hierarchy of all the terms was done by Rakoczy In this survey we are interested in particular in inﬂuence approach the relation which as we suggest the strongest correlation between social media and the rency tendency and in general with the ﬁnancial trends The research involving inﬂuence and inﬂuential users is an important part of social network analysis The term inﬂuence is used widely and intuitively means the capacity to inﬂuence the character development or behavior of someone or something or the eﬀect itself We name inﬂuencer the person exerting the inﬂuence action Zamuda et Although this intuitive meaning is well understood in social network analysis inﬂuence seems to be an ambiguous term that depends strictly on the presumed assumptions For instance Kempe et focused on inﬂuence in the mation ﬂow and spread sense On the other hand other works and targeted the quantity aspect of inﬂuence targeting mainly users in central positions in social network The analysis of inﬂuence is an interdisciplinary domain involving not only social network analysis but also social sciences Sociology use of graph theory statistics and others The existing methods considering inﬂuence regard this topic in two diﬀerent approaches which are i Inﬂuential users discovery and ii Inﬂuence diﬀusion information spread within the network with particular focus on the most optimized way to diﬀuse the information in social network mally Inﬂuential Users Discovery The methods for inﬂuential users discovery try to ﬁnd users who have an impact on the network and the users who in some way structurally by modifying the behavior adding information to network etc try to answer the following tion Which users are in such position and are making such diﬀerence to other users that the structure of the network behavior actions or preferences of other users is changed because of this inﬂuential activity Here we can ﬁnd two approaches Based only on the central position of the user approaches Degree centrality closeness centrality and betweenness centrality or more complex which takes into account more aspects like user s tory content and other inﬂuence properties Many methods which used topology criteria are based on PageRank Alternatively there are also some works that have used additional tion provided directly from users about the topic namely hashtags RetweetRank and MentionRank are examples of such approaches that are using tag information in order to group users together via subject There have been approaches to provision cloud resources elastically based on social media tially for disaster situations These methods are similar using as base for the network either retweets or mentions Additionally to the works presented above there are obviously other works that also deal with the inﬂuence ation In the famous work of Hirsh the author presents metric that aims to calculate the inﬂuence of researches based on individual s publications Inﬂuence Diﬀusion and Inﬂuence Maximization The issue of inﬂuence diﬀusion is general problem of how the information spreads within network where users nodes are connected by edges which signify the inﬂuence The best known methods are the Linear Threshold LT model the modiﬁed LT named Delayed LT DLT model or Independent Cascade IC model Forecasting Cryptocurrency Value by Sentiment Analysis Inﬂuence maximization is particular subproblem originating directly from the inﬂuence diﬀusion problem that is particularly interesting and studied widely Inﬂuence maximization is searching the answer to the following tion Which users to target for spreading some information in order to have maximum possible users in social network about this mation Hence such methods aim to ﬁnd users who will share the information most widely propagate it further On the contrary to approaches for the ential users discovery these methods are connected strictly with diﬀusion of information General Model general and simple model for for evaluating the inﬂuence between user s network platform is ARIM It is based on the users proactive and reactive behaviors that can be found on basically any social networking site and can be used with diﬀerent data sets few interesting approaches based on ARIM were proposed method to calculate inﬂuence in citation networks and an algorithm to predict the tation using the known inﬂuence value Finally very interesting and new approach of was deﬁned Speciﬁc Challenge B Social Media Feeds Annotation and Dictionary Deﬁnition market sentiment dictionary for ﬁnancial social media data applications is sented in and then analysis of ﬁnancial Tweets is described in The dictionary in with words hashtags and emojis is available publicly at http under CC license It is built from posts in the StockTwits dataset at that time providing labeled posts from users and crawled through StockTwits API https The dictionary stores unstemmed tokens appearing at least ten times and showning cant diﬀerence in test signiﬁcance level between expected and observed frequency Also stopwords punctuations digits URLs user ids and tickers are removed from the input posts while still speciﬁcally processing jis Input posts with less than two words are also removed As the paper also analyses sentiment it is discussed that based on this dictionary the sentiment of investors may depend on the positions they hold positive investor sentiment does not imply bullish market sentiment due to the target of the investor The FiQA Task https is ported by Social Sentiment Index SSIX Horizon project https and it includes training instances to predict continuous ment In utilizing the dictionary the FiQA Task is evaluated over diﬀerent neural network models with Keras https keras Convolution Neural Network CNN Bidirectional Long Memory and Convolution Recurrent Neural Network CRNN Zamuda et Speciﬁc Challenge C Filtering Tweets As Sentiment Analysis SA usually works on top of Twitter feeds it is sary to the Tweets before SA as described in the following processing the data as deﬁned in is the process of cleaning and preparing the text for classiﬁcation This phase relevant role in sentiment analysis tasks indeed reducing the redundancy and the noise intrinsic in the online data allowing better and fast classiﬁcation The method can be distinguished in two stages Transformation and ﬁltering The ﬁrst stage is quite standard consisting in operations like white space removal expanding ation stemming stop words removal and negation handling while instead the second stage regards the choice and selection of the features which in Machine Learning is called feature selection Such task can be described mathematically as the problem to ﬁnd an optimal function f T S where T is the cleaned text space and S Rd the feature space where the optimum is deﬁned according to certain metric that is deﬁned priori In text mining context the feature selection task is composed by three steps Choice of the type of features giving weight to each feature and selecting the relevant features scoring each potential feature according to particular metric and then taking the n best features According to the mathematical description given above the problem is equivalent to choosing the metric with which to evaluate the embedding map We note that diﬀerently from other ﬁelds like image or audio processing when combination of features does not have any meaning then the type of features must be chosen by the user In this section we will focus on the third step The possible methods to extract features speciﬁc for Sentiment Analysis while the ranking of these features is addressed in Sect For complete review of the possible type of features we refer to and and for description of the weight models and for complete review on feature selection on text mining we refer to The most common metric in feature selection is the sure expressing the divergence between the feature and the class drawback of this metric is that it works only when the dataset is big and there are rare features Another two popular metrics are the Accuracy measuring the expected accuracy of simple classiﬁer built from the single feature and the the harmonic mean of the precision and recall As observed in all these metrics are equivalent and one outperforms the others according to the dataset In the review where twelve metrics are pared Forman proposed new metric the Separation BNS an extension of the where also the frequency of the appearance of the feature is considered This metric is good in most common cases but when predictions move outside of this set good results are not guaranteed For example in highly skewed data context the BNS does not work better and two information theoretic measures outperform it Information Gain and IT F Forecasting Cryptocurrency Value by Sentiment Analysis Speciﬁc Challenge D Sentiment Analysis Sentiment Analysis also known as opinion mining refers to the use of natural language text analysis and computational linguistics to identify or extract jective information from the attitude of from set of customer resources in order to classify the polarity From the point of view of text mining Sentiment Analysis is an automatic classiﬁcation massive task as function of the positive or negative emotions transmitted by the textual message In general Sentiment Analysis tries to determine the opinion from person with respect to topic Such opinion may involve an evaluation the emotional state of the user at the time of writing or the emotional communicative intention how the customer tries to inﬂuence the reader or interlocutor Present approaches can be grouped into three main categories based statistical methods and hybrid techniques techniques classify text into emotional categories based on the presence of unambiguous aﬀective words like happy sad bored etc These methods also imply the use of lexical aﬃnity to assign arbitrary words certain aﬃnity to particular emotions Statistical methods take advantage of Machine Learning techniques like port Vector Machines mutual inference semantic analysis etc More sophisticated methods try to detect the emotion and what is the target of such feeling Grammar dependent relations among words are usually applied to achieve such complex purpose implying deep grammar analysis of the message Hybrid approaches leverage on both knowledge representation and Machine Learning techniques These methods take advantage of knowledge tation models like ontologies and semantic nets being able to extract implicit information On the one hand there are open source tools that use Machine Learning techniques statistics and natural language processing to automate the ing of huge amounts of data including web pages online news sion fora social networks web blogs etc On the other hand knowledge based systems use public access resources like SentiWordNet or SenticNet to extract semantic and aﬀective information linked to natural language concepts Sentiment Analysis can also be performed on visual content images and videos denoted as Multimodal sentiment analysis One of the ﬁrst approaches in this direction was SentiBank utilizing an adjective noun pair representation of visual content In order to measure the precision and recall of Sentiment Analysis tool it is usually compared with human judgments about others opinions However humans when evaluated in the same way only show an of precision on average and this means that program with of precision works almost as well as humans Zamuda et Speciﬁc Challenge Technical Analysis and Forecasting SA wide range of applications for example monitoring the review of consumer products evaluating the popularity of public people or discovering fraud but the SA can also be applied in prediction settings Indeed SA was used with good results to predict Elections football matches stock prices and cryptocurrency ﬂuctuations In this survey we are interested in the latter application but in all the diﬀerent contexts described in this paragraph the prediction problem from mathematical point of view can be described in the same way Find function f S R such that given the sentiment st called the dependant or the explanatory variable the associated value is the prediction unknown priori of the independent or response variable f st simple linear regression model could be summarized by the equation βst where other unobserved factors determining are captured by the error term The dependent variable in the textual sentiment ﬁnance literature is typically Bitcoin price trade volume or Bitcoin return and volatility as in Logistic regression is used if the response variable is dichotomous Here the dependent variable is binary variable whose value is equal to if certain event happened and otherwise The idea is to examine if Sentiment Analysis is associated signiﬁcantly with certain event In almost all the examples listed above in this section the models used to estimate the function f are restricted to the linear ones Linear or logistic regression in case the features are correlated or Naive Bayes when the dataset is relatively small The only exception is represented by where the authors used Fuzzy Neural Networks hybrid system that combines the ories of Fuzzy Logic and Neural Networks Before proceeding with the description of recent forecasting models for cryptocurrency it is worth lining two points First such simple models are used because the feature space S is in general low dimensional indeed more complex models like Support Vector Machines or Neural Network architectures suﬀer from overﬁtting ond although many works show the relationship between the market and the social sentiments as highlighted by if the ﬁnancial instruments attract suﬃcient messages in general the sentiments do not lead ﬁnancial kets in statistically signiﬁcant way The latter observation can explain the reason why most of the work on cryptocurrency forecasting based on SA are tested with the cryptocurrency Bitcoin see or with ciﬁc cryptocurrency like ZClassic For ﬁnancial time series analysis in order to overcome the problem of the incomplete reliability of sentiments it is usual to combine SA with models see and reference therein with such approach being suggested also for cryptocurrency indeed as shown Forecasting Cryptocurrency Value by Sentiment Analysis in where the traded volume is driven primarily by past returns This idea considers multimodal architectures and although it is suggested in many works it is still not applied When forecasting cryptocurrencies values key topic is to consider if they should be classed as currencies assets or investment vehicles If they are traded for investment purposes like hedging or pricing instruments investigating the volatility of these cryptocurrencies becomes important and could help others make better informed decisions in terms of portfolio and risk management Speciﬁcally cryptocurrencies volatility levels are usually much higher than ditional currencies Volatility models were used in to test the eﬀects of sentiment or variables on the second moment of stock returns models and the realized volatility approach were employed stock volatility prediction using Recurrent Neural Networks with SA was sented in In the ability of several GARCH models to explain the Bitcoin price volatility was studied and the optimal model was presented in terms of to the data GARCH modeling of other Cryptocurrencies was presented in study on forecasting high frequency volatility for rencies and traditional currencies with Support Vector Regression was presented in As another approach to forecasting in fuzzy inference system for forecasting cryptocurrency price variation is developed with trading rules being optimized through the use of Genetic Algorithm the use of Genetic rithms to determine trading rules also been used before in Foreign Exchange markets FX One of the advantages of this type of methodology compared with black box machine learning models is its interpretability Speciﬁc Challenge F Ranking ICOs After the introduction of new measurement indicator like CSAI such tion brings with it new challenge of how to include the new indicator within existing schemes that fuse diﬀerent indicators to then support In fusing the diﬀerent data streams together one approach is to assign weights or ranks to indicators The Machine Learning algorithm designs ating on Big Data such as stability selection might be used on top of indicator values when tackling the ranking of ICOs As stability selection also highlights outliers it might also be helpful in optimizing value chains like energy eﬃciency and sustainability given proper access to blockchain data including cryptocurrencies as well as possible Big Data from the SA streams Speciﬁc Challenge H Cryptocurrency Portfolio Selection As cryptocurrency evaluation usually includes taking into account risk eral cryptocurrencies might be needed as set to balance this risk Namely Zamuda et portfolio selection of cryptocurrencies might increase robustness when trading In portfolio selection optimization algorithms are applied and are gaining importance as their applicability over global mization problems contributes towards better multiple criteria As perspective therefore this is an important challenge to be listed among future directions connected to cryptocurrency analysis In it is stated that it is possible to reduce investment risk tially by adding several diﬀerent cryptocurrencies to portfolio using the tional Markowitz model In similar manner in performance of an equally weighted portfolio and an optimal Markowitz portfolio considering four popular cryptocurrencies is compared considering diﬀerent levels of and reaching the conclusion that there are signiﬁcant diﬀerences between the two portfolios In the diversiﬁcation advantages of including cryptocurrencies in investment portfolios is analyzed justiﬁed by the fact that tocurrencies are highly connected to each other but not to other ﬁnancial assets In data from ten cryptocurrencies are used and the conclusion is reached that through diversiﬁcation investment results can be reached Portfolio selection is inherently problem even when using models like the Markowitz framework The mal portfolio will always be compromise solution between risk and return By changing parameters in the Markowitz model it is possible to deﬁne an cient frontier that will exhibit the existing compromises There is evidence that cryptocurrencies can be seen more as speculative asset than as currency These ﬁndings justify the use of optimization models for mining optimal portfolios that also consider investments in cryptocurrencies If there are metrics capable of calculating expected risk and return for rencies investments then most of the available methodologies can be used in this setting In approach is presented considering the multiperiod portfolio optimization using Diﬀerential Evolution is tic for global optimization that is also applied successfully to many other challenges like therefore improvements in can also have potential impact on portfolio selection and hence cryptocurrency analysis In multiperiod problem investors have the possibility of changing the cation of their investments during the considered horizon In it is stated that this ﬂexibility of rebalancing the portfolio is advantageous especially due to the high volatility that cryptocurrencies present Portfolio rebalancing is also considered in where information about the market psychology is included evaluated using fuzzy reasoning processes that are able to be determined under overevaluation possibilities The investor proﬁle will hence determine the cient solution that is most adequate to risk aversion preferences Forecasting Cryptocurrency Value by Sentiment Analysis Optimization Algorithms Metaheuristics have long been used for portfolio optimization In review is presented of Evolutionary Algorithms used for portfolio ment As recent contribution review of Swarm Intelligence methods applied to portfolio optimization can be found in Genetic Algorithm that takes into consideration transaction costs is duced in Genetic Algorithm is also used in The authors consider cardinality constraints ﬂoor constraints and also constraints The objective functions consider the maximization of return and minimization of risk along with the maximization of the sampling distance This last objective is only used to improve the behavior of the optimization algorithm evolutionary approach that is able to produce an eﬃcient frontier of portfolios is introduced in taking into account some constraints like lower and upper limits to the investment that can be made in each asset or the maximum and minimum number of assets that the portfolio should consider This type of dinality constraints is also considered in the work of where the authors describe hybrid algorithm combining Evolutionary Algorithms quadratic gramming and pruning heuristic Five diﬀerent Evolutionary Algorithms are compared in for the cardinality constrained portfolio optimization problem The authors conclude that seems to be better but all approaches perform better than ones framework taking into account adverse return ations only is described in Two diﬀerent Genetic Algorithms are compared NSGAII and embedding the use of technical analysis indicators Other applications of Genetic Algorithms can be found in Ant Colony Optimization is the methodology chosen in to tackle objective portfolio selection and the authors realize that there are some eﬃcient portfolios that are extremely diﬃcult to ﬁnd The authors report better results than the ones obtained with simulated annealing and NSGA Particle Swarm Optimization is used by The method seems to be particularly well suited to be used for investment portfolios An Artiﬁcial Bee Colony tic is described in for portfolio optimization interpreting returns as fuzzy numbers Varying the risk tolerance parameter will lead to diﬀerent eﬃcient portfolios The authors acknowledge that constraints have an tant impact on the investment strategy Fuzzy numbers are also used by to represent the uncertainty in future returns Three objectives are considered there simultaneously The maximization of the possibilistic expected returns the minimization of the downside risk absolute below mean and the minimization of the skewness of every portfolio Cardinality constraints are also included The portfolio optimization is transformed into ive problem using fuzzy normalization and uniform design method in The resulting model is then solved using an invasive weed optimization algorithm The original model considers two more objectives in addition to the Markowitz model the stock proﬁt gained relative to its market price and Zamuda et the representation of experts recommendations Fuzzy numbers are also used in to describe asset returns and develop approach based on genetic algorithm In an approach is presented that uses Fuzzy Neural Network embedded into an heuristic that explores simplex local searches Finally use machine learning tools namely deep reinforcement learning to deﬁne cryptocurrencies portfolios based on technical aspects only price and movement Speciﬁc Challenge G Investment Approaches with the New ICO Sentiment Indicator The technical analysis covered in this chapter is the SA Sect with ing Sect and indicators fusion Sect followed by optimization algorithms supported portfolio selection Sect Besides technical sis however investment approaches with cryptocurrencies must take into account particular characteristics like the herding eﬀect as identiﬁed in Risks from these characteristics can be caused through lack of diversiﬁcation of tocurrencies portfolios making investors more exposed to investment risk as explained in the previous section These characteristics also include long ory stochastic volatility and leverage Keeping pertained characteristics in mind and together through technical analysis trader might then evaluate cryptocurrency as whole and trade the cryptocurrency as ﬁnancial asset Conclusion This chapter covered the diﬀerent aspects of necessary perspectives needed when preparing forecasting and investment supported by cryptocurrency social media Sentiment Analysis Eight speciﬁc perspectives were identiﬁed and the current was covered in separate sections of the chapter all focusing around new type of indicator the CSAI In the following work some more speciﬁc implementations and experimental results could be presented based on the guidelines outlines and integration possibilities presented in this chapter Acknowledgements This chapter is based upon work from COST Action Modelling and Simulation for Big Data Applications cHiPSet ported by COST European Cooperation in Science and Technology The author AZ acknowledges the ﬁnancial support from the Slovenian Research Agency Research Core Funding AZ also acknowledges EU support under Project HPC RIVR This chapter is also based upon work from COST Action Improving Applicability of Optimisation by Joining ory and Practice ImAppNIO supported by COST The authors CP TC and IS also acknowledge that the work presented in this chapter is supported ﬁnancially by the eDREAM Project Grant number by the European Commission as part of the Framework Programme This work was supported by the Fundacao Ciencia Tecnologia FCT under Project Grant Author RS also acknowledges that work was supported by the Ministry of Education Forecasting Cryptocurrency Value by Sentiment Analysis Youth and Sports of the Czech Republic within the National Sustainability Programme Project further supported by the European Regional Development Fund under the Project References Abbasi Chen Salem Sentiment analysis in multiple languages feature selection for opinion classiﬁcation in Web forums ACM Trans Inf Syst TOIS Abraham Higdon Nelson Ibarra Cryptocurrency price prediction using tweet volumes and sentiment analysis SMU Data Sci Rev Anagnostopoulos Mamanis The cardinality constrained portfolio optimization problem an experimental evaluation of ﬁve multiobjective evolutionary algorithms Expert Syst Appl Aouni Doumpos Steuer On the increasing importance of multiple criteria decision aid methods for portfolio selection Oper Res Soc Aranha Iba Modelling cost into genetic portfolio mization system by seeding and objective sharing In IEEE Congress on Evolutionary Computation pp IEEE Singapore Athey Parashkevov Sarukkai Xia Bitcoin pricing adoption and usage theory and evidence Stanford University Graduate School of Business Research Paper https Baccianella Esuli Sebastiani SentiWordNet an enhanced lexical resource for sentiment analysis and opinion mining In Seventh International Conference on Language Resources and Evaluation LREC vol pp Bagheri Peyhani Akbari Financial forecasting using ANFIS works with particle swarm optimization Expert Syst Appl Balcilar Bouri Gupta Roubaud Can volume predict Bitcoin returns and volatility approach Econ Model Bartoletti Bracciali Lande Pompianu general framework for Bitcoin analytics arXiv preprint Baur Hong Lee Bitcoin medium of exchange or speculative assets Int Fin Markets Inst Money Bianchi Cryptocurrencies as an asset class An empirical assessment June WBS Finance Group Research Paper Bollen Mao Zeng Twitter mood predicts the stock market Comput Sci Borth Ji Chen Breuel Chang visual sentiment ontology and detectors using adjective noun pairs In Proceedings of the ACM International Conference on Multimedia pp ACM Bouri Gupta Roubaud Herding behaviour in cryptocurrencies Fin Res Lett https Brauneis Mestel in work Fin Res Lett https Brauneis Mestel Price discovery of cryptocurrencies Bitcoin and beyond Econ Lett Zamuda et Buyya et manifesto for future generation cloud computing research directions for the next decade ACM Comput Surv Cambria Hussain Sentic Computing Techniques Tools and Applications vol Springer Dordrecht https Cambria Olsher Rajagopal SenticNet common and sense knowledge base for sentiment analysis In AAAI Conference on Artiﬁcial Intelligence Cambria Schuller Xia Havasi New avenues in opinion mining and sentiment analysis IEEE Intell Syst Casino Dasaklis Patsakis systematic literature review of applications current status classiﬁcation and open issues Telematics Inform in Press https Cataldi Aufaure The million follower fallacy audience size does not prove on Twitter Knowl Inf Syst Catania Grassi Ravazzolo Forecasting cryptocurrencies ﬁnancial time series CAMP Working Paper Series Chandrashekar Sahin survey on feature selection methods Comput Electr Eng Chen Huang Chen analysis of ﬁnancial Tweets In Companion of the Web Conference on the Web Conference pp International World Wide Web Conferences Steering Committee Chen Huang Chen market sentiment tionary for ﬁnancial social media data applications In Proceedings of the Financial Narrative Processing Workshop FNP Chen Artiﬁcial bee colony algorithm for constrained possibilistic portfolio optimization problem Phys Stat Mech Appl Chen Blockchain tokens and the potential democratization of ship and innovation Bus Horiz Cheoljun Kaizoji Kang Pichl Bitcoin and investor sentiment statistical characteristics and predictability Physica Chiam Mamun Low realistic approach to evolutionary multiobjective portfolio optimization In IEEE Congress on Evolutionary Computation pp IEEE Chu Stephen Saralees Joerg GARCH modeling of cies J Risk Fin Manag Cioara et Enabling new technologies for demand response decentralized validation using blockchain In IEEE International Conference on ronment and Electrical Engineering and IEEE Industrial and Commercial Power Systems Europe pp IEEE Claudia Tudor Marcel Ionut Ioan Massimo Blockchain based decentralized management of demand response programs in smart energy grids Sensors Colianni Rosales Signorotti Algorithmic trading of cryptocurrency based on Twitter sentiment analysis Project Corbet Meegan Larkin Lucey Yarovaya Exploring the dynamic relationships between cryptocurrencies and other ﬁnancial assets Econ Lett Cretarola Fig Patacca model for the BitCoin theory estimation and option pricing https https Forecasting Cryptocurrency Value by Sentiment Analysis Das Mullick Suganthan Recent advances in diﬀerential evolution an updated survey Swarm Evol Comput David Handbook of Digital Currency Bitcoin Innovation Financial ments and Big Data Elsevier Amsterdam https Deng Lin portfolio selection with nality constraints using improved particle swarm optimization Expert Syst Appl Dey Haque Opinion mining from noisy text data Int Doc Anal Recogn IJDAR D mello Distributed software dependency management using blockchain In Euromicro International Conference on Parallel tributed and Processing PDP pp IEEE Pavia to appear Doerner Gutjahr Hartl Strauss Stummer Pareto ant colony optimization metaheuristic approach to multiobjective portfolio tion Ann Oper Res Easley Hara Basu From Mining to Markets The Evolution of Bitcoin Transaction Fees SSRN eDREAM eDREAM Project http European Commission Energy Strategy and Energy Union Secure Competitive and sustainable Energy https Ertenlice Kalayci survey of swarm intelligence for portfolio mization algorithms and applications Swarm Evol Comput Feldman Techniques and applications for sentiment analysis Commun ACM Forman An extensive empirical study of feature selection metrics for text classiﬁcation Mach Learn Res Mar Galvez Mejuto Future challenges on the use of blockchain for food traceability analysis TrAC Trends Anal Chem Georgoula Pournarakis Bilanakos Sotiropoulos Giaglis Using time series and sentiment analysis to detect the determinants of Bitcoin prices In MCIS Proceedings https Zamuda combined economic and emission mal optimization by surrogate diﬀerential evolution Appl Energy Guidolin Hyde McMillan Ono predictability in stock and bond returns when and where is it exploitable Int J Forecast Guyon Elisseeﬀ An introduction to variable and feature selection Mach Learn Res Mar Haddi Liu Shi The role of text in sentiment analysis Procedia Comput Sci Haveliwala PageRank In Proceedings of the tional Conference on World Wide Web pp ACM Hayes Cryptocurrency value formation an empirical analysis leading to cost of production model for valuing Bitcoin Telematics Inform Zamuda et Hirsch An index to quantify an individual s scientiﬁc research output Proc Nat Acad Sci Hussein survey on sentiment analysis challenges J King Saud Sci Jiang Liang Cryptocurrency portfolio management with deep ment learning In Intelligent Systems Conference IntelliSys pp IEEE Kaminski Nowcasting the Bitcoin market with Twitter signals arXiv preprint https Katsiampa Volatility estimation for Bitcoin comparison of GARCH models Econ Lett Kearney Liu Textual sentiment in ﬁnance survey of methods and els Int Rev Fin Anal Kempe Kleinberg Tardos Maximizing the spread of inﬂuence through social network In Proceedings of the Ninth ACM SIGKDD International ference on Knowledge Discovery and Data Mining pp ACM Khayamim Mirzazadeh Naderi Portfolio rebalancing with respect to market psychology in fuzzy environment case study in Tehran Stock Exchange Appl Soft Comput Kim Hovy Identifying and analyzing judgment opinions In ings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics pp Association for Computational Linguistics Kim Lee Park Choo Kim Kim When Bitcoin encounters information in an online forum using text mining to analyse user opinions and predict value ﬂuctuation PloS One Ladislav BitCoin meets Google Trends and Wikipedia quantifying the tionship between phenomena of the Internet https Lamon Nielsen Redondo Cryptocurrency Price Prediction Using News and Social Media Sentiment http Law Sabett Solinas How to make mint the cryptography of mous electronic cash Technical report National Security Agency Oﬃce of mation Security Research and Technology Cryptology Division nia Avenue DC USA June Li Shah Learning stock market sentiment lexicon and word vector from StockTwits In Proceedings of the Conference on tational Natural Language Learning CoNLL pp Li Chamrajnagar Fong Rizik Fu prediction of alternative cryptocurrency price ﬂuctuations using gradient boosting tree model arXiv preprint Li Wang The technology and economic determinants of cryptocurrency exchange rates the case of Bitcoin Decis Support Syst Lim et Blockchain technology the identity management and tication service disruptor survey Int Adv Sci Eng Inf Technol Lin Liu Genetic algorithms for portfolio selection problems with minimum transaction lots Eur Oper Res Forecasting Cryptocurrency Value by Sentiment Analysis Linton Teo Bommes Chen Dynamic topic modelling for cryptocurrency community forums In Chen Overbeck L eds Applied Quantitative Finance SC pp Springer Heidelberg https Liu Yu Feature selection for highly skewed sentiment analysis tasks In Proceedings of the Second Workshop on Natural Language Processing for Social Media SocialNLP pp Liu Portfolio diversiﬁcation across cryptocurrencies Fin Res Lett https Liu Qin Li Wan Stock volatility prediction using recurrent neural networks with sentiment analysis In Benferhat Tabia Ali M eds LNCS LNAI vol pp Springer Cham https Lucey et Future directions in international ﬁnancial integration research crowdsourced perspective Int Rev Fin Anal Lumanpauw Pasquier Quek novel memetic fuzzy system based ﬁnancial portfolio management In IEEE Congress on Evolutionary Computation pp IEEE Singapore Ma Gans Tourky Market structure in Bitcoin mining Technical report National Bureau of Economic Research Macedo Godinho Alves portfolio optimization with multiobjective evolutionary algorithms and technical analysis rules Expert Syst Appl Mai Shan Bai Wang Chiang How does social media impact Bitcoin value test of the silent majority hypothesis Manag Inf Syst Mao Wei Wang Liu Correlating S P stocks with Twitter data In Proceedings of the First ACM International Workshop on Hot Topics on Interdisciplinary Social Networks Research pp ACM Mashayekhi Omrani An integrated eﬃciency model with fuzzy returns for portfolio selection problem Appl Soft Comput Matta Lunesu Marchesi The predictor impact of Web search media on Bitcoin trading volumes In International Joint Conference on edge Discovery Knowledge Engineering and Knowledge Management vol pp IEEE Mba Pindza Koumba diﬀerential evolution roach for cryptocurrency portfolio optimization Fin Markets folio Manag McNally Roche Caton Predicting the price of Bitcoin using machine learning In Euromicro International Conference on Parallel tributed and Processing PDP pp IEEE Mendes Godinho Dias forex trading system based on genetic algorithm Heuristics Metaxiotis Liagkouras Multiobjective evolutionary algorithms for folio management comprehensive literature review Expert Syst Appl Millard Blockchain and law incompatible codes Comput Law Secur Rev Mohammadi Saraee Mirzaei inﬂuence maximization in social networks Inf Sci Zamuda et Nakamoto Bitcoin electronic cash system Nauck Klawonn Kruse Foundations of Systems Wiley Hoboken Nguyen Kim survey about consensus algorithms used in blockchain Inf Process Syst Ortony Clore Collins The Cognitive Structure of Emotions bridge University Press Cambridge Panarello Tapas Merlino Longo Puliaﬁto Blockchain and IoT integration systematic survey Sensors Pang Lee Seeing stars exploiting class relationships for sentiment rization with respect to rating scales In Proceedings of the Annual Meeting on Association for Computational Linguistics pp Association for putational Linguistics Peng Zhou Cao Yu Niu Jia Inﬂuence analysis in social networks survey Netw Comput Appl Peng Albuquerque Padula Montenegro The best of two worlds forecasting high frequency volatility for cryptocurrencies and traditional currencies with Support Vector Regression Expert Syst Appl Petcu Nicolae Sheridan Next generation HPC clouds view for scientiﬁc and applications In Lopes et eds LNCS vol pp Springer Cham https Petrican et Evaluating forecasting techniques for integrating household energy prosumers into smart grids In International Conference on gent Computer Communication and Processing pp IEEE Phillips Gorse Predicting cryptocurrency price bubbles using social media data and epidemic modelling In IEEE Symposium Series on putational Intelligence SSCI pp IEEE Phillips Gorse of cryptocurrency market returns and social media topics In Proceedings of the International Conference on Frontiers of Educational Technologies pp ACM Platanakis Sutcliﬀe Urquhart Optimal vs diversiﬁcation in cryptocurrencies Econ Lett Ponsich Jaimes Coello survey on multiobjective evolutionary algorithms for the solution of the portfolio optimization problem and other ﬁnance and economics applications IEEE Trans Evol Comput Pop et Decentralizing the stock exchange using blockchain an based implementation of the Bucharest Stock Exchange In International Conference on Intelligent Computer Communication and Processing pp IEEE Puri Decrypting Bitcoin prices and adoption rates using Google search CMC Senior Theses Radosavljevic Grbovic Djuric Bhamidipati World Cup outcome prediction based on Tumblr posts In KDD Workshop on Sports Analytics Rakoczy Bouzeghoub Gancarski Users views on others analysis of confused terms in social network In Debruyne et eds OTM LNCS vol pp Springer Cham https Forecasting Cryptocurrency Value by Sentiment Analysis Rakoczy Bouzeghoub Lopes In the search of quality inﬂuence on small scale discovery In Panetto Debruyne Proper Ardagna Roman Meersman R eds OTM LNCS vol Springer Cham https Rakoczy Bouzeghoub ence in citation networks In International Conference on Research Challenges in Information Science RCIS May Nantes France pp Rakoczy Bouzeghoub tion prediction using inﬂuence conversion In IEEE International ence on Trust Security and Privacy in Computing and IEEE International Conference on Big Data Science and Engineering August New York NY USA pp Reyna Chen Soler On blockchain and its gration with IoT Challenges and opportunities Future Gener Comput Syst Rezaei Pouya Solimanpur Jahangoshai Rezaee Solving objective portfolio optimization problem using invasive weed optimization Swarm Evol Comput Ruiz Hristidis Castillo Gionis Jaimes Correlating cial time series with activity In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining pp ACM Suarez Use of heuristic rules in evolutionary methods for the selection of optimal investment portfolios In IEEE Congress on Evolutionary Computation pp IEEE Saborido Ruiz Vercher Luque Evolutionary optimization algorithms for fuzzy portfolio selection Appl Soft Comput Skolpadungket Dahal Harnpornchai Portfolio optimization using genetic algorithms In IEEE Congress on Evolutionary putation pp IEEE Smith Caton Social In Euromicro International Conference on Parallel Distributed and Processing PDP pp IEEE Cambridge Sokolov Ransomware Activity Demand for Blockchain Settlement and the Upper Bound of Mining Reward SSRN Working Paper https Soleimani Golmakani Salimi portfolio tion with minimum transaction lots cardinality constraints and regarding tor capitalization using genetic algorithm Expert Syst Appl Stenqvist Predicting Bitcoin price ﬂuctuation with Twitter ment analysis Stevenson Mikels James Characterization of the aﬀective norms for english words by discrete emotional categories Behav Res Methods Stocchi Lunesu Ibba Baralla Marchesi The future of Bitcoin synchrosqueezing wavelet transform to predict search engine query trends In KDWeb Zamuda et Storn Price Diﬀerential evolution simple and eﬃcient heuristic for global optimization over continuous spaces J Global Optim Sung Moon Lee The inﬂuence in Twitter are they really inﬂuenced In Cao et eds LNCS LNAI vol pp Springer Cham https Szor The Art of Computer Virus Research and Defense Pearson Education London Tonelli Ducasse Fenu Bracciali IEEE International Workshop on Blockchain Oriented Software Engineering IWBOSE March Campobasso Italy IEEE Tudor et Optimized ﬂexibility management enacting data centres ipation in smart demand response programs Future Gener Comput Syst Tumasjan Sprenger Sandner Welpe Predicting elections with Twitter what characters reveal about political sentiment In tional AAAI Conference on Web and Social Media Fourth International AAAI Conference on Weblogs and Social Media vol pp Tupinambas Leao Lemos Cryptocurrencies transactions sor using genetic fuzzy rules based system In IEEE tional Conference on Fuzzy Systems Turney Thumbs up or thumbs down semantic orientation applied to pervised classiﬁcation of reviews In Proceedings of the Annual Meeting on Association for Computational Linguistics pp Association for tational Linguistics Viktorin Senkerik Pluhacek Kadavy Zamuda Distance based parameter adaptation for based diﬀerential evolution Swarm Evol November https Vytautas Niels Jochen Using sentiment analysis to predict day Bitcoin price movements J Risk Fin Wang et survey on consensus mechanisms and mining management in blockchain networks arXiv preprint Weng Lim Jiang TwitterRank ﬁnding ential Twitterers In Proceedings of the Third ACM International Conference on Web Search and Data Mining pp ACM Werner Murray Is all that talk just noise The information content of internet stock message boards Fin Xiao Noro Tokuda Finding oriented inﬂuential Twitter users based on topic related hashtag community detection J Web Eng Yogatama Dyer Ling Blunsom Generative and discriminative text classiﬁcation with recurrent neural networks In International Conference on Machine Learning ICML Zafarani Abbasi Liu Social Media Mining An Introduction bridge University Press New York Zamuda Brest control parameters randomization frequency and propagations in diﬀerential evolution Swarm Evol Comput Zamuda Nicolau Zarges discrete optimization marking pipeline survey taxonomy evaluation and ranking In ceedings of the Genetic and Evolutionary Computation Conference Companion GECCO pp Forecasting Cryptocurrency Value by Sentiment Analysis Zamuda Zarges Stiglic Hrovat Stability selection using genetic algorithm and logistic linear regression on healthcare records In Proceedings of the Genetic and Evolutionary Computation Conference Companion GECCO pp Zamuda Sosa Success history applied to expert system for underwater glider path planning using diﬀerential evolution Expert Syst Appl April Zhang Fuehres Gloor Predicting stock market indicators through Twitter I hope it is not as bad as I fear Behav Sci Zheludev Smith Aste When can social media lead ﬁnancial markets Sci Zopounidis Galariotis Doumpos Sarri AndriosopouloS tiple criteria decision aiding for ﬁnance an updated bibliographic survey Eur Oper Res Open Access This chapter is licensed under the terms of the Creative Commons Attribution International License http which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give appropriate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this chapter are included in the chapter s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the chapter s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder Author Index Kołodziej Joanna Komínková Oplatková Zuzana Krzyszton Mateusz Kuonen Pierre Larsson Elisabeth Marechal Catherine Marks Michal Matos Dias Joana Mauri Giancarlo Mettouris Christos Mikołajewski Dariusz Moldovan Dorin Molina Jose Möller Steffen Nejkovic Valentina Ewa Nobile Marco Novović Olivera Olğaç Abdurrahman Olğaç Simla Papadopoulos Apostolos Papadopoulos George Petrovic Nenad Pllana Sabri Pop Claudia Pop Cristina Prokopowicz Piotr Rached Imen Rancic Svetozar Respicio Ana Righero Marco Salomie Ioan Senkerik Roman Albanis Nikolas Aldinucci Marco Ancourt Corinne Bajrovic Enes Bashroush Rabih Benkner Siegfried Bougueroua Lamine Bracciali Andrea Brdar Sanja Burguillo Juan Cioara Tudor Crescimanna Vincenzo Damián Alejandro Francavilla Alessandro Giordanengo Giorgio Horacio Gortazar Francisco Grelck Clemens Gribaudo Marco Grujić Nastasija Grzonka Daniel Hosseinpour Farhoud Iacono Mauro Jakobik Agnieszka Jarynowski Andrzej Kadavy Tomas Karageorgos Anthony Kessler Christoph Kilanioti Irene Kisielewicz Paweł Koivisto Mike Author Index Sikora Andrzej Spolaor Simone Stojanovic Dragan Stojanovic Natalija Stojnev Ilic Aleksandra Valkama Mikko Vecchi Giuseppe Viktorin Adam Vipiana Francesca Visa Ari Vitabile Salvatore Talvitie Jukka Tchorzewski Jacek Tosic Milorad Truică Türe Aslı Turunen Esko Tyburek Krzysztof Katarzyna Widłak Adrian Zafari Afshin Zamuda Aleš,,[]
Received March accepted March date of publication March date of current version April Digital Object Identifier Review on Explainability in Multimodal Deep Neural Nets GARGI JOSHI RAHEE WALAMBE AND KETAN KOTECHA Institute of Technology SIT Symbiosis International Deemed University Pune India Centre for Applied Artiﬁcial Intelligence Symbiosis International Deemed University Pune India Corresponding authors Ketan Kotecha director and Rahee Walambe This work was supported by the Ministry of Human Resource Development MHRD Scheme for Promotion of Academic and Research Collaboration SPARC of the Government of India under Grant ABSTRACT Artiﬁcial Intelligence techniques powered by deep neural nets have achieved much success in several application domains most signiﬁcantly and notably in the Computer Vision applications and Natural Language Processing tasks Surpassing performance propelled the research in the applications where different modalities amongst language vision sensory text play an important role in accurate predictions and identiﬁcation Several multimodal fusion methods employing deep learning models are proposed in the literature Despite their outstanding performance the complex opaque and nature of the deep neural nets limits their social acceptance and usability This given rise to the quest for model interpretability and explainability more so in the complex tasks involving multimodal AI methods This paper extensively reviews the present literature to present comprehensive survey and commentary on the explainability in multimodal deep neural nets especially for the vision and language tasks Several topics on multimodal AI and its applications for generic domains have been covered in this paper including the signiﬁcance datasets fundamental building blocks of the methods and techniques challenges applications and future trends in this domain INDEX TERMS Deep multimodal learning explainable AI interpretability survey trends vision and language research XAI I INTRODUCTION Remarkable improvements of deep neural nets in the pendent tasks based on several modalities such as vision sensor data textual data and language have given rise to many new trends and applications in the integrated space of deep multimodal learning Deep neural nets have proved exceptionally effective for several single modality or multimodality tasks However the complex hidden layers processing in the deep neural nets makes them difﬁcult to interpret opaque and models with little or understanding of their internal states and process Gaining meaningful knowledge and ing of how and why the model arrived at particular decision or outcome is crucial in model explainability making it one of the important evaluation metrics The lack of ing of the underlying process questions the model s ibility and transparency impacting its social acceptability The associate editor coordinating the review of this manuscript and approving it for publication was Weiping Ding and usability In multimodal environment with diverse modalities having varied scales and representations the extraction of information from various heterogeneous sources is essential for the integration and fusion of these modalities Several applications and tasks for multimodal AI are proposed in the literature combining various ities However the most prevalent applications are found in vision and language tasks We have considered the image and video modalities in the vision tasks and text and audio modalities in language tasks in this work Multimodality extracts and combines vital information from the respective modality source and solves given problem with richer representation and performance than the individual modalities The complementary nature of modalities is explored in to complement the missing data or noise in modalities The inter intra and interactions correlations and relationships between multiple modalities with high mutual information are explored to have improved predictions and performance An optimal fusion scheme would combine the modalities and ensure that the This work is licensed under Creative Commons Attribution License For more information see https VOLUME Joshi et Review on Explainability in Multimodal Deep Neural Nets resultant model reﬂects the salient features of input ities to generate rich joint representation for downstream task Vision and language models exploit cues in the question and biases in the data distribution with minimum dependence on visual content leading the model to answer task However due to the of vision info the model does answer the questions with high dence even though they are wrong with intrinsic weaknesses In Visual Questioning Answering to diagnose the model s performance the inverse Visual Question Answering task is proposed in multimodal setting Answering questions without proper grounding and pointing to evidence pers performance with multiple modalities The monly used method for interpreting the multimodal setting results is Grad CAM s attention maps Still they do not identify whether the model looks at the correct region to answer the questions Attention maps do not guarantee explanatory power as the models and human tion does not concentrate on the same areas while ing the output Lack of alignment in the pairs emphasizes that mere accuracy is not enough if it is not accompanied by valid justiﬁcation to be right for the right reason Explainability efforts are also carried out by generating visual and textual explanations but they can result in multiple explanations with varied evaluation metrics Modular approaches make the system interpretable by design but are majorly tested on synthetic datasets such as CLEVR To exploit the complementary assistive and explanatory information and improved predictive power from multiple modalities deep understanding of the model s working predictions and ﬂaw detection are of utmost importance by using the respective modalities This growing critical need and importance have led to several reviews emphasizing the multifaceted explainability topic in models such as deep neural nets The existing literature number of surveys on able AI such as and on explainable deep learning In Gilpin et suggested onomy for explaining the explanations by classifying the XAI methods based processing representation and the type of explanation produced with their evaluation based on completeness to model and substitute tasks In survey of different methods on understanding and interpreting deep neural nets are discussed In survey of explainable deep learning applications for medical imaging tasks from the perceptive of deep learning researcher developer and is presented Reference presents the rization of different explainability methods applicable to the medical and healthcare sector In survey on pretability methods in machine learning from causal spective is presented Graph neural nets show key role in explainability from causal perspective Constructing multimodal feature representation space spanning diverse modalities from causal and counterfactual perspective in the medical domain is detailed Reference presents an line charts overview of the Explainable AI XAI approaches for the ural Language Processing domain Visual analytics plays vital role in understanding the deep neural net models using different techniques such as diagrams temporal metrics and sionality reduction analysis graphical methods analyze model parameters individual computational neurons and tion units Visual representations interactions attribution and feature visualization techniques provide interpretability scalability bias and adversarial attack detection covering ferent visualization techniques Existing surveys in the ﬁeld present either generalized or speciﬁc perceptive to XAI ods techniques and applications from the unimodal context In contrast we focus on the applicability of explainable AI in the multimodal setting involving diverse heterogeneous multiple modalities that have not been previously addressed and are key to this work This review encounters ity in multimodal tasks with speciﬁc focus on the vision and language tasks where interpretability in the model is established using disentangled representations multimodal explanations and counterfactuals techniques ORGANIZATION OF THE SURVEY This paper provides an introduction to explainability in deep neural nets with speciﬁc focus on the explainability in the multimodal setting We consider and discuss deep timodal learning for different vision and language tasks and typical challenges in the multimodal environment The ods and techniques of multimodal data fusion and integration are discussed in Section Different types of explainability techniques in unimodal and multimodal settings are ered The taxonomy of varying XAI techniques is presented in Section The signiﬁcance of explainability in modal networks with introspection and justiﬁcation systems is discussed in Section Different multimodal tion techniques such as approaches terfactual explanations interactive approaches approaches techniques are clearly and tinctly classiﬁed and discussed in Section Explanation evaluation methods based on the human mental model and automated processes are reviewed in Section Different explainability requirements to satisfy the needs and tations of different stakeholders are analyzed in Widely used datasets for explainability in multimodal works and applications are listed in Section Topics aligned with multimodal explainability setting such as multimodal bias and fairness adversarial attacks that enhance ness and interpretability are discussed in subsequent tions in Section and Section Finally we conclude the survey outcomes as observations and tions that showcase the gaps and ﬁndings with the ther scope of improvement and persisting challenges future research trends and directions to pave roadmap for further research in this active domain in Section and Section respectively VOLUME Joshi et Review on Explainability in Multimodal Deep Neural Nets TABLE Multimodal applications applied in various generic domains II MULTIMODAL MACHINE LEARNING The McGurk effect highlighted that the audio and visual information is merged into uniﬁed integrated perception that led to speech recognition AVSR systems giving rise to multimodal multisensory interfaces and timodal information retrieval systems Large scale datasets faster GPUs visual and language features are key enablers of multimodal machine learning research in the deep learning Human perception is multimodal Humans have the ent cognitive ability to relate and process information from multiple heterogeneous modality sources at single instance through the senses We perceive and tackle things in timodal way Modality is the form in which information is stored or represented and is conveyed through media With the recent technological advances the data come from diverse number of sources For example data on social media sites is heterogeneous high dimensional complex and is represented by multiple modalities such as image text audio and video in the verbal vocal and visual form These diverse modalities differ in their scales representation format varied predictive power weights and contributions towards the ﬁnal task Optimal data fusion schemes such as early late and hybrid fusion schemes are developed to fuse the modalities at data feature decision and intermediate mixed levels Deep neural nets based methods and graphical models are employed for analysis and handling such data depending on the downstream task Individual modalities are mapped onto common shared representation vector space either through joint or coordinated representation Multimodal setting leverages improved predictive power compared to its unimodal counterparts due to involvement and knowledge extraction from multiple modalities This achieves improved results and richer representation with features reducing data size compared to single modality representations The interplay between ple heterogeneous and high dimensional diverse modality sources with diverse representation formats makes ability key concern for multimodal data This leads to ing comprehensive global insights about the model s design working principle process ﬂaw detection and handling the bias and fairness issues Multimodal data provide complementary additive combined and prehensive information exploring the inter intra shared and associations and correlations between ent modalities Several applications are reported in literature where ple modalities are leveraged Table enlist such applications in various generic domains Table refers to number of multimodal applications applied to critical domains namely healthcare autonomous robots ﬁnance as there is growing use of AI in these domains leading to tremendous need for explainability for social acceptance and usability VOLUME Joshi et Review on Explainability in Multimodal Deep Neural Nets TABLE Multimodal applications in critical domains TABLE Continued Multimodal applications in critical domains However due to persistent heterogeneity in multimodal data it remains challenging to develop new and efﬁcient analytical methodologies It is complex task to deal with the multimodal data to explore their comprehensive beneﬁts that have attracted rich attention in the research domain in recent years Hence there is an urgent need to stand the approaches methods and techniques for modal data fusion and build an integrative framework for developing tools and applications in various disciplines Interpretability explainability and contextual cognitive soning assist in understanding multimodal data in better way With the development of new benchmarks we can identify and handle the ﬂaws in model evaluation metrics dataset bias robustness and spurious correlations in the multimodal data TYPICAL CHALLENGES IN MULTIMODAL SETTING Multimodal AI is inherently complex Following challenges are identiﬁed VOLUME Representation is the method or format in which modalities are represented that extracts the mentary or redundant information between multiple modalities Due to the heterogeneous nature of timodal data its representation is very important and at the same time challenging too For the sound is signal and the image is representation with varied scales and dimensions to represent How to bring them into the same common representation space is an essential aspect of implementation Translation refers to the process of explaining transform or convert data from one modality to another when heterogeneous The relationship between ties is often subjective For example translating video to its corresponding text description Alignment is the mapping of the direct correlations between from two or more different modalities For example we may want to align the streaming video and its subtitles To overcome this challenge we measure the similarity between other modalities and deal with possible dencies and ambiguities Alignment and representation can be considered as overlapping tasks with marginal difference Fusion is the method of integrating or fusing tion from two or more modalities to perform tion For for speech recognition the lip motion s visual description is combined with the speech signal to predict spoken words The information coming from different modalities may have varying predictive power importance contribution and noise topology possibly missing data in at least one of the modalities is the process of sharing knowledge and information between multiple modalities It is ily applied when one modality is rich source of mation whereas the other is poor information source The knowledge between modalities can be exchanged and shared to enrich the modeling process such as in Transfer Learning Multimodal data fusion is often prone to overﬁtting as we have data from various sources and variable learning rates that generalize differently The overﬁtting problem can be addressed by implementing dynamic and adaptive fusion scheme and regularization technique such as ent blending that computes the optimal blend of modalities based on their overﬁtting behavior Explainability in the multimodal setting is vital to have comprehensive global view of data and address the associated challenges Table describes modality representations architectural requirements and models in multimodal vision and language paradigm MULTIMODAL DATA FUSION TECHNIQUES Depending on the level at which the fusion of input ities occurs in the network the multimodal data fusion Joshi et Review on Explainability in Multimodal Deep Neural Nets TABLE Representation and pretraining of vision and language modalities mechanisms are classiﬁed as early fusion data or feature level fusion late fusion decision level fusion and hybrid intermediate or joint fusion The fusion mechanisms are highly data task and hence the priate and optimal fusion mechanism is critical The data fusion methods are broadly classiﬁed into early late and hybrid fusion approaches its EARLY FUSION Early fusion is traditional way of integrating data before its analysis It two methods The ﬁrst method is ing data by removing the correlations The second method is to combine data at latent space Statistical solutions such as Principal Component Analysis Independent Component Analysis and Canonical Component Analysis are proposed for fusion by reducing the data dimensions Early fusion is applied and performed on unprocessed raw data Features are extracted before fusion for modalities with variable sampling rates to avoid complexity Syncing of data sources is also difﬁcult when they are either in discrete and continuous forms So converting data sources into ﬁxed representation is too ﬁcult and with early fusion techniques Early data fusion is assumed to be conditional dependent but there is high correlation among multiple modalities for example MRI is associated with depth Modalities are associated at higher level of abstraction The comes of individual modality are expected to be processed irrespective of each other Features are fused using simple concatenation pooling and gated units In reality modalities have different dimensions for fusion The major drawback of using early fusion is it removes large amount of data from the modalities before fusing it The method fails in the synchronization of time stamps This problem can be solved if we collect the data at similar sampling rates Different solutions to overcome this lem are proposed combining sequential and discrete events with continuous data through training pooling and volution An early fusion approach is schematically represented in Fig VOLUME Joshi et Review on Explainability in Multimodal Deep Neural Nets FIGURE An early fusion approach LATE FUSION Late fusion uses individual modality sources for fusion during It is drawn from the ensemble classiﬁers using the techniques of bagging and boosting When the modal data sources are uncorrelated in terms of sampling rate data dimensionality this technique can be used Even though there is such proof that late fusion is better than early one many researchers prefer late fusion over early fusion integration of modalities involve rules such as the Bayes rule fusion majority vote for this approach Late fusion methods are common as they resemble human cognitive abilities and can be integrated to generate single mon decision Once we go to the higher abstraction level the importance of content decreases hence the actual fusion level plays vital role late fusion approach is ically depicted in Fig FIGURE hybrid fusion approach TABLE Comparision between early late and hybrid fusion techniques based on prominent features FIGURE late fusion approach HYBRID FUSION The deep neural network acts as building block for mediate fusion It is the most widely used approach It changes input data to abstraction Hybrid fusion learns joint representation of different modalities The fusion takes place at the commonly shared representation layer The loss is propagated back to the feature extractor network during the training process Various modalities can be combined using slow or gradual fusion However this kind of fusion may lead to model overﬁtting and fail to learn the different correlations Deep multimodal fusion mance is improved by reducing the data dimensions After constructing shared representation layer PCA and auto encoders are used Hybrid data fusion is far superior to early and late fusion slow or gradual fusion approach to integrating multiple fusion layers by fusing modalities from their high to low contribution performs well hybrid fusion approach is depicted in Fig ison of early late and hybrid fusion approaches is shown in Table RECENT DATA FUSION TECHNIQUES In the last few years more sophisticated approaches to fuse multimodal data are reported in the literature These are presented in this section The fusion of unimodal embedding spaces into common joint or shared representation ing knowledge of semantic visual attributes and contextual language features is necessary to perform various multimodal integrated tasks The most common and widely used multimodal fusion pooling techniques reported in the erature are concatenation multiplication and weighted sum The model learns intra model features than the intermodal one the greedy pretraining approach is also used in different settings The tensor fusion network where the unimodal bimodal and trimodal interactions are modeled using Cartesian product is presented in It imposes high computational ments and complexity on the system All the modalities are used without any extraction The multimodal fusion technique addresses the shortfall of tensor fusion networks by using tensors for fusion but results in complex architecture with lot of computation and cessing multimodal fusion techniques provide excellent performance but some approaches only consider bilinear or trilinear pooling which considers correlations but results in very high dimensionality issues large outer product computation which lacks exploiting the multilinear fusion power Feature fusion at single instance ignores the local tions leading to performance degradation slow gradual fusion approach fusing modalities from higher to VOLUME lower predictive power is recommended Multimodal deep learning performance is improved by maximizing the variation in mutual information of different channels Other approaches such as Multimodal Tucker fusion pose and form matrix decomposition overcomes bilinear polling s drawbacks by reducing complexity through tucker decomposition fusion for view sequential learning models the and interactions for datasets in Dynamic adaptive fusion scheme where the network decides the optimal way to fuse the modalities dynamically is sented in fusion by exploiting tion across modalities by exchanging modality Channel is interpretable to large extent ing fusion that can model the inter and intra model off during fusion with dynamic approach and channel exchange are proposed The multimodal fusion task also been modeled as neural architecture search algorithm to ﬁnd an appropriate search space and suitable architecture to fuse the modalities neural model architecture based on global workspace theory from cognitive science is proposed to cope with uncertainties in data fusion with attention models across different modalities Deep work with common and unique sequence mation is proposed for sentiment analysis that models the inter and intra modalities with reliance on attention anism Deep multimodal data fusion results can be optimized by including the model ity interpretability justiﬁcation and reasoning capabilities for better and improved performance and predictions summary of recent multimodal data fusion approaches is presented in Table This section provided review of the traditional and recent methods for data fusion techniques ious multimodal applications are also provided However all the ways especially the approaches employ the models which are hard to interpret The lying functioning of these networks is not evident and it becomes difﬁcult to justify the outcomes of such models It is necessary to bring in the explainable AI techniques to understand and explain such methods working and processes It is even more challenging in multimodal setting due to various scales of representations alignment and resolutions than single modality setting as discussed previously In the next section we present the review of the existing explainable AI methods and their applications in multimodal AI tasks with speciﬁc reference to vision and language tasks III EXPLAINABLE AI XAI XAI is multidisciplinary ﬁeld involving different tives from social science cognitive science psychology and interaction Artiﬁcial Intelligence tems powered by deep neural nets have achieved art results in various domains including Computer Vision Natural Language Processing and Speech recognition Joshi et Review on Explainability in Multimodal Deep Neural Nets TABLE Recent multimodal data fusion techniques But the primary focus lies on building intelligent systems achieving higher accuracy and predictive power ing the trust and transparency aspect The underlying complexity and hidden layer processing in the deep ral nets make them opaque and black box models with an accuracy and interpretability tradeoff more performing models are less interpretable It is hard to understand interpret and explain these models internal processing and processes limiting their social acceptance and usability In general systems are interpretable if humans understand and interpret their working mechanism and process by asking questions like why the system made particular prediction Why answer the interpretability aspect and how justiﬁes how the system came up to speciﬁc decision answer the explainability part Interpretability is the degree to which human can stand the cause of decision and can consistently predict the model s results Deep neural nets lack interpretability as it is difﬁcult to analyze which modalities or features are driving the predictions scenarios ing human lives by automated algorithmic decision tance such as in legal healthcare ﬁnance transport military and autonomous vehicles we expect AI systems to provide their predictions with proper evidence and justiﬁcation The system s explanation should be human interpretable and understandable mapping the human mental model to build trust transparency reliability for success and failure robust fair and unbiased applications underlying ethical machine learning principles Explainability is legal concern to comply with the EU General Data Protection and Regulation GDPR act asking for Right to tion to the users of an automated tem To understand the inner working and learning mechanism model to analyze the predictions design improvement detect and mitigate VOLUME Joshi et Review on Explainability in Multimodal Deep Neural Nets sarial attacks artifacts in the datasets required in handling bias and fairness issues in the models explainability become prime concern to be addressed The XAI ﬁeld is continuously evolving and is critical in developing new AI algorithms and methods to explore how the models work and why they succeed or fail to improve their design driving us towards the responsible AI paradigm of the future AI models are often right but sometimes for the wrong However the justiﬁcation for the decision is often absent or vague Explainability in deep neural nets can be introduced in three different model training and development stages namely in pre modeling modeling and phase LEVELS OF EXPLANATION MODELLING Pre explainability is included before the model development process It primarily involves understanding and describing the data using exploratory data analysis dataset documentation dataset rization explainable feature engineering techniques data collection without biases and good experimental design to ensure clarity During of inherently able models such models are explainable by design and typically employ intrinsic methods Hybrid els such as deep KNN Contextual Explanation work Neural Network joint prediction and explanation and architectural adjustments through regularization techniques are examples Post hoc Modelling is implemented after the model is developed by extracting explanations for already oped models through perturbations gation methods and proxy models as it can be freely applied to any model without any constraints B SCOPE OF EXPLANATION The explanation s scope is either local or global ing on whether the explanation is derived from single data instance or the entire model Often local explanations are preferred over the global as they derive predictions for particular data point rather than the model as whole XAI taxonomy is represented in depicting the ability stages scope and working principle MODEL SPECIFIC EXPLANATION explanations apply to speciﬁc model Human interpretable explanations such as methods have been proposed for the convolution neural networks However they do not focus on the entire region in an image to address query MODEL AGNOSTIC EXPLANATION Model agnostic methods that are independent and irrespective of the model have general modularity in design and can be applied for other domains Various post FIGURE XAI Taxonomy hoc explainability approaches such as partial dependent plot PDP show the marginal effect of the ﬁnal predictions features Individual conditional expectation ICE plot visualize the dependence of features on the ﬁnal prediction for unique data points providing more insights than the PDP approach of averaging on all data points Permutation feature importance works because particular feature is important if alteration of that feature results in large model errors Local interpretable explanations LIME is method applied for single data point and observes change in the output based on the corresponding shift in input Shapley values are used when each data point s contribution is variable in the ﬁnal prediction However they work in collaborative environment for ing ﬁnal predictions Prototypes and criticisms help in mizing the overgeneralization of the dataset prototype is data point that represents the entire dataset Criticism is data point that is not well represented by the prototype both type and criticism describe the data and lead to interpretable predictions Inﬂuence functions show the inﬂuence of each feature on the ﬁnal prediction They are used to stand model behavior model debugging detect dataset errors and even create adversarial attacks In model agnostic visual explanation method for the dictions of object detectors irrespective of the model s inner working is presented Human importance aware tuning HINT method improves visual grounding by attending to the same visual features in an image which humans ﬁnd important for predictions are few further ples of model agnostic methods FEATURE ATTRIBUTION BASED METHOD Feature methods highlight image regions that are signiﬁcant contributors in however it lacks semantic reasoning and interactions Following ods are reported in the literature Visualization techniques focus on highlighting the input features most contributing and affecting the model s put They are classiﬁed into back and methods Back methods look for relevant features based on gradients passed through the network Visualization techniques such as weighted activations in Class Activation Mapping CAM methods like saliency maps that focus on pixel intensities are based on high contributing tures Gradient Input improves the sharpness VOLUME of the attribution maps The attribution is computed using the input s partial derivatives concerning the input and multiplying them by themselves captures gradient information in layers to produce localization map of important features Integrated ent computes the model s prediction output gradient to its input features and requires modiﬁcation to the original deep neural network DeepLift calculates contribution scores based on reference activations or methods based on mathematical decomposition Layer gation LRP computes backward relevance tion to highlight the contributing features Shapley Additive explanations SHAP Shap is the average contribution of all data points in prediction All these methods require access to the model parameters and thus an understanding of the model architecture Methods Visualize feature relevance by comparing output between inputs and altered or changed copy of the input like Occlusion sensitivity RISE LIME etc to identify sensitive features for prediction are proposed DISTILLATION METHODS Distillation methods are classiﬁed into local approximation models that build an approximate local model to derive insights for predictions based on single data point ple LIME The model translation method builds surrogate model on top of the original model whose interpretation is expected decision trees that are inherently explainable INTRINSIC METHOD Intrinsic methods are inherently explainable They explain using models attention mechanisms that focus on important visual and textual regions Joint training approaches that jointly model the predictions and tions fall under this category and are further discussed in section Fig represents the taxonomy of various unimodal and multimodal deep explainability methods which are sented here Different deep XAI techniques are presented in Table IV EXPLAINABILITY IN MULTIMODAL DATA SIGNIFICANCE OF EXPLAINABILITY IN MULTIMODAL DATA Multimodal explanations play vital role in building ligent systems powered with understanding and reasoning capabilities inherent and integral to humans In world settings systems that are performing and able are desired Unimodal vision or language systems offer either visualizations of important input features or post hoc justiﬁcations incapable of providing introspection and reason in multiple situations and ios In contrast the multimodal setting of explanation explores the complementary and explanatory strengths in the different modalities leveraging improved explanations that can justify localize the evidence better supporting the ﬁnal decision and offer signiﬁcant beneﬁts over unimodal Joshi et Review on Explainability in Multimodal Deep Neural Nets FIGURE Methods for explaining deep neural nets FIGURE Multimodal explainability workflow pipeline approaches In speciﬁc scenarios language modality may provide more insights and valuable information than the visual one to understand the concept and rationale better and vice versa showcasing the complementary behavior of modalities in which one modality assist in enhancing the performance of the other Multimodal sources extract more and comprehensive information from varied sources Hence they can offer diagnostic strengths that help understand the model working mechanism model debugging identify ﬂaws in the model and ensure whether the model works as intended The multimodal explainability models can also identify adversarial attacks and defense mechanisms fairness and bias providing scope for troubleshooting rectiﬁcation improving overall model performance tive and explanatory power The primary explainable goal for an opaque black box explanatory system such as deep neural nets is to answer how and why the model makes certain prediction by inspecting the driving factors behind their decisions In reality VOLUME Joshi et Review on Explainability in Multimodal Deep Neural Nets TABLE Different XAI techniques for explaining deep neural nets it is found that many explanatory systems are based on tiﬁc modeling than explanation generation and hence are not and They do not satisfy the needs of different stakeholders For instance consider the task of interpreting an for diagnostic purposes the radiologists are keen on mapping the visual evidence in the image to predict the diagnosis On the other hand the general clinician is more interested in the ﬁnal tion of the case This demands justiﬁcations that are based satisfying the requirements of different stakeholders simultaneously and show the interplay between multimodal interactions required for building interactive explanation interfaces followed with feedback mechanism able to porate change in the system Evolution of vision and language tasks transformed from simple tasks requiring cessing of fused multimodal embeddings such as image tioning VQA to complex tasks such as visual common sense reasoning requires reasoning and deep standing of semantic context Such tasks demand the model to comprehend natural language and identify objects in the scene and capture inherent relationships between vidual entities present in the input The model s ability to reason their predictions become essential leading to the emergence of Explainable AI for multimodal settings Human visual explanations can help systems know where to attend human textual explanations and how they attended image regions to complement multimodal explanations INTROSPECTION AND JUSTIFICATION SYSTEMS Deep explanatory systems are broadly siﬁed into justiﬁcation or tems Humans understand the explanations but it lacks in deriving the causal aspect and interpretation The systems focus on the network s internal behavior but are not well understood by humans In settings justiﬁcation and spection both systems are desired as the generation of lucid explanations improves human understanding and performance signiﬁcantly MULTIMODAL EXPLAINATION METHODS In this section we provide the classiﬁcation of deep based multimodal explanation techniques based on the approach they follow to explain visual and textual modalities into methods active approaches and methods following the taxonomy represented in Fig ATTENTION BASED APPROACHES approaches focus on certain factors in the data more than others by assigning more weight and tance In multimodal tasks such as visual captioning visual question answering or visual entailment attention nisms play crucial role in the alignment and fusion across different modalities In the approaches such as in visualization methods like the tion features provide the explanation Attention mechanisms are primarily used for the VQA task to attend or focus the image region on answering the query correctly The VQA task of answering natural language questions about images is explored in Efforts primarily focus on ing interpretable models with speciﬁc interest in exploring the input images or text in questions The VQA model looks at these texts or images while answering the question In VOLUME guided backpropagation and visualization techniques are employed to showcase vital regions on the image and text on which the VQA model focuses To idate if explanations make VQA models more interpretable to humans in the authors have proposed metrics for failure and knowledge prediction and found that human in loop HIL approaches make the model more understandable to humans The pioneering work in generating multimodal tions was proposed by Park et based on Teaching AI to Explain its Decision TED approach The Pointing and Justiﬁcation model for the visual tion answering using and activity recognition task using datasets use attention mechanisms to explain the answer of VQA task with textual explanations and corresponding visual regions They pass attention masks between modules and explore modalities complementary and diagnostic strengths emphasizing the multimodal explanations But this kind of model provides an indirect inconsistent explanation of the model s internal working identifying model ﬂaws and requires dataset that is augmented with explanations annotations In authors developed multimodal approach generating explanations supporting deep network decisions in attentive pointing maps and text In this model clinical diagnostic decision is conveyed with visual pointing and textual explanation in coordinated fashion with visual word constraint model In authors proposed faithful multimodal explanation framework with and attention anism to have consistent visual and textual explanations by segmenting the image for precise localization ing rationales improve human understanding and quality of explanations In the authors used supervised attention model that trains human rationales to generate explanations The VQA task requires different capabilities at varying els discusses the VQA models form well on perception and reasoning questions but provide inconsistent explanations modeling task Network Tuning SQuINT forces the model to attend the similar areas of the image when ing the reasoning and the associated perception thereby improving performance and consistency Efforts have been made to leverage explainability in the task of VQA by using textual and visual explanations using parse trees Hierarchical patterns to provide valid nations and in sequential data using attention In module for spatial grounding in VQA is proposed This model addresses several visual recognition challenges including the ability to infer human intent the reason both locally and globally about the image and tively combines visual language and spatial inputs In authors evaluated the effect of explanations on the user s mental model for the VQA task by proposing an explainable VQA system using spatial and object features using the BERT language model on user perception of competency They Joshi et Review on Explainability in Multimodal Deep Neural Nets generated visual and textual explanations to complement the knowledge base enhancing the model s prediction and interpretability Multiple tasks image attention and edge base improve the overall model performance In authors have introduced training method that ensures that image explanations of correct answers map more than other competitors image areas mapping with the human mental model and decrease the incorrect answer probability in this region In the joint probability in VQA is imized by Hierarchical Feature Network where each hierarchical feature combines the attention maps with semantics Textual explanations are also generated for cars using an attention mechanism and video description tasks In the proposed action cation model is based on the evidence using conditional variation autoencoder CVAE which provides better results than attention approaches and improved grounding between humans and agents In authors have proposed an approach to enhance VQA performance by comparing competing explanations In Zellers et provided explanations for visual common sense reasoning through multiple choices They proposed Visual Commonsense Reasoning VCR task that answers text question based on an image and provides reasoning accordingly Both the answers and justiﬁcations are provided in generating explanations along with decisions Thus VCR is more suitable to be applied for prototype model debugging to audit the model ing process In the multiword answer and rationale model for ViQAR are generated that goes beyond VQA in abstraction and reasoning abilities An interpretable tion mechanism is recently used in for predicting anticancer compound sensitivity using based convolution encoders for the task of drug discovery Despite the wide use and beneﬁts exploration challenges persist with attention mechanism as visual explanations erated using attention mechanisms do not explain if the model attends the right area The region in the image to be focused on answering particular question is not ﬁxed Lack of ground truth for evaluation of explanations imposes several restrictions Attention maps do not look for the same area as humans do Attention s explanatory power is tionable as they lack an association with the tion weights and gradients mapping for generating faithful explanations COUNTERFACTUAL EXPLANATIONS Human thinking is contrastive and causal in the form of cause and effect We ask why particular X decision is taken and why not instead For example suppose ciﬁc loan application is rejected In that case we are more interested in the measures and minimum possible changes to be undertaken to ﬂip the decision to be accepted in the future Multimodal explanations based on counterfactuality provide recommendations that provide actionable insights and recourse Specifying the minimal desired changes VOLUME Joshi et Review on Explainability in Multimodal Deep Neural Nets required to ﬂip the decision in favor of the user mapping well with the human mental model leveraging the and discriminative features and enhancing the model trust transparency accountability reliability social acceptance and usability Interactive machine learning with AI approach paves the way towards modal causal learning Some efforts in this direction are highlighted In setting the visual tion is the region with high positiveness or negativeness to measure how the target classiﬁer changes corresponding to the negative class when speciﬁc area is removed from the input using accuracy The textual explanation is compatible with the visual counterpart and measures how the target siﬁer changes corresponding to the negative class when ciﬁc region is removed from the input using accuracy In textual explanation model is proposed to tigate why the model predicted class x instead of class X based on counterfactuals They offered model using explanation annotations and its counterparts The model improves the textual explanation quality but explanations are not accurate and faithful to the ing model In counterfactual visual explanation is generated based on the paper by Hendricks et The explanations are directly generated from the base model from the model s neurons and are accurate for the ing model and are without additional attribute annotations In multimodal data classiﬁcation model is built to classify video to particular class and justify why it is not classiﬁed to the other on counterfactuality Kanehira et developed complementary tion model by applying joint training approach to erate prediction and explanation maximize the modality interaction information and ensure that the explanations are complementary to each other There is persistent language bias in VQA models In counterfactual setting is employed where visual ground truth input is considered to be absent in particular case Similarly studying the effect of visual biases synthesized similar but different images than ground truth that learned how and why the output change with visual distortions For visual captioning counterfactual explanations help analyze the models working mechanism and the reasons behind certain predictions Counterfactual explanations emphasize that observations present or missing lead to speciﬁc output In counterfactual resilience in image descriptions is obtained by parsing entities tic attributes and color information separately Contrastive learning provides neural models with tence using relevant and irrelevant pairs It improves modal representations in pretraining handling noise and bias in the data INTERACTIVE APPROACHES An interactive approach to explanation leverages parency in machine learning systems Interpretability is also explored by analyzing the accuracy in prediction for VQA in interactive systems In an interactive active model is proposed that alters the model tion and provides user feedback if the forecast is incorrect by combining model explanation and annotation and evaluating the model explanation on the metric of user trust mental model and usability In the authors suggested the use of virtual agents to generate better multimodal explanations Alipour et evaluated multimodal explanations for VQA with and without explanations and established that explanations improve accuracy if the VQA system is wrong They introduced an active attention mechanism ering different attention maps The generated explanations and annotations with feedback loop are combined to rectify wrong answers In the authors have proposed XAlgo an interactive algorithm explaining the system s internal state through question answering conversational systems are designed to provide better nations than conventional systems for customer relationship management with an interactive approach for multisensory fusion GRAPH BASED APPROACHES Reasoning out the task is carried out with learning interactions in the image scene graph Visual Reasoning task requires machines to ideally look beyond the face value of any image to capture correct relations and context before generating suitable descriptions In semantic attributes are present in the scene with semantic bottleneck based on context The Context Semantically Interpretable Bottleneck CSIB vides clear and interpretable explanation of each prediction by making the decision process of CNN more interpretable EXPLAINATION USING SCENE GRAPHS The scene graph for an image is the graphical representation of its contents The nodes are the depicted objects and the edges are the relationships between them In graphs with only objects and relations are generated uses multimodal approach for generating textual explanations for the visual question answering task using both image and language modalities without collecting any additional data and generated natural language explanation for VQA using scene graph and visual attention mechanism With two variants based on region descriptors objects and tions proved multimodal efﬁcacy approach empirically This approach doesn t rely on manually generated explanation data as they use already available annotations from scene graphs addressing the model s drawbacks proposed by Park et KNOWLEDGE GRAPHS Knowledge graphs signiﬁcantly increase interpretability and explainability through semantic information and domain knowledge base infusion in healthcare and educational tors Multimodal Hierarchical Attention Network in which knowledge graph with tiple modalities and different features is built for the medical VOLUME ﬁeld In comprehensive view on the neuro bolic AI perceptive is provided and integration of knowledge graphs in deep learning models for model interpretability is proposed ATTRIBUTE BASED METHODS The signiﬁcance of attributes in providing explanations is far more critical and there are efforts in this direction In the authors proposed an explanation model using attributes counter attributes with examples and counter visual ples To establish the intuitiveness of attributes in building class discriminative multimodal explanations associating visual features with attribute information and better text image grounding for the visual reasoning task In the authors proposed the modal approach for generating the textual justiﬁcation from the visual image attributes to represent and explain novel concepts with minimal supervision Robust representations are generalized across different tasks using spatiotemporal attention mechanism and joint training approach for visual and textual modality in the learning paradigm In the FLEX model is introduced that generates faithful language explanation for CNN decisions even for unseen class and provides rationale proposed Semantically Interpretable Activation Maps SIAM Attribute maps are linearly combined to see what different features the model learned and how particular regions enhance interpretability Visual common sense reasoning uses other object detection techniques for better text to image grounding and assigns attributes to object grounding with fewer parameters Activation maps are used in by sidering different class pairs as complementary Therefore they can provide more discriminative cues to generate CAM using representative classes with discriminative cues using complementary regions activated for better and accurate CAM generation In model produces textual and visual explanations improving user trust and model ﬂaws by generating complementary multimodal explanations To justify classiﬁcation decision in learning ZSL paradigm with joint visual attribute ding and explanation module to generate multimodal explanation is proposed Different multimodal explanation generation approaches are represented in Table VI EXPLAINATION EVALUATION Evaluation of explanations with metrics and methods is major point of concern to leverage explainability and user conﬁdence as they are and subjective Explanation evaluation tasks are categorized into the tionally grounded evaluation such as depth of decision tree evaluation evaluation asks crowd ers for their preferences for better analysis and feedback evaluation involves human mental model human expert simultaneously explaining the outcome of the model Explanations are diverse and subjective making their evaluation difﬁcult without access Joshi et Review on Explainability in Multimodal Deep Neural Nets TABLE Multimodal explainationation generation approaches to the ground truth Evaluation of explanations without ground truth based on generalizing ability persuasibility is proposed in with criteria for good explanation ization performance ﬁdelity faithfulness relevance per bility usability user satisfaction and causality Human users can associate with the cause and effect scenarios Better explanations involving novelty and completeness are desired Explanations shall support actual model ior to be accurate The model simulatability perceptive duces simulatability metric for evaluating textual explanations making human users predict the output Human interpretable evaluation of explanation is based on the accuracy response time consistency and satisfaction established through simulation veriﬁcation and counterfactuals Automated evaluation metrics such as is used for automatic evaluation of machine translation that is quick inexpensive and that correlates highly with human evaluation and that little marginal cost per METEOR is an automatic machine translation metric used for unigram matching between the translation and ence translations ROUGE counts the number of overlapping units such as word sequences and word pairs VOLUME Joshi et Review on Explainability in Multimodal Deep Neural Nets between the summary to be evaluated The ideal summaries created by humans CIDEr are human image description tion metric for automatically evaluating image descriptions SPICE is semantic propositional image caption uation metric for the automatic assessment of image ing and improved evaluation compared to Cider METEOR Automated evaluation and human evaluation of explanation through crowdsourcing are followed for evaluating tions in In Explanations are evaluated based on the novel metric from the paradigm that sures the degree to which the explanations help the student model simulate teacher model in an improved way for better scalability for unseen data schematic workﬂow pipeline of the fusion methods tasks applications multimodal predictions and explanation generation approaches is presented in Fig VII DIVERSE EXPLAINABILITY REQUIREMENTS Explainability requirements are often user role and goal speciﬁc and hence are diverse For instance model creators might require an understanding of how layers of deep network respond to input data to debug or validate the model In contrast often need functional explanation to understand how some output outside model is produced For instance mode l examiners might require an understanding of how model uses input data to predict to ensure that the model is trustworthy not biased or comply with the regulations Modular approaches have higher interpretability The multimodal task of VQA becomes more interpretable and coherent with multimodal interpretability wherein the answers can be justiﬁed in both textual and visual formats VIII DATASETS FOR VALIDATING THE EXPLAINABILITY IN MULTIMODAL NEURAL NETS Multimodal XAI domain remained unexplored due to the lack of availability of datasets Recently efforts in this direction are started In new dataset is proposed based on Raven s Progressive Matrices RPM for the task of visual recognition reasoning comprising images and related RPM problems with annotations based dataset is sampled from the available VQA and Visual Genome datasets for the release This work focused on countable quantitative question ing for answering speciﬁc queries asking how many In two novel datasets dedicated to explainability for visual tion answering and activity recognition tasks comprising textual justiﬁcations for each input pair The dataset since then been sidered benchmark for many other explainable models In the dataset is proposed for VQA dictions and explanations to improve overall performance CUB dataset an learning baseline dataset is signiﬁcant for methods for multimodal bias evaluation are some lar multimodal explanation datasets The dataset s availability makes the explainability goal more understandable and able in all contexts to leverage future research in the ﬁeld IX MULTIMODAL BIAS AND FAIRNESS Modalities exploit at different scales and their tion can vary laying more importance on certain ity over the other providing suboptimal results Imbalance data and feature selection introduce biases in models and machine learning algorithms leading to lack of fairness and transparency Familiar sources of bias are through sourcing workers and natural perceptions Bias persists in word embeddings and at the sentence level Algorithms often replicate and amplify the bias in the multimodal datasets To detect and mitigate the bias in proposed ization approach based on maximizing functional entropy and balancing modality contributions Diversity in multimodal information often leads to such biases In how biases affect automated recruitment systems is demonstrated Even after masking the inputs gender and ethnicity discrimination based on bias are present in the records In the VQA task the models often pick up statistical irregularities introducing bias leading to memorizing rather than learning the task with wrong evaluation Unimodal biases in the textual inputs neglect visual information impacting multimodal aspects Such biases often lead to massive drops in performance when confronted with data outside training distributions Generalized and trivial questions are commonly answered with prior lingual knowledge instead of querying the image Therefore keyword dependencies over correct image reasons are necessary to obtain accurate interpretable models and are comprehended via attention maps For the task of image captioning visual cues in the training images carry bias Most models have gender bias Other such efforts focus on two signiﬁcant subtasks or captioning in case of occlusions and correct gender classiﬁcation otherwise Such methods make multimodal frameworks more reliable interpretable by allowing the models to provide reasonable predictions for the right reasons rather than looking for mere performance looking at cause and effect aspects ADVERSARIAL ATTACKS ENHANCE INTERPRETIBILITY In addition to small changes or alterations perturbations called adversarial perturbations result in adversarial ples leading to change in output These adversarial ples can mislead the classiﬁers to make wrong classiﬁcation decisions They are so minute that human eyes often missed them Adversarial examples can also be used for ing neural networks Masking visual modality to see the partial or complete inﬂuence of statistical language patterns through adversarial attacks classiﬁcation by attributes they are also natural candidates to study misclassiﬁcation and robustness In the interpretation of adversarial ples discriminative attributes predict the correct and wrong class predictions and adversarial perturbation increases the network s robustness In adversarial VOLUME attacks for the VQA model are proposed The show and fool algorithm for attacking visual grounding with adversarial examples shows that models can be fooled through ial attacks despite attention and localization showcasing the need to establish strong defense and mitigation mechanisms against adversarial attacks XI OBSERVATIONS AND RECOMMENDATIONS Existing unimodal vision or language systems offer either visualizations of important input features or post hoc justiﬁcations Lack of complete introspection and justiﬁcation highlights the need for further exploration of multimodal approaches for explainability Natural language explanations can be mutually sistent There are persistent inconsistencies and biases in the multimodal data such as in visual question answering Measures to tackle prevalent bias and ness issues are of utmost importance Even if the classiﬁer is very accurate without having access to complete explanations to understand how decisions are made in the model it is not sure that it is making decisions for the right reasons The model may learn things that should not affect generalization and focus on faithful explanations complete to model and substitute task Building trust by asking questions transparency and accountability is not enough we need auditable explainable interactive augmented and learning from human feedback Artiﬁcial intelligence supplemented with human intelligence with human in the loop approaches for better evaluation should be proposed Conﬁdence that human knows how and why the sion was made should follow with recourse in case of dissatisfaction In case of disagreement with systems output measures to change the same should be feasible which can be accomplished with the interactive system design with feedback loop Multimodal diagnostic and explanatory capabilities need to be further explored for inconsistencies to age comprehensive and and discriminative feature understanding of relations and interpretation of explanations with hybrid composite approaches to model multiple data modalities Multimodal explanation generated may not be related complete and faithful accurate to the model s internal decisions and processes Hence lack of proper grounding reasoning and context adaptation will focus on developing new models Explanations lack addressing the lay users ers domain experts and stakeholders to simulate the human cognitive process and design There is need for interactive trustworthy and diverse multimodal explanations to satisfy different stakeholders needs Joshi et Review on Explainability in Multimodal Deep Neural Nets Explanations lack proper evaluations on bias fairness trustworthiness ﬁdelity generalized ability causality completeness novelty and quality in mapping the human mental model due to the diversity aspect in the evaluation metrics subjective nature of explanation Different automatic and human ation metrics with theoretical backgrounds need to be developed Despite the great insights into various explanation modes efﬁcacy previous studies do not interactively involve the human subjects and feedback in ducing these explanations Involving human subjects can improve the quality and trust and usability of explanation Multimodal explanations lack robustness and are prone to adversarial attacks through small input tions An adversarial defense mechanism shall be established to combat the situation Measures to tackle the language bias and fairness issues in the data due to diverse information from multiple modalities are carried out on broad scale There is need to evaluate the multimodal explanations on the grounds of causality and visual grounding An explanation lack infusion of the prior domain knowledge base required in the cess and explanation where knowledge graphs provide promising direction XII DISCUSSIONS AND FUTURE DIRECTIONS Multimodal research achieved much success in various downstream tasks However there is still long way to meet up to the human level performance as challenges sist with representation alignment translation fusion and Lack of common sense and reasoning tual adaptation labeled data requirements development of novel and improved architectures and evaluation metrics are still prevalent explanation theory is foundationally established and is more explaining with analogies and examples multiple modalities contrastive and counterfactuals explanations show promising edge in this way as an explanation is as vital as prediction and are jointly modeled The explanation structure shall be based on the human mental model and evaluated with human in the loop approach to better understand outcomes An mous scope of improvement exists for building interpretable and explainable models for multimodal settings instead of explaining the model predictions in post hoc manner ent and model should be developed leveraging visual and textual modalities integrated beneﬁts The type of explanation appealing to humans is not established multimodal interactive explanations with user feedback can improve quality and user satisfaction Interacting with nations of machine learning models is an enabler for entiﬁc discoveries for interaction Due to the engagement of multiple modalities multimodal nations leverage the ability to satisfy different stakeholder s VOLUME Joshi et Review on Explainability in Multimodal Deep Neural Nets requirements with global context enabling the ment of vigilant AI systems that offer trust and improved transparency in the process Combining and approaches results in pretability and accuracy in the models Multimodal tions involving different visual and textual modalities will better understand the semantic and context in understandable format Explainability in multimodal nets becomes prime requirement in critical domains such as medical legal ﬁnance autonomous vehicles robotics and other ﬁelds with diverse modality involvement The centric interactive approach in design backed up with tive domain knowledge leads to quality explanations fostering new facets and better understanding and traceability of model working and decisions with XAI in hand to achieve integrated interpretability Contrastive learning probabilistic graphical models like causal networks and counterfactuals open vast arena of interpretable and transparent deep learning algorithms thereby reducing overall bias and increasing the system s reliability Simultaneously it opens the requirement for novel datasets for models and relevant metrics quantifying the separation from data and suring the reasoning and cognition capabilities over complex datasets Adversarial attacks tend to enhance the system s robustness and interpretability encountering the need for defense mechanisms Recently OPEN AI have reported multiple algorithms evant to this body of work which are presented in next subsection OPEN AI AND CLIP MODELS Multimodal learning aims to learn concepts through several modalities After GPT open AI came up with based architecture that combines image and language with Dall and CLIP models Unlike biological neuron timodal neuron can represent abstract features and concepts in theme compared to single feature sentation Understanding the literal symbolic and conceptual meaning is crucial development in multimodal learning Dall is image generation model from text captions that ﬁlls the gap between vision and text for wide range of concepts explained in ral language It uses parameter version of the transformer model to interpret natural language inputs and generate corresponding images from text captions lar to CLIP building visual concepts through language The second breakthrough in Multimodal AI introduces Open AI s CLIP model trained on image sentence similarity scores connecting images and text model that stands for contrastive language image pretraining classifying image text pairs Inspired by the learning paradigm it uses single pretraining task to generalize to other interest domains This multifaceted neuron s working is interpreted by different ture visualization and data example techniques with tional efﬁciency due to the approach Still it can not generalize for all the tasks Despite the performance the CLIP model is subject to associative bias in the underlying data The Middle East neuron was associated with terrorism An immigration neuron responded to Latin America ing speciﬁc group of people and typographic adversarial attacks are reported on this model raising severe concerns about its social acceptance There is still an urgent ment to analyze the models societal impacts concerning the data biases that they carry Multimodal explainability feature visualization and data examples techniques play promising role in understanding the working mechanism and detecting and dealing with the underlying biases and adversarial attacks on these models OpenAI investigates their recent CLIP model s inner ings via faceted feature visualization and deduces ﬁndings that some neurons in the last layer respond to distinct cepts across multiple modalities The neuron ﬁre for tographs drawings and signs depicting the same concept even when the images are vastly distinct They identify and investigate neurons corresponding to persons geographical regions religions emotions and much more Both and CLIP represent signiﬁcant advancements in transformer models Indeed they are important milestones for the puter vision community XIII CONCLUDING REMARKS This review described and categorized the different ability methods and techniques in multimodal setting The importance of explainability techniques concerning diverse image and text modalities in vision and language settings is fascinating The context reasoning and semantic attributes harnessed by different explanation methods focus on the proper distinction Crucial aspects of bias fairness and sarial defense mechanisms aligned with explainability in timodal nets are highlighted To derive the full beneﬁts of multimodal setting new benchmarks and diagnostic datasets play prime role Explainability is prominent aspect in multimodal environment establishing trust and transparency in the working mechanism understanding and tracing model ﬂaws Recently the pace with which the ﬁeld is evolving the upcoming developments are tremendous We hope our survey is step to provide roadmap for further ments and research directions in this active domain REFERENCES Mogadala Kalimuthu and Klakow Trends in integration tasks datasets Online Available of vision and language research survey of and methods https LeCun Bengio and Hinton Deep learning Nature vol pp May doi Young Hazarika Poria and Cambria Recent trends in deep learning based natural language processing review article IEEE Comput Intell vol pp doi Park Hendricks Akata Rohrbach Schiele Darrell and Rohrbach Multimodal explanations Justifying sions and pointing to the evidence in Proc IEEE Conf Comput Vis Pattern Jun pp Online Available https VOLUME Adadi and Berrada Peeking inside the vey on explainable artiﬁcial intelligence XAI IEEE Access vol pp Molnar Interpretable Machine Learning Morrisville NC USA Lulu Press Goebel Chander Holzinger Lecue Akata Stumpf Kieseberg and Holzinger Explainable AI The new in Proc Int Conf Mach Learn Knowl Extraction pp Miller Explanation in artiﬁcial intelligence Insights from the social sciences Artif vol pp Lahat Adali and Jutten Multimodal data fusion An overview of methods challenges and prospects Proc IEEE vol pp doi Ngiam Khosla Kim Nam Lee and Ng Multimodal deep learning in Proc Int Conf Mach Learn ICML pp Liu Li Xu and Natarajan Learn to combine modalities in multimodal deep learning Online able https Liu Xiang Hospedales Yang and Sun Inverse visual question answering new benchmark and VQA diagnosis tool IEEE Trans Pattern Anal Mach vol pp doi Agrawal Batra Parikh and Kembhavi Don t just assume look and answer Overcoming priors for visual question answering in Proc Conf Comput Vis Pattern Jun pp doi R Selvaraju Das Vedantam Cogswell Parikh and Batra Why did you say that Online Available http Chandrasekaran and Chattopadhyay Do explanations make VQA models more predictable to human in Proc Conf Empirical Methods Natural Lang pp Schramowski Stammer Teso Brugger Herbert Shao Luigs Mahlein and Kersting Making deep neural works right for the right scientiﬁc reasons by interacting with their nations Nature Mach vol pp Hendricks Akata Rohrbach Donahue Schiele and Darrell Generating visual explanations in Proc Eur Conf Comput pp Anne Hendricks Hu Darrell and Akata Generating terfactual explanations with natural language Online Available http Hu Andreas Rohrbach Darrell and Saenko Learning to reason module networks for visual question answering in Proc IEEE Int Conf Comput Vis ICCV pp doi Johnson Hariharan Van Der Maaten Hoffman Zitnick and Girshick Inferring and executing programs for visual reasoning in Proc IEEE Int Conf Comput Vis ICCV pp doi Johnson Hariharan van der Maaten Zitnick and Girshick CLEVR diagnostic dataset for compositional language and elementary visual reasoning in Proc IEEE Conf Comput Vis Pattern Recognit CVPR Jul pp doi Goyal Khot Batra and Parikh Making the v in VQA matter Elevating the role of image understanding in visual question answering in Proc IEEE Conf Comput Vis Pattern Recognit CVPR Jul pp doi Xie Ras van Gerven and Doran Explainable deep ing ﬁeld guide for the uninitiated Online Available http Arrieta Ser Bennetot Tabik Barbado Garcia Molina Benjamins Chatila and Herrera Explainable Artiﬁcial Intelligence XAI Concepts taxonomies opportunities and challenges toward responsible AI Inf Fusion vol pp Jun Carvalho Pereira and Cardoso Machine learning interpretability survey on methods and metrics Electronics vol Jul Joshi et Review on Explainability in Multimodal Deep Neural Nets Tjoa and Guan survey on explainable artiﬁcial intelligence XAI Towards medical XAI Online able https Gilpin Bau B Yuan Bajwa Specter and Kagal Explaining explanations An overview of interpretability of machine learning in Proc IEEE Int Conf Data Sci Adv Anal DSAA pp Guidotti Monreale Ruggieri Turini Giannotti and Pedreschi survey of methods for explaining black box models ACM Comput vol Lipton The mythos of model interpretability Queue vol pp Jun and Kim Towards rigorous science of pretable machine learning Online able http Xie Ras van Gerven and Doran Explainable deep ing ﬁeld guide for the uninitiated Online Available https Montavon Samek and Müller Methods for interpreting and understanding deep neural networks Digit Signal vol pp Singh Sengupta and Lakshminarayanan Explainable deep learning models in medical image analysis J vol doi X I Toward Xai and Tjoa survey on explainable IEEE Trans artiﬁcial intelligence XAI Toward medical xai Neural Netw Learn early access doi Moraffah Karami Guo Raglin and Liu Causal interpretability and evaluation ACM SIGKDD Explor vol pp doi https methods Online Available for machine Holzinger Malle Saranti and Pfeifer Towards modal causability with graph neural networks enabling information fusion for explainable AI Inf Fusion vol pp Jul doi Danilevsky Qian Aharonov Katsis Kawas and Sen for natural language processing Online Available https the state of explainable AI survey of Hohman Kahng Pienta and Chau Visual analytics in deep learning An interrogative survey for the next frontiers IEEE Trans Vis Comput vol pp Mcgurk and Macdonald Hearing lips and seeing voices Nature vol pp doi Baltrusaitis Ahuja Multimodal machine learning survey and taxonomy IEEE Trans Pattern Anal Mach vol pp doi and Morency Zhang Yang Chen and Li survey on deep ing for big data Inf Fusion vol pp Jul doi Snoek Worring and Smeulders Early versus late fusion in semantic video analysis in Proc Annu ACM Int Conf Multimedia pp doi Atrey Hossain Saddik and Kankanhalli timodal fusion for multimedia analysis survey Multimedia vol pp doi Zhang Yang X and Deng Multimodal intelligence Representation learning information fusion and applications IEEE Sel Topics Signal vol pp Mar doi Amer Shields Siddiquie Tamrakar Divakaran and Chai Deep multimodal fusion hybrid approach Int Comput vol pp Apr doi Srivastava and Salakhutdinov Multimodal learning with deep Boltzmann machines Mach Learn vol pp Hase Zhang Xie and Bansal ity Can models generate explanations of their behavior in ural language in Proc Findings Assoc Comput Linguistics EMNLP pp doi VOLUME Joshi et Review on Explainability in Multimodal Deep Neural Nets Angelou Solachidis Vretos and Daras based multimodal fusion with metric learning for multimodal siﬁcation Pattern vol pp doi Yang Ramesh Chitta Madhvanath Bernal and Luo Deep multimodal representation learning from temporal data in Proc IEEE Conf Comput Vis Pattern Recognit CVPR Jul pp doi Vilone and Longo Explainable artiﬁcial systematic review http intelligence Online Available Ramachandram and Taylor Deep multimodal learning survey on recent advances and trends IEEE Signal Process vol pp doi Osman and Falk Multimodal affect recognition Current approaches and challenges in Emotion and Attention Recognition Based on Biological Signals and Images Rijeka Croatia InTech ch Sharma Agrahari Singh and Firoj Image captioning comprehensive survey in Proc Int Conf Power Electron IoT Appl Renew Energy Control PARC pp Li Tao Li and Fu Visual to text Survey of image and video captioning IEEE Trans Emerg Topics Comput vol pp Wu Teney Wang Shen Dick and Van Den Hengel Visual question answering survey of methods and datasets Comput Vis Image vol pp and Information fusion in multimedia information retrieval in Proc AMR pp Kludas Bruno Zellers Bisk Farhadi and Choi From recognition to cognition Visual commonsense reasoning in Proc Conf Comput Vis Pattern Recognit CVPR Jun pp Anderson Wu Teney Bruce Johnson Sunderhauf Reid Gould and van den Hengel igation Interpreting navigation instructions in real environments in Proc Conf Comput Vis Pattern Jun pp doi Natarajan and Shanthi survey on multimodal biometrics tication and template protection in Proc Int Conf Intell Comput Commun Smart World pp Münzner Schmidt Reiss Hanselmann Stiefelhagen and Dürichen sensor fusion techniques for multimodal human activity recognition in Proc ACM Int Symp Wearable pp doi Nweke Teh Mujtaba and Data fusion and multiple classiﬁer systems for human activity detection and health monitoring Review and open research directions Inf Fusion vol pp Mar doi Faouzi Leung and Kurian Data fusion in intelligent transportation systems Progress and survey Inf Fusion vol pp doi Kaya and Salah Multimodal personality trait analysis for explainable modeling of job interview decisions in Explainable and Interpretable Models in Computer Vision and Machine Learning Springer pp Xiao Codevilla Gurram Urfalioglu and Multimodal autonomous driving Antonio IEEE Trans Intell Transp early access doi Aranjuelo Unzueta and Multimodal deep learning for advanced driving systems in Proc Int Conf Articulated Motion Deformable Objects pp Cultrera Seidenari Becattini Pala and Bimbo Explaining autonomous driving by learning visual tion in Proc Conf Comput Vis Pattern Recognit CVPR Workshops Jun pp Yurtsever Lambert Carballo Takeda and Member survey of autonomous driving Common practices and emerging nologies IEEE Access vol pp Feng Rosenbaum Hertlein Glaeser Timm Wiesbeck and Dietmayer Deep object detection and semantic segmentation for autonomous driving Datasets methods and challenges IEEE Trans Intell Transp vol pp Mar doi Zablocki Pérez and Cord Explainability systems Review and Available of challenges https autonomous Online driving Bojarski Choromanska Choromanski Firner Ackel Muller Yeres and Zieba Visualbackprop Efﬁcient tion of cnns for autonomous driving in Proc IEEE Int Conf Robot Automat ICRA May pp Kim and J Canny Interpretable learning for cars by visualizing causal attention in Proc IEEE Int Conf Comput pp Kim Rohrbach Darrell J Canny and Akata Textual nations for vehicles in Proc Eur Conf Comput pp Holzinger Explainable AI and causability in medicine vol pp Linardatos and Papastefanopoulos Explainable AI review of machine learning interpretability methods Entropy vol Fellous Sapiro Rossi Mayberg Ferrante and Mirabella Explainable artiﬁcial intelligence for neuroscience Behavioral neurostimulation Frontiers vol doi Tonekaboni and Goldenberg What clinicians want izing explainable machine learning for clinical end use in Proc Mach Learn Healthcare pp Yang Ye and Xia Unbox the for the medical explainable AI via and data fusion review two showcases and beyond Online Available https Lucieri Naseer Bajwa Dengel and Ahmed Achievements and challenges in explaining deep learning based nosis systems pp Online Available http Alberdi Aztiria and Basarab On the early diagnosis of Alzheimer s disease from multimodal signals survey Artif Intell vol pp Jul doi Silva and Plis How to integrate data from multiple biological layers in mental health in Personalized Psychiatry Big Data Analytics in Mental Health Passos Mwangi and Kapczinski Eds Springer doi Das Boucher Rogers Makowski Klein and Beck Integration of omics data and phenotypic data within uniﬁed extensible multimodal framework Frontiers vol doi Srinivasan Chander and Pezeshkpour Generating explanations for loan denials using GANs in Proc NeurIPS Workshop Li and Yoo Multimodal deep learning for ﬁnance Integrating and forecasting international stock markets Online Available https Lahat Adalý and Jutten Challenges in multimodal data fusion in Proc Eur Signal Process Conf EUSIPCO pp Uppal Bhagat Hazarika Majumdar Poria Zimmermann and Zadeh research in vision and language review of current and emerging trends Online Available https Guo Wang and Wang Deep multimodal representation learning survey IEEE Access vol pp doi Multimodal Challenges vKaﬂe Shrestha and Kanan Challenges research Frontiers and Online Available and Artif https doi prospects vol pp language vision in Wang Tran and Feiszli What makes training classiﬁcation networks hard in Proc Conf Comput Vis Pattern Recognit CVPR Jun pp Weitz Schiller Schlagowski Huber and André Let explain Exploring the potential of virtual agents in interaction design Multimodal User Interfaces able ai pp VOLUME Girshick Donahue Darrell and Malik Rich feature chies for accurate object detection and semantic segmentation in Proc IEEE Conf Comput Vis Pattern Jun pp doi Alberti Ling Collins and Reitter Fusion of detected objects in text for visual question answering in Proc Conf Empirical ods Natural Lang Process Int Joint Conf Natural Lang Process pp doi Li Duan Fang Gong Jiang and Zhou VL universal encoder for vision and language by training in Proc AAAI Conf Artif pp doi Zhu Cao Li Lu Wei and Dai of generic representations pp Online Available http Chen Li Yu Kholy Ahmed Gan Cheng and Liu Uniter Learning universal representations Online Available https Lu Batra Parikh and Lee ViLBERT Pretraining for Online Available agnostic tasks pp http representations visiolinguistic Tan and Bansal LXMERT Learning encoder representations from transformers in Proc Conf Empirical Methods Natural Lang Process Int Joint Conf Natural Lang Process pp doi Hochreiter and Schmidhuber Long memory Neural vol pp doi Chung Gulcehre Cho and Bengio Empirical evaluation of gated recurrent neural networks on sequence modeling pp Online Available http Liggins II Hall and Llinas Handbook of Multisensor Data Fusion Theory and Practice Boca Raton FL USA CRC Press J López Val and Alonso Pérez Agreda Principal nents analysis Aten Primaria vol pp doi Tharwat Independent component analysis An introduction Appl Comput doi Correa Adalı Li and Calhoun Canonical tion analysis for data fusion and group inferences IEEE Signal Process vol pp Jul Poria Cambria and Gelbukh Deep convolutional neural work textual features and multiple kernel learning for timodal sentiment analysis in Proc Conf Empirical Methods Natural Lang pp doi Forshed Idborg and Jacobsson Evaluation of ent techniques for data fusion of and ric Intell Lab vol pp doi Kiela Grave Joulin and Mikolov Efﬁcient classiﬁcation in Proc AAAI Conf Artif pp Martínez and Yannakakis Deep multimodal fusion bining discrete events and continuous signals Proc Int Conf Multimodal pp doi and Henderson Multisensor data fusion in Handbook of Robotics Siciliano and Khatib Eds Springer pp Jain Nandakumar and Ross Score normalization in multimodal biometric systems Pattern vol pp doi Verma and Tiwary Multimodal fusion framework multiresolution approach for emotion classiﬁcation and recognition from physiological signals NeuroImage vol pp doi Abavisani and Patel Deep multimodal subspace ing networks IEEE Sel Topics Signal vol pp doi Huang Pareek Seyyedi Banerjee and Lungren Fusion of medical imaging and electronic health records using deep learning systematic review and implementation guidelines npj Digit vol doi Joshi et Review on Explainability in Multimodal Deep Neural Nets Karpathy Toderici Shetty Leung Sukthankar and video classiﬁcation with convolutional ral networks in Proc IEEE Conf Comput Vis Pattern Jun pp doi Dalla Mura Prasad Paciﬁci Gamba Chanussot and J Benediktsson Challenges and opportunities of multimodality and data fusion in remote sensing Proc IEEE vol pp doi Zadeh Chen Poria Cambria and Morency Tensor fusion network for multimodal sentiment analysis Online Available http Liu Shen Lakshminarasimhan Liang Zadeh and Morency Efﬁcient multimodal fusion with speciﬁc factors in Proc Annu Meeting Assoc Comput tics pp doi Fukui Park Yang Rohrbach Darrell and Rohrbach Multimodal compact bilinear pooling for visual question answering and visual grounding in Proc Conf Empirical Methods Natural Lang pp doi Liu Yuan and Wang Towards good practices for fusion in video classiﬁcation Online Available http Hou Tang Zhang Kong and Zhao Deep multimodal multilinear fusion with polynomial pooling Proc Adv ral Inf Process pp Bramon Boada Bardera Rodriguez Feixas Puig and Sbert Multimodal data fusion based on mutual information IEEE Trans Vis Comput Graphics vol pp doi Cadene Cord and Thome MUTAN tucker fusion for visual question answering in Proc Multimodal IEEE Int Conf Comput Vis ICCV pp doi Zadeh Pu Liang Mazumder Poria Cambria Memory fusion network for Online Available learning and Morency sequential http Sahu and Vechtomova Dynamic modal https data fusion for Online Available Cangea Velickovic and Lio XFlow deep neural networks for audiovisual classiﬁcation IEEE Trans Neural Netw Learn vol pp doi Wang Huang Sun Xu Rong and Huang Deep multimodal fusion by channel exchanging in Proc Adv Neural Inf Process vol pp Vielzeuf Pateux Baccouche and Jurie MFAS Multimodal fusion architecture search in Proc Conf Comput Vis Pattern Recognit CVPR Jun pp Bao Fountas Olugbade and timodal data fusion based on the global workspace theory in Proc Int Conf Multimodal pp doi Verma Wang Ge Shen Jin Wang Chen and Liu Deep higher order sequence fusion for modal sentiment analysis Online Available https Zhang Liu and Suen Towards robust pattern tion review Proc IEEE vol pp Jun Raghu and Schmidt survey of deep learning for tiﬁc discovery pp Online Available http Gunning Explainable artiﬁcial intelligence XAI in Defense Advanced Research Projects Agency vol Arlington VA USA Gilpin Testart Fruchter and Adebayo Explaining explanations to society Online Available http Srinivasan and Chander Explanation perspectives from the nitive survey in Proc Int Joint Conf Artif pp Goodman and Flaxman European Union regulations on mic and right to explanation AI vol pp VOLUME Joshi et Review on Explainability in Multimodal Deep Neural Nets Biran and Cotton Explanation and justiﬁcation in machine ing survey in Proc Workshop Explainable AI XAI vol pp Dubey and Xing Contextual explanation works Mach Learn vol pp and Jaakkola ral Networks Online Available http Wu Hughes Parbhoo Zazzi Roth and Beyond sparsity Tree regularization of deep models for ity in Proc AAAI Conf Artif pp Ross Hughes and Right for the right sons Training differentiable models by constraining their explanations in Proc Int Joint Conf Artif pp doi Kaya Gurpinar and Salah score fusion and decision trees for explainable automatic job candidate IEEE Conf Comput Vis ing from video CVs tern Recognit Workshops CVPRW Jul pp doi in Proc Simonyan Vedaldi and Zisserman convolutional networks Visualising image and saliency maps http Deep inside classiﬁcation models Online Available Tobias Springenberg Dosovitskiy and Riedmiller Striving for simplicity The all convolutional net Online Available http Brox Fong and Vedaldi Interpretable explanations of black boxes by meaningful perturbation in Proc IEEE Int Conf Comput Vis ICCV pp Ribeiro Singh and Guestrin Why should I trust you Explaining the predictions of any classiﬁer in Proc ACM SIGKDD Int Conf Knowl Discovery Data Mining pp Friedman Greedy function approximation gradient boosting machine Ann vol pp Goldstein Kapelner Bleich and Pitkin Peeking inside the black box Visualizing statistical learning with plots of individual tional expectation Comput Graph vol pp Lundberg and Lee uniﬁed approach to interpreting model predictions in Proc Adv Neural Inf Process pp Kim Khanna and Koyejo Examples are not enough learn to criticize Criticism for interpretability in Proc Adv Neural Inf Process Syst NIPS pp Wei Koh and Liang Understanding predictions via inﬂuence functions Online Available http Petsiuk Das and Saenko Rise Randomized input sampling for explanation of models Online Available https Selvaraju Lee Shen Jin Ghosh Heck Batra and Parikh Taking HINT Leveraging explanations to make vision and language models more grounded in Proc Int Conf Comput Vis ICCV pp through Shrikumar Greenside and Kundaje Learning important features Online Available http Axiomatic attribution Online Available Sundararajan Taly and Yan for deep networks http differences propagating activation Binder Bach Montavon Müller and Samek wise relevance propagation for deep neural network architectures in Proc Inf Sci Appl ICISA Springer pp Mahendran and Vedaldi Visualizing deep convolutional neural networks using natural Int Comput vol pp doi Zeiler and Fergus Visualizing and understanding convolutional networks in Proc Eur Conf Comput pp Hoste Dumas and Signer Mudra uniﬁed multimodal action framework in Proc Int Conf multimodal Interfaces ICMI pp doi Vaswani Shazeer Parmar Uszkoreit Jones Gomez Kaiser and Polosukhin Attention is all you need in Proc Adv Neural Inf Process pp Hind Wei Campbell Codella Dhurandhar Mojsilović Ramamurthy and Varshney TED Teaching AI to explain its decisions in Proc Conf AI Ethics pp Mohseni Zarei and Ragan multidisciplinary survey and framework for design and evaluation of explainable AI systems Online Available http Kanehira Takemoto Inayoshi and Harada modal explanations by predicting counterfactuality in videos in Proc Conf Comput Vis Pattern Recognit CVPR Jun pp Camburu Shillingford Minervini Lukasiewicz and Blunsom Make up your mind adversarial generation of inconsistent natural language explanations in Proc Annu Meeting Assoc Comput Linguistics pp doi Camburu Online Available https Mittelstadt Russell and Wachter Explaining explanations in AI in Proc Conf Fairness Accountability Transparency pp Explaining networks neural deep Zhang Gao Jia Zhu and Zhu RAVEN dataset for relational and analogical visual REasoNing in Proc Conf Comput Vis Pattern Recognit CVPR Jun pp doi Zhou Khosla Lapedriza Oliva and Torralba Learning deep features for discriminative localization in Proc IEEE Conf put Vis Pattern Recognit CVPR Jun pp Ray Yao Kumar Divakaran and Burachas Can you explain that Lucid explanations help tive image retrieval Online Available http Antol Agrawal Lu Mitchell Batra Zitnick and Parikh VQA Visual question answering in Proc IEEE Int Conf Comput Vis ICCV pp Goyal Mohapatra Parikh and Batra Towards parent AI systems Interpreting visual question answering els Online Available http Lee Kim and Ro Generation of multimodal tion using visual word constraint model for explainable diagnosis in Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support Springer pp Wu and Mooney Faithful multimodal explanation for visual question answering CoRR vol Strout Zhang and Mooney Do human rationales improve machine explanations Online Available http Vqa Parikh Horvitz Ribeiro and Nushi ing at VQA models Introspecting VQA models with in Proc Conf Comput Vis Pattern Jun pp Alipour Ray Lin Mani Hinthorn Yoo and Russakovsky Point and ask Incorporating pointing into visual question answering Online Available http and Burachas The impact of explanations on AI competency prediction in VQA in Proc IEEE Int Conf Humanized Comput Commun with Artif Intell HCCAI pp doi Schulze Yao Wu and Mooney reasoning for robust visual tion answering in Proc Adv Neural Inf Process vol pp Vidya Sagar Appaji Lakshmi and Srinivasa Rao Maximizing joint probability in visual question answering models Int Adv Sci vol pp Zhou video understanding Doctoral dissertation VOLUME Joshi et Review on Explainability in Multimodal Deep Neural Nets Wu Chen and Mooney Improving VQA and its explanations by comparing competing explanations Online Available https Wickramanayake Hsu and Lee Flex Faithful linguistic explanations for neural net based model decisions in Proc AAAI Dua Srinivas Kancheti and N Balasubramanian Beyond VQA Generating answer and rationale to visual tions Online Available http Wiegreffe and Pinter Attention is not not explanation Proc Conf Empircal Methods Nat Lang Process Int Joint Conf Nat Lang cess Conf pp doi Karimi Schölkopf and Valera Algorithmic recourse From counterfactual explanations to interventions Online Available http Wachter Mittelstadt and Russell Counterfactual tions without opening the black box Automated decisions and the GDPR Harvard J Law vol pp doi Kanehira and Harada Learning to explain with complemental Available Online examples http Goyal Wu Ernst Batra Parikh and Lee Counterfactual visual explanations in Proc Int Mach Learn ICML Jun pp Niu Tang Zhang Lu Hua and Wen Counterfactual VQA look at language bias Online Available http Fang Kong Fowlkes and Yang Modularized textual ing for counterfactual resilience in Proc Conf Comput Vis Pattern Recognit CVPR Jun pp Shi Shuang Geng Jiang Gao Fu Melo and Contrastive pretraining Online Available http Sokol and Flach One explanation does not ﬁt all The promise of interactive explanations for machine learning transparency Künstliche Intelligenz vol pp Jun doi Alipour Schulze Yao Ziskind and Burachas study on multimodal and interactive explanations for visual question answering Online Available http Rebanal Tang Combitsis Chang and Chen XAlgo Explaining the internal states of algorithms via question answering Available https Online Braines and Harborne Multimodal explanations for multisensor fusion in Proc RSM Artif Intell Military Multisensor Fusion Engines pp Krishna Zhu Groth Johnson Hata Kravitz Chen Kalantidis Li Shamma Bernstein and Visual genome Connecting language and vision using crowdsourced dense image annotations Int Comput vol pp May doi Marcos Lobry and Tuia Semantically interpretable activation maps explanations within CNNs in Proc Int Conf Comput Vis Workshop ICCVW pp doi Lu Krishna Bernstein and Visual relationship detection using language priors in Proc Eur Conf Comput Vis ECCV Gaur Faldu and Sheth Semantics of the Can knowledge graphs help make deep learning systems more pretable and explainable Online Available http Futia and Vetrò On the integration of knowledge graphs into deep learning models for more comprehensible challenges for future research Information vol doi Gulshad and Smeulders Explaining with counter visual attributes and examples in Proc Int Conf Multimedia Jun pp Lin Jain and Schwing Tags and attributes based VCR baselines in Proc NeurIPS Meng Huang Li and Wu Class activation map generation feature fusion Online Available http by representative class selection and Liu and Tuytelaars deep explanation model for learning IEEE Trans Image vol pp doi Patro Patel and Namboodiri Robust explanations for visual question answering in Proc IEEE Winter Conf Appl Comput Vis WACV Mar pp Yu Kim and Kim joint sequence fusion model for video question answering and retrieval in Proc Eur Conf Comput pp doi Yang Gao Sadiya and Chai Commonsense justiﬁcation for action explanation in Proc Conf Empirical Methods Natural Lang pp doi Baier Spieckermann and Tresp tion fusion using recurrent neural networks in Proc Eur Symp Artif Neural Comput Intell Mach Learn ESANN pp Manica Oskooei J Born Subramanian and Rodríguez Martínez Toward explainable anticancer pound sensitivity prediction via multimodal tional encoders Mol Pharmaceutics vol pp doi Chen Chen Xu Zhang Cao Qin and Zha Personalized fashion recommendation with visual explanations based on multimodal attention network Towards visually explainable mendation in Proc Int ACM SIGIR Conf Res Develop Inf Jul pp doi Ghosh Burachas Ray and Ziskind Generating ral language explanations for visual question answering using scene graphs and visual attention Online able http Fu Hospedales Xiang and Gong Learning multimodal latent attributes IEEE Trans Pattern Anal Mach vol pp doi Xu Wang Wang Xu Lin Dai and Wu Where is the model looking at and explain the network attention IEEE Sel Topics Signal vol pp Mar doi ul Hassan Mulhem Pellerin and Quenot Explaining visual classiﬁcation using attributes in Proc Int Conf Multimedia Indexing CBMI pp Yang Du and Hu Evaluating explanation without ground truth in interpretable machine learning Online Available https Ras van Gerven and Haselager Explanation methods in deep learning Users values concerns and challenges Online Available https Lage Chen J Narayanan Kim Gershman and An evaluation of the of explanation pp Online Available http Papineni Roukos Ward and Zhu BLEU method for automatic evaluation of machine translation in Proc Annu Meeting Assoc Comput Linguistics ACL pp doi Lavie and Agarwal METEOR An automatic metric for MT uation with high levels of correlation with human judgments in Proc Workshop Stat Mach Jun pp Online Available http Vedantam Zitnick and Parikh METEOR An matic metric for MT evaluation with high levels of correlation with human judgments in Proc IEEE Comput Soc Conf Comput Vis Pattern Mar pp doi Anderson Fernando Johnson and Gould SPICE Semantic propositional image caption evaluation in Proc ECCV VOLUME Joshi et Review on Explainability in Multimodal Deep Neural Nets Pruthi Dhingra Soares Collins Lipton Neubig and Cohen Evaluating Explanations How much do explanations from the teacher aid students Online able https Ribera and Lapedriza Can we do better explanations proposal of usercentered explainable AI in Proc IUI Workshops Li Tao Joty Cai and Luo Explaining elaborating and enhancing your answers for visual questions Online Available https Welinder Branson Mita Wah and Schroff birds Tech pp Online Available http Gat Schwartz Schwing and Hazan Removing bias in modal classiﬁers Regularization by maximizing functional entropies in Proc Adv Neural Inf Process Syst NeurIPS Pena Serna Morales and Fierrez Bias in multimodal AI Testbed for fair automatic recruitment in Proc Conf put Vis Pattern Recognit Workshops CVPRW Jun pp doi Cadene Dancette Cord and Parikh RUBi Reducing unimodal biases in visual question answering in Proc NeurIPS pp Sharma Kalra Chaudhary Patel and Morency Attend and attack Attention guided adversarial attacks on visual question answering models in Proc Conf Neural Inf Process Syst NeurIPS pp Xu Chen Liu Rohrbach Darrell and Song Fooling vision and language models despite localization and attention mechanism in Proc Conf Comput Vis Pattern Jun pp doi Rudin Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead Nature Mach vol pp doi Ramesh Pavlov Goh Gray Voss Radford Chen and Sutskever generation Online Available https Radford Kim Hallacy Ramesh Goh Agarwal Sastry Askell Mishkin Clark Krueger and Sutskever Learning Transferable Visual Models From Natural Language Supervision Online Available https JOSHI received the degree GARGI in computer science and engineering from Babasaheb Ambedkar Marathwada University BAMU Aurangabad in and the degree from the University of Pune in She is currently pursuing the degree in computer intelligence deep engineering artiﬁcial ing and multimodal XAI domain with Symbiosis International Deemed University Pune From to she worked as an Assistant Professor with the Patil College of Engineering Ambi and Pune She is also Junior Research Fellow with Symbiosis International Deemed University She is interested and keen on recent advances in AI machine learning and deep learning technology RAHEE WALAMBE received the and degrees from Lancaster University in From to she was Research Consultant with various organizations in the trol and robotics domain Since she been working as an Associate Professor with the Department of Electronics and tions Symbiosis Institute of Technology biosis International University Pune India Her research interests include applied deep learning and AI in the ﬁeld of robotics and healthcare KETAN KOTECHA received the and degrees from the Indian Institute of nology Mumbai is currently the Head of the Symbiosis Centre for Applied AI and the Dean of the Faulty of Engineering Symbiosis tional University Pune India is also an Expert of Artiﬁcial Intelligence and Machine Learning was recipient of several awards and research grants in AI application areas VOLUME,,[]
Article Gas Detection and Identiﬁcation Using Multimodal Artiﬁcial Intelligence Based Sensor Fusion Parag Narkhede and George Ghinea Rahee Walambe Shruti Mandaokar Pulkit Chandel Ketan Kotecha Symbiosis Institute of Technology Symbiosis International Deemed University Pune India Symbiosis Centre of Applied Artiﬁcial Intelligence Symbiosis International Deemed University Pune India head College of Engineering Design and Physical Sciences Brunel University London UK Correspondence These authors contributed equally to this work Abstract With the rapid industrialization and technological advancements innovative engineering technologies which are cost effective faster and easier to implement are essential One such area of concern is the rising number of accidents happening due to gas leaks at coal mines chemical industries home appliances etc In this paper we propose novel approach to detect and identify the gaseous emissions using the multimodal AI fusion techniques Most of the gases and their fumes are colorless odorless and tasteless thereby challenging our normal human senses Sensing based on single sensor may not be accurate and sensor fusion is essential for robust and reliable detection in several applications We manually collected gas samples samples per class for four classes using two speciﬁc sensors the gas sensors array and thermal camera The early fusion method of multimodal AI is applied The network architecture consists of feature extraction module for individual modality which is then fused using merged layer followed by dense layer which provides single output for identifying the gas We obtained the testing accuracy of for fused model as opposed to individual model accuracies of based on Gas Sensor data using LSTM and based on thermal images data using CNN model Results demonstrate that the fusion of multiple sensors and modalities outperforms the outcome of single sensor Keywords convolutional neural network early fusion gas detection term memory multimodal data Introduction Engineering innovation refers to the solving the social and industrial problems via use of the innovative engineering technologies and approaches With the rise of alization and bridging of gap between different strata of society use of chemicals been on rise Assistive technology is the technological domain consisting of systems having either software or hardware alone or both designed to enhance and maintain human capabilities in situations that require special attention Different solutions in assistive technology range from unmanned surveillance applications to healthcare applications like automated wheelchairs pose estimates etc In this work we propose an assistive technology solution for very relevant problem of gas detection and identiﬁcation for domestic industrial and outside environments Industrial hazards can cause chemical radioactive damage to the ing environment With the rapid developments in the industrialization and automated chemical plants gas leakage is common issue Explosions ﬁres spills leaks and waste emissions are some of the consequences of industrial accidents Residential cooking Citation Narkhede Walambe Mandaokar Chandel Kotecha Ghinea Gas Detection and Identiﬁcation Using Multimodal Artiﬁcial Intelligence Based Sensor Fusion Appl Syst Innov https Received November Accepted January Published January Publisher s Note MDPI stays tral with regard to jurisdictional ms in published maps and nal afﬁliations Copyright by the authors censee MDPI Basel Switzerland This article is an open access article distributed under the terms and ditions of the Creative Commons tribution CC BY license https Appl Syst Innov https https Appl Syst Innov of and carelessness in disposing of wastes generate unnecessary fumes are the signiﬁcant reasons of fume leakages An article presented in the media revealed that burning wood biomass and dung led to of the estimated premature deaths from outdoor air pollution which constitutes about of the total deaths due to outdoor pollution Harmful gases such as Liquid Petroleum Gas LPG Compressed Natural Gas CNG Methane Propane and other ﬂammable and toxic gases if not used carefully and quately may lead to accidents and in some cases disastrous consequences gas leak is an unintended crack hole or porosity in joint or machinery which excludes different ﬂuids and gases allowing the escape of closed medium In any plant or industrial setup gas leak test is quality control step that must be performed before device is set up As precautionary measure gas sensors are set up near the leakage prone equipment However the sensors are not able to detect gas in mixed gas environment Sensors are also prone to and limited to their operating characteristics Human intervention is not always possible in leakage situations primarily due to the hazardous nature of gases Smoke emissions during leakages give rise to unclear vision problems ﬁre and smoke leakages demand the immediate evacuation of persons with mobile disability Breathing these dangerous fumes may lead to dizziness unconsciousness and mass disaster if not treated properly In the case of gas leakage in chemical factories it can cause explosions Therefore detecting gas leakages and explosions within short period is of utmost importance Early detection of gas leakage with higher accuracy and reliability using the techniques is an essentially required assistive technology solution Detecting particular gas or different gases in the mixture of gases is also lenging and requires technological attention Existing methods of mixed gas detection methods include way of using Colorimetric Tape In this method dry material of tape reacts with the gas being emitted and leaves special stain for different gases under consideration The more the gas concentration the darker the stain on the tape Gas Chromatography is another methodology that separates mixtures of gases based on differences in boiling points polarity and vapor pressure This method high separation efﬁciency but requires large apparatus and workforce to operate Other than the chemical methods of gas detection and the advancements in interdisciplinary technologies various Artiﬁcial Intelligence AI based techniques are also reported in the literature Different machine learning algorithms such as Logistic Regression Random est and Support Vector Machines SVM are proposed in the literature for gas detection However these methods require multiple hyperparameters tuning and statistical tion for accurate and robust gas classiﬁcation It increases the processing time the power used and computations Adbul Majeed provided methodology that selected top weighted features from complex datasets for improving the time complexity as well as accuracy of the machine learning models Khalaf proposed an electronic nose system of classiﬁcation and concentration estimation that uses least square regression An array of eight different gas sensors is used to identify gases concentration in In this work Deep convolutional neural networks are employed for the application of gas classiﬁcation It was shown that the deep learning algorithms can learn features from the measurements from gas sensors in better way and can achieve higher classiﬁcation accuracy Bilgera et presented fusion of different AI models for Gas Source Localization to determine the point of leakage in ground using six various gas sensors Pan et presented deep learning approach consisting of hybrid framework comprised of the Convolutional Neural Network CNN and Long term memory LSTM to extract sequential information from transient response curves Fast Gas Recognition algorithm based on hybrid CNN and Recurrent Neural Network RNN is presented in It was shown that the fusion model outperforms Support Vector Machine SVM Random Forest neighbors Liu et described two network structures Deep Belief Networks and Stacked Autoencoders to extract abstract gas features from Then the Softmax classiﬁers are constructed using these features These reported approaches use sequential methods based on the gas sensor data directly Appl Syst Innov of However there are several issues with using only gas detection and identiﬁcation approach The primary reason is that the proportion of gas in air is very low in some cases and the gases are not identiﬁable with standard gas sensors This generates false negatives or false positives and hence hampers the detection accuracy of the system Additionally sensors are typically less sensitive and may not provide accurate measurements Another method observed for gas detection is the use of thermal imaging When gas is leaked the surrounding temperature increases compared with the normal conditions The increase in temperature can be characterized and analyzed by thermal imaging cameras This concept can be utilized to detect leakages The system for Methane and Ethane gas leak detection using thermal camera is proposed in Jadin and Ghazali presented method for detecting gas leak using infrared image analysis The system was designed by the technique of image processing which are data acquisition image preprocessing image processing feature extraction and classiﬁcation Single modality sensing methods may not achieve the system s required accuracy and robustness as such systems are limited to sensor characteristics Individual sensors are limited to temporal and spatial characteristics thermal imaging system can identify the presence of gas but fails to identify its type Hence concept of sensor data fusion came into existence Data fusion combines information from multiple sources to obtain the better output compared to any individual modality taken alone Kalman ﬁlter proposed in is one of the most widely used sensor fusion algorithms in robotics applications like position and orientation estimation guided vehicles etc However it requires the input data from two sensors in similar format In the situation under consideration the gas sensor data is scalar value whereas input from thermal image is vector Hence Kalman ﬁlter can not be used in this application of fusion and vectors With the advancement and ﬂexibility of AI frameworks combination of different AI algorithms can be used to extract important features in an efﬁcient and improved manner and improve classiﬁcation accuracy This paper presented an methodology that employs the Deep Learning DL frameworks for performing fusion of multimodality data from multiple sources to detect and classify the gasses The system is equipped with various gas detecting sensors and thermal imaging camera and sensor fusion is performed using the DL algorithms The focus of the proposed method is to extract features using two different deep learning paradigms and apply an early fusion method to fuse these features to train classiﬁer for detecting and subsequently identifying the gas The proposed method can be used to detect particular gas in mixed environment of gases It does not require manual operator to operate and is more robust solution as it incorporates the measurements from multiple gas sensors and thermal imaging cameras In case one modality is generating false negatives the fusion with other modality can help identify the correct outcome more effectively On the other hand if one modality is giving false positives the other modality helps to bring down the combined accuracy of fused output thereby providing accurate predictions The main contributions of the paper can be listed as follows an innovative multimodal framework for the fusion of two separate ties for robust and more reliable gas is proposed and presented the use of early fusion of the outputs of deep learning architectures CNN and LSTM is demonstrated for Gas Detection and identiﬁcation of the leaked gases In summary the main contributions of this work are twofold Firstly multimodal framework for the fusion for gas detection and identiﬁcation is presented in this paper This framework is faster easier to deply and generic Secondly the use of early fusion of the of outputs from CNN and LSTM is demonstrated for Gas Detection and identiﬁcation of the leaked gases The vanilla architectures are considered for the implementation of CNN and LSTM frameworks Having advanced frameworks like AlexNet ResNet will add to the computational complexity of the system due to very deep architectural frameworks the use of CNN facilitates faster processing and is also Appl Syst Innov of suitable for the deployment in systems The results show that false positives and negatives in the fused output are lower than the individual modalities The experimental setup is designed to collect the data using gas sensor array and thermal camera to preprocess the collected data and validate the developed framework Our approach is highly generic and can be extended to number of other applications involving multiple sensors and their data fusion Innovation lies in the development of AI techniques for solving highly relevant social and industrial issue of identifying gas leakage and controlling it in time to reduce loss of property and human lives in extreme cases The paper is organized as Section provides brief overview of timodal fusion methods The frameworks for data collection and preprocessing along with the proposed system architecture are presented in Section Section provides detailed discussion on obtained results and Section concludes the paper by mentioning future scope Theoretical Background Fusing the data from multiple sensors makes the system more robust and reliable than the single systems There are various methods of sensor fusion using AI paradigms proposed in the literature This section brieﬂy discusses these methods as precursor to our system framework and experimentation setup Methodologies for Multimodal Data Fusion modality refers to something that can be experienced in the environment It is type of information that can be felt and is stored Some examples could mation image information smell taste auditory video and touch Multimodal Sensor Fusion refers to combining sensor data from different sources to produce more consistent accurate and useful information than individual sensors to reduce false positives and false negatives The fusion architectures can be of three types early fusion late fusion and brid fusion Early fusion combines the raw data or the features extracted from the raw data This is suitable technique when there exists high correlation between modalities The feature extracting algorithms are applied to the individual modalities and then fused together using the process of concatenation to get the ﬁnal feature vector classiﬁer model is trained using this feature vector and ﬁnal predictions are made In this method the fusion is performed before the classiﬁcation which allows the interaction of features at low level In late fusion the decisions are taken based on the individual modalities separately Predictions from individual modalities are then combined the using statistical method like mean mode median etc As it is combination of decisions it is also known as Decision Fusion Technique This technique is preferred when there exists time relationship between the modalities Hybrid fusion combines the advantages of early and late fusion for better fusion of features as well as decisions Convolutional Neural Network Each Thermal image consists of features and are stored digitally in RGB format Simple Neural Networks are not able to generalize complex patterns in images Convolutional Neural Networks CNN learns to recognize differences and patterns in images CNN consists of Convolution Max Pooling Flattening and ANN layers The primary purpose of convolution is to ﬁnd features in an image using feature detector and put them into feature map Recurrent Neural Network Recurrent Neural Networks RNN consists of an essential memory element due to which the present output depends not only on current input but also on previous input However as the input sequence size increases the problem of vanishing gradient is observed during backpropagation This problem makes RNN unsuitable for tions requiring dependencies To overcome this advanced versions of RNNs Appl Syst Innov of known as Long Memory LSTM consisting of gates and memory elements were introduced These gates help regulate and extract information from the input and pass on gradients to the next node enabling the new sequence to be trained as equivalent as the earlier sequence and prioritize learning Also LSTMs are more effective than conventional RNN Sensor measurements are continuous stream of data and hence LSTM framework is applicable for extracting the features from the sensor measurements The thermal camera provides images and CNN is an appropriate choice for feature extraction The two considered modalities are having different characteristics and do not have any correlation Hence in our proposed framework we have employed early fusion of features extracted by the LSTM model from gas sensors and by the CNN model from the thermal images data The further section provides the details of the pipeline for data collection using the speciﬁed sensors preprocessing the collected data and developing the fusion frameworks for the proposed work Framework for System Design and Experimentation The system consists of gas sensors and thermal camera for identifying the gas concentrations and thermal images of the type of gases The block diagram indicating the data collection process is presented in Figure Figure provides the structure and steps followed for training the network and Figure indicates the testing phase The detailed description for the processes indicated in these ﬁgures is provided in further sections Gas Sensors Gas Sensors detect the presence of gas by converting the chemical information to electrical information Metal Oxide Semiconductor MQ gas sensors are appropriate as they are compact have fast response speed and long service life Each sensor consists of heating element that produces the analog output voltage proportional to the gas concentration The performance of Gas sensor depends on various sensor characteristics like sensitivity selectivity detection limit response time etc Different gas sensors namely and are used in the present work These sensors are sensitive to various gases like Methane Butane LPG Alcohol Smoke Natural Gas Carbon Monoxide Air Quality etc Table Figure Process of Data Collection Gas Values corresponding to gas concentrationimage Appl Syst Innov of Figure Network Training Process Figure Network Testing Process Class Label imageLSTM FrameworkCNN FrameworkAugmentation Rotation tilting FeaturesFeaturesConcatenationNeural NetworkClass Perfume smokeCompare with Actual ClassWeights UpdateGas Values corresponding to gas concentrationimageLSTM FrameworkCNN FrameworkFeaturesFeaturesConcatenationNeural NetworkClass Perfume smokeimage Appl Syst Innov of Table Gas sensors and sensetive gases Sensor Sensitive Gas Methane Butane LPG Smoke Alcohol Ethanol Smoke Natural Gas LPG LPG Butane Gas Carbon Monoxide Hydrogen Gas Air Quality Benzene Smoke Thermal Camera Thermal camera is device that measures the temperature variations using the infrared light Every pixel on camera image sensor is an infrared temperature sensor and gets temperature of all points at the same time The images are generated according to temperature format and displays images in the form of RGB Unlike normal imaging cameras thermal camera is not constrained by dark surroundings and can work with any environment regardless of its shape and texture Seek Thermal Camera used in this work is compact thermal camera consisting of Thermal Sensor ﬁeld of view measurement of temperature range to framerate Hz and Thermal Pixels to be able to see thermal image easily The gas sensors and thermal camera are used simultaneously to collect data for training and testing of the developed fusion model The next part of the paper describes the data collection and its preprocessing in detail Data Collection and Preprocessing To the best of the authors knowledge data consisting of thermal images and gas sensors for the representation of gas yet been collected and available in the open domain for direct use Hence in this work data of the sensors and thermal imaging camera is collected manually for model training and validation purposes The experimental data is collected through an array of gas sensors as well as using the Seek Thermal Camera The gas sensors were placed at mm apart In the experimentation two speciﬁc gas sources are identiﬁed namely the gases originating from perfumes and gases emitted by incense sticks The experimentation setup and workﬂow for the data collection is shown in Figure Figure Experimental Setup for data collection Sensor readings and thermal images are recorded for each of these two gas sources were collected at time interval of s continuously for one and half hours In this time gas was sprayed with an interval of s for the ﬁrst min with s intervals for the next min and s intervals for the next min few representative samples for three classes gas perfume and smoke with the thermal image and corresponding gas array data are shown in Table The sensors provide the analog voltage equivalent to the gas Appl Syst Innov of concentration The analog value is converted to the digital value using an analog to digital converter These bits of digital values are shown for representation purposes in Table Each sensor is sensitive to more than one gas and hence sensors are calibrated appropriately data set in total consists of samples where samples belong to perfume samples belong smoke samples belong to mixture of perfume and smoke and samples belong to neutral environment gas Table Data samples for thermal image and their corresponding gas array obtained for classes Gas Perfume Alcohol Smoke Thermal Image Gas Sensor Measurements Thermal Image Gas Sensor Measurements Data Preprocessing Deep learning models require large amount of training data for appropriate and efﬁcient operation Due to the availability of limited data data augmentation techniques are used which helped to increase the dataset size The diversity of limited thermal images is increased using data augmentation techniques such as rescaling and resizing The Figure shows the ground truth image Figure and all images generated using rotation and tilting operations Figure b Figure Ground Truth image and augmented images b Feature Extraction from Thermal Images Using CNN total of thermal images and corresponding labels Gas Alcohol Smoke mixture of Alcohol and Smoke are considered in this experimentation Appl Syst Innov of split of was done such that out of total images are used for training and samples are used for Validation whereas images are used for testing purposes In the process of development of the CNN model multiple experimentations were carried out with different architectures and various hyperparameter tuning approaches were applied It was found that three layer architecture followed by dropout layer dropout of is providing the best accuracy and recall Model is optimized with different optimizers and the best performing optimizer is selected for further processes An ADAM optimizer with learning rate with decay of and regularization in the ﬁrst two Pool pairs are applied to avoid overﬁtting of the model The model is trained for epochs which resulted in the testing accuracy of Feature Extraction from Gas Sensor Measurements Using LSTM Sensor measurements are sequential and hence sequence model namely LSTM work is used for extracting the features from these measurements The architecture of the LSTM model consists of the input layer followed by single LSTM layer with cells LSTM layer is regularized with regularization The LSTM layers are followed by the classiﬁer layer with the Softmax activation function This LSTM network was trained on different optimizers with ﬁxed learning rate of to ﬁnd the best optimizer Through the trial and error it was observed that Adam optimizer was ﬁtting to the model the best and also converging quickly Hence Adam optimizer is selected for analysis and experimentation work It can be observed that Adam optimizer ﬁts and converges quickly The model is trained for epochs and we obtained the testing accuracy of Multimodal Fusion of Image and Sequence Data In this phase of the work the features extracted from the thermal images and gas sensor measurements fused for accurate decision making The proposed architectures of the image and sequence data fusion model are presented in Figure early fusion and Figure late fusion The focus of the work was to build fused classiﬁer that consists of both gas sensor sequence array and thermal images In the fusion process LSTM and CNN s output must be in the same feature space before fusion can be performed The fusion model is optimized with an Adam optimizer with learning rate and decay Regularization is applied for avoiding overﬁtting of the fusion model The model is trained for epochs which resulted in the testing accuracy of Figure Framework for the proposed Early fusion model Gas Sensor Array Sequence DataThermal Image PreprocessingPreprocessingLSTMCNNFeature VectorFeature VectorConcatenationClassifierFusedOutput Appl Syst Innov of Figure Framework for the proposed Late fusion model Late fusion model is also implemented for the fusion of gas sensors array data with the thermal image Late fusion being the decision level fusion the predictions of from vidual models namely LSTM model and CNN model are obtained individually Then the Late fusion process is applied in two ways In ﬁrst trial maximum of the predictions from individual results is taken as ﬁnal fusion value Hereafter this is referred as Max fusion In another trial arithmetic average of the individual model predictions is considered as ﬁnal fusion referred as Average fusion The presented models of early and late fusion are implemented and validated with the dataset available The next section describes the results obtained and comparison between the fusion models Results and Discussion The multimodal fusion model for gas detection and is presented in this work Two modalities namely thermal images and gas sensor measurements are considered in this work of gas detection The CNN architecture is applied for extracting features from the thermal images whereas LSTM framework is used for extracting features from the sequences of gas sensor measurements The implementation of the proposed model in done using the Python using Keras framwork on TensorFlow platform Open source Google Colab GPU is used for training and testing of the proposed model It is based on Intel Xeon Processor with GB RAM The CNN model starts converging at around the epoch whereas LSTM reaches convergence at around the epoch It was observed that the fused model stabilizes at around the epoch itself The accuracy of the gas sensor array is comparatively lower since the outcome of one sensor out of sensors considered is typically not very accurate due to the mixing of gases in the air The thermal model individually performs comparatively better however in the air the thermal signature of gaseous emissions may be generated due to multiple gases or multiple sources of exhausts and having gas sensor to validate the type of gas is extremely helpful in identiﬁcation It was noticed and observed that the individual models are underperforming compared to the fusion models In the fusion models the individual modalities either collaborate or oppose the outcomes of the individual modality thereby making the system more reliable and accurate By performing regularization techniques on individual models namely CNN and LSTM testing accuracy of with Thermal Images and for Gas Sequences is achieved However the early fusion of features from both CNN and LSTM provided the testing accuracy of which is greater than individual models accuracies In the case of late fusion max fusion and average fusion the accuracy was observed to be around Table shows the individual training and testing accuracy loss precision recall and scores for all four classes considered in this study The accuracy comparison for the individual models is shown in Figure It can be observed that the fusion models outperform the individual models as the predictions in the individual models are based on Gas Sensor Array Sequence DataThermal Image PreprocessingPreprocessingLSTMCNNFeature VectorFeature AverageFusedOutputClassifierClassifier Appl Syst Innov of both the modality data It can also be noticed that the accuracy of early fusion is slightly higher than the late fusion models as in this case the fusion happens at the feature level which allows the interaction amongst the modalities The confusion matrices are also plotted for all the frameworks and are shown in Figure Table Quantitative comparison of the individual models with fused models Training Accuracy Testing Accuracy Class Precision Recall LSTM Model only CNN Model only Early Fusion Model Gas Perfume Smoke Mixture Gas Perfume Smoke Mixture Gas Perfume Smoke Mixture The fusion model is trained for epochs and accuracy and loss curves are analyzed and provides better performance than individual models Figure Accuracy comparison of different models It is evident from the confusion matrices that the false positives and false negatives obtained from the fusion models are considerably lower than the individual models Hence it can be concluded that the fusion models are outperforming the individual models Also the higher testing accuracy of the fusion models demonstrates that the resultant fusion system is more robust and reliable than individual models and performs the task of gas identiﬁcation and classiﬁcation with superiority Analytically false positives and false negatives appear due to various aspects of the model and data Primary reason could be the mixing of gases to an extent which makes it difﬁcult for the model to clearly classify The majority of the false predictions are arriving because of moderate probability of predicting class model can be trained rigorously using the more and varied data samples to solve the false prediction due to boundary line probabilities CNNLSTMEarly FusionMax FusionAverage FusionACCURACY COMPARISON Appl Syst Innov of Figure Confusion matrices CNN b LSTM c Early fusion d Max Fusion and Average Fusion predictions over test set of samples Conclusions In this work multimodal fusion framework for reliable identiﬁcation and detection of gases is developed We considered four classes individual gases alcohol vapor obtained from perfume and smoke from incense sticks as mixture of these gases and gas for data collection using sensors namely thermal camera for capturing the thermal signature of the gases and array of gas sensors numbers for detection of speciﬁc gases The data collected is unique and samples of both Thermal Images and Gas Sensor Sequence of vector size Sensors Both these modalities were fused using Early and Late Fusion Techniques In summary the contribution of this work is in bringing in innovative engineering tools for solving real world problem by developing more reliable gas detection method involving two modalities and fusing them The multimodal model outperforms the individual models by supporting or opposing the individual modalities In case if one modality fails the other modality can work alone until repair takes place This is essential in applications such as leak detection in chemical plants identiﬁcation of explosives etc The proposed architecture is based on the deep learning frameworks and hence require large number of data samples for appropriate training of the network The complex data samples involving different combinations of multiple gases in data sample will lead to the robust training of the network Also to have efﬁcient and effective operation dedicated hardware processing module is essential The future course of action will focus on the collection of datasets comprising of multiple gases and their combinations in different environmental conditions Author Contributions Conceptualization and methodology and software and validation and data curation and draft preparation and and editing and supervision and funding acquisition and All authors have read and agreed to the published version of the manuscript Funding This research was funded by Symbiosis International Deemed University under the grant Minor Research Project Grant void letter number Institutional Review Board Statement Not applicable True Labels CNNTrue Labels b LSTMTrue Labels c Early FusionTrue Labels d Max FusionTrue Labels Average Fusion Appl Syst Innov of Informed Consent Statement Not applicable Data Availability Statement The data presented in this study are available on request from the corresponding authors Conﬂicts of Interest The authors declare conﬂict of interest References Trivedi Purohit Soju Tiwari Major industrial disasters in India An ofcial newsletter of Volume Available online http accessed on January Zhou Zhao Zhao Chen Research on ﬁre and explosion accidents of oil depots In Proceedings of the International Conference on Applied Engineering Wuhan China April Volume pp Mudur Lakhs of early deaths tied to home emissions Telegraph India Online Published online on September Available online https accessed on July MDC Systems Inc Detection Methods Online Resource Available online https accessed on July Fox Kozar Steinberg Chromatography and Gas Spectrometry In Encyclopedia of Separation Science Wilson Ed Academic Press Cambridge MA USA pp CrossRef Wang Wang Hong Gas leak location detection based on data fusion with time difference of arrival and energy decay using an ultrasonic sensor array Sensors CrossRef PubMed Stauffer Dolan Newman Chapter Chromatography and Gas Spectrometry In Fire Debris Analysis Academic Press Cambridge MA USA pp CrossRef Yin Zhang Tian Zhang Temperature modulated gas sensing system for and fast detection IEEE Sens J CrossRef Bermak Shi Chan Fast and robust gas identiﬁcation system using an integrated gas sensor technology and Gaussian mixture models IEEE Sens J CrossRef Majeed Improving time complexity and accuracy of the machine learning algorithms through selection of highly weighted top k features from complex datasets Ann Data Sci CrossRef Khalaf Electronic Nose System for Safety Monitoring at Reﬁneries Eng Sustain Dev Peng Zhao Pan Ye Gas classiﬁcation using deep convolutional neural networks Sensors CrossRef Bilgera Yamamoto Sawano Matsukura Ishida Application of convolutional long memory neural networks to signals collected from sensor network for autonomous gas source localization in outdoor environments Sensors CrossRef Pan Zhang Ye Bermak Zhao X fast and robust gas recognition algorithm based on hybrid convolutional and recurrent neural network IEEE Access CrossRef Liu Hu Ye Cheng Li Gas recognition under sensor drift by using deep learning Int Intell Syst CrossRef Hamilton Charalambous B Leak Detection Technology and Implementation IWA Publishing London UK Avila Leak Detection with Thermal Imaging Patent March Marathe Leveraging Drone Based Imaging Technology for Pipeline and RoU Monitoring Survey In SPE Symposium Asia Paciﬁc Health Safety Security Environment and Social Responsibility Society of Petroleum Engineers Kuala Lumpur Malaysia Jadin Ghazali Gas leakage detection using thermal imaging technique In Proceedings of the International Conference on Computer Modelling and Simulation Cambridge UK March pp Elmenreich review on system architectures for sensor fusion applications In IFIP International Workshop on Software Technolgies for Embedded and Ubiquitous Systems Springer Germany pp Kalman new approach to linear ﬁltering and prediction problems J Basic Eng Mar CrossRef Luo Ye Zhao Pan Cao Classiﬁcation of data from electronic nose using gradient tree boosting algorithm Sensors CrossRef PubMed Simonyan Zisserman Very deep convolutional networks for image recognition arXiv Khalaf Pace Gaudioso Gas detection via machine learning Int Comput Electr Autom Control Inf Eng Krizhevsky Sutskever Hinton Imagenet classiﬁcation with deep convolutional neural networks Commun ACM CrossRef Zhang Ren Sun J Deep residual learning for image recognition In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Vegas NV USA June pp Liu Li Xu Natarajan Learn to combine modalities in multimodal deep learning arXiv Appl Syst Innov of Dong Gao Tao Liu Wang Performance evaluation of early and late fusion methods for generic semantics indexing Pattern Anal Appl CrossRef Lahat Adali Jutten Multimodal data fusion An overview of methods challenges and prospects Proc IEEE CrossRef Shea Nash An introduction to convolutional neural networks arXiv Rumelhart Hinton Williams Learning representations by errors Nature CrossRef Hochreiter Schmidhuber J Long memory Neural Comput CrossRef Bao Yue Rao deep learning framework for ﬁnancial time series using stacked autoencoders and term memory PLoS ONE CrossRef PubMed Yu Xiao Zhao X new method of mixed gas identiﬁcation based on convolutional neural network for time series classiﬁcation Sensors CrossRef PubMed Pashami Lilienthal Trincavelli Detecting changes of distant gas source with an array of MOX gas sensors Sensors CrossRef PubMed Awang Z Gas sensors review Sens Transducers Havens Sharp Thermal Imaging Techniques to Survey and Monitor Animals in the Wild Methodology Academic Press Cambridge MA USA,,[]
Bias in Multimodal AI Testbed for Fair Automatic Recruitment Alejandro Ignacio Serna Aythami Morales Julian Fierrez School of Engineering Universidad Autonoma Madrid Spain Abstract The presence of algorithms in society is rapidly increasing nowadays while concerns about their transparency and the possibility of these algorithms ing new sources of discrimination are arising In fact many relevant automated systems have been shown to make cisions based on sensitive information or discriminate tain social groups certain biometric systems for recognition With the aim of studying how current multimodal algorithms based on heterogeneous sources of information are affected by sensitive elements and inner biases in the data we propose ﬁctitious automated cruitment testbed FairCVtest We train automatic ment algorithms using set of multimodal synthetic proﬁles consciously scored with gender and racial biases CVtest shows the capacity of the Artiﬁcial Intelligence AI behind such recruitment tool to extract sensitive tion from unstructured data and exploit it in combination to data biases in undesirable unfair ways Finally we present list of recent works developing techniques ble of removing sensitive information from the making process of deep learning architectures We have used one of these algorithms SensitiveNets to experiment learning for the elimination of sitive information in our multimodal AI framework Our methodology and results show how to generate fairer based tools in general and in particular fairer automated recruitment systems Introduction Over the last decades we have witnessed great advances in ﬁelds such as data mining Internet of Things or Artiﬁcial Intelligence among others with data taking on special evance Paying particular attention to the ﬁeld of machine learning the large amounts of data currently available have led to paradigm shift with handcrafted algorithms being replaced in recent years by deep learning technologies Machine learning algorithms rely on data collected from society and therefore may reﬂect current and historical ases if appropriate measures are not taken In this nario machine learning models have the capacity to cate or even amplify human biases present in the data There are relevant models based on machine learning that have been shown to make decisions largely inﬂuenced by gender or ethnicity Google s or book s ad delivery systems generated undesirable crimination with disparate performance across population groups New York s insurance regulator probed Health Group over its use of an algorithm that researchers found to be racially biased the algorithm prioritized ier white patients over sicker black ones More cently Apple Credit service granted higher credit limits to men than even though it was programmed to be blind to that variable the biased results in this case were originated from other variables The usage of AI is also growing in human resources partments with and screening software becoming increasingly common in the hiring pipeline But automatic tools in this area have exhibited worrying ased behaviors in the past For example Amazon s ing tool was preferring male candidates over female dates The access to better job opportunities is crucial to overcome differences of minority groups However in cases such as automatic recruitment both the models and their training data are usually private for corporate or legal reasons This lack of transparency along with the long tory of bias in the hiring domain hinder the technical ation of these systems in search of possible biases targeting protected groups This deployment of automatic systems led ments to adopt regulations in this matter placing special emphasis on personal data processing and preventing gorithmic discrimination Among these regulations the new European Union s General Data Protection Regulation GDPR adopted in May is specially relevant for its impact on the use of machine learning algorithms The GDPR aims to protect EU citizens rights concerning data Figure Information blocks in resume and personal attributes that can be derived from each one The number of crosses represent the level of sensitive information high medium low protection and privacy by regulating how to collect store and process personal data Articles and This normative also regulates the right to explanation ticles by which citizens can ask for explanations about algorithmic decisions made about them and requires measures to prevent discriminatory effects while ing sensitive data according to Article sensitive data cludes personal data revealing racial or ethnic origin litical opinions religious or philosophical beliefs On the other hand one of the most active areas in chine learning is around the development of new modal models capable of understanding and processing formation from multiple heterogeneous sources of tion Among such sources of information we can include structured data in tables and unstructured data from images audio and text The implementation of these els in society must be accompanied by effective measures to prevent algorithms from becoming source of tion In this scenario where multiple sources of both tured and unstructured data play key role in algorithms decisions the task of detecting and preventing biases comes even more relevant and difﬁcult In this environment of desirable fair and trustworthy AI the main contributions of this work are We present new public experimental framework around automated recruitment aimed to study how multimodal machine learning is inﬂuenced by biases present in the training datasets We have evaluated the capacity of popular neural work to learn biased target functions from multimodal sources of information including images and tured data from resumes We develop learning method based on the elimination of sensitive information such as gender or ethnicity from the learning process of multimodal approaches and apply it to our automatic recruitment testbed for improving fairness Our results demonstrate the high capacity of commonly used learning methods to expose sensitive information gender and ethnicity and the necessity to implement priate techniques to guarantee making processes The rest of the paper is structured as follows Section analyzes the information available in typical resume and the sensitive data associated to it Section presents the general framework for our work including problem lation and the dataset created in this work FairCVdb tion reports the experiments in our testbed FairCVtest ter describing the experimental methodology and the ent scenarios evaluated Finally Section summarizes the main conclusions What else does your resume data reveal Studying multimodal biases in AI For the purpose of studying discrimination in Artiﬁcial Intelligence at large in this work we propose new mental framework inspired in ﬁctitious automated ing system FairCVtest There are many companies that have adopted predictive Figure Block diagram of the automatic learning process and to stages where bias can appear tools in their recruitment processes to help hiring managers ﬁnd successful employees Employers often adopt these tools in an attempt to reduce the time and cost of hiring or to maximize the quality of the hiring process among other reasons We chose this application because it comprises personal information from different nature The resume is traditionally composed by structured data including name position age gender experience or cation among others see Figure and also includes structured data such as face photo or short biography face image is rich in unstructured information such as identity gender ethnicity or age That information can be recognized in the image but it requires cognitive or automatic process trained previously for that task The text is also rich in unstructured information The language and the way we use that language determine attributes lated to your nationality age or gender Both image and text represent two of the domains that have attracted major interest from the AI research community during last years The Computer Vision and the Natural Language ing communities have boosted the algorithmic capabilities in image and text analysis through the usage of massive amounts of data large computational capabilities GPUs and deep learning techniques The resumes used in the proposed FairCVtest framework include merits of the candidate experience education level languages etc two demographic attributes gender and ethnicity and face photograph see Section for all the details Problem formulation and dataset The model represented by its parameters vector w is trained according to multimodal input data deﬁned by n features x xn Rn Target function T and learning strategy that minimizes the error between the put and the Target function T In our framework where x is data obtained from the resume T is score within the interval ranking the candidates according to their its score close to corresponds to the worst candidate while the best candidate would get Biases can be duced in different stages of the learning process see Figure in the Data used to train the models the ing or feature selection B the Target function C and the Learning strategy As result biased Model F will produce biased Results D In this work we focus on the Target function C and the Learning strategy The get function is critical as it could introduce cognitive biases from biased processes The Learning strategy is ally based on the minimization of loss function deﬁned to obtain the best performance The most popular approach for supervised learning is to train the model w by minimizing loss function L over set of training samples S L T j min w X xj FairCVdb research dataset for multimodal AI We have generated synthetic resume proﬁles cluding features obtained from information blocks demographic attributes gender and ethnicity and face photograph The blocks are education attainment generated from US Census Bureau Education tainment without gender or ethnicity distinction availability previous experience the existence of recommendation letter and language proﬁciency in set of different and common languages chosen from US Bureau Language Spoken at Home Each guage is encoded with an individual feature features in tal that represents the level of knowledge in that language Each proﬁle been associated according to the der and ethnicity attributes with an identity of the DiveFace database DiveFace contains face images pixels and annotations equitably distributed among mographic classes related to gender and ethnic groups Black Asian and Caucasian including different identities see Figure FairCVtest Description and experiments FairCVtest Scenarios and protocols In order to evaluate how and to what extent an algorithm is inﬂuenced by biases that are present in the FairCVdb get function we use the FairCVdb dataset previously troduced in Section to train various recruitment systems under different scenarios The proposed FairCVtest testbed consist of FairCVdb the trained recruitment systems and the related experimental protocols First we present different versions of the recruitment tool with slight differences in the input data and target tion aimed at studying different scenarios concerning der bias After that we will show how those scenarios can be easily extrapolated to ethnicity bias The Scenarios included in FairCVtest were all trained using the competencies presented on Section with the lowing particular conﬁgurations Scenario Training with Unbiased scores T U and the gender attribute as additional input Scenario Training with scores T G and the gender attribute as additional input Scenario Training with scores T G but the gender attribute wasn t given as input Scenario Training with scores T G and feature embedding from the face photograph as additional input In all cases we designed the candidate score tor as feedforward neural network with two hidden layers both of them composed by neurons with ReLU tion and only one neuron with sigmoid activation in the output layer treating this task as regression problem In Scenario where the system takes also as input an embedding from the applicant s face image we use the trained model as feature extractor to tain these embeddings is popular tional Neural Network originally proposed to perform face and image recognition composed with layers ing residual or shortcuts connections to improve racy as the net depth increases s last lutional layer outputs embeddings with features and we added fully connected layer to perform bottleneck that compresses these embeddings to just features taining competitive face recognition performances Note that this face model was trained exclusively for the task of face recognition Gender and ethnicity information were not intentionally employed during the training process Of course this information is part of the face attributes Figure summarizes the general learning architecture of FairCVtest The experiments performed in next section will try to evaluate the capacity of the recruitment AI to detect protected attributes gender ethnicity without being explicitly trained for this task Figure Examples of the six demographic groups included in DiveFace for ethnic groups Therefore each proﬁle in FairCVdb includes tion on gender and ethnicity face image correlated with the gender and ethnicity attributes and the resume tures described above to which we will refer to candidate competencies xi The score T j for proﬁle j is generated by linear n as bination of the candidate competencies xj xj xj T j βj n X αixj i where n is the number of features competencies αi are the weighting factors for each competency xj i ﬁxed manually based on consultation with human recruitment expert and βj is small Gaussian noise to introduce small degree of variability two proﬁles with the same competencies do not necessarily have to obtain the same sult in all cases Those scores T j will serve as groundtruth in our experiments Note that by not taking into account gender or nicity information during the score generation in tion these scores become agnostic to this information and should be equally distributed among different graphic groups Thus we will refer to this target function as Unbiased scores T U from which we deﬁne two target functions that include two types of bias Gender bias T G and Ethnicity bias T Biased scores are generated by plying penalty factor Tδ to certain individuals belonging to particular demographic group This leads to set of scores where with the same competencies certain groups have lower scores than others simulating the case where the process is inﬂuenced by certain cognitive biases introduced by humans protocols or automatic systems Figure Multimodal learning architecture composed by Convolutional Neural Network and fully connected network used to fuse the features from different domains image and structured data Note that some features are included or removed from the learning architecture depending of the scenario under evaluation variability see Equation this loss will never converge to Scenario shows the worst performance what makes sense since there s correlation between the bias in the scores and the inputs of the network Finally Scenario shows validation loss between the other Scenarios As we will see later the network is able to ﬁnd gender features in the face embeddings even if the network and the beddings were not trained for gender recognition As we can see in Figure the validation loss obtained with biased scores and sensitive features Scenario is lower than the validation losses obtained for biased scores and blind tures Scenarios and In Figure we can see the distributions of the scores dicted in each scenario by gender where the presence of the bias is clearly visible in some plots For each scenario we compute the divergence KL P from the female score distribution Q to the male P as measure of the bias impact on the classiﬁer output In Scenarios and Figure and respectively there is gender difference in the scores fact that we can corroborate with the KL divergence tending to zero see top label in each plot In the ﬁrst case Scenario we obtain those results because we used the unbiased scores T U during the ing so that the gender information in the input becomes irrelevant for the model but in the second one Scenario because we made sure that there was gender tion in the training data and both classes were balanced Despite using target function biased the absence of this information makes the network blind to this bias paying this effect with drop of performance with respect to the scores T G but obtaining fairer model The Scenario Figure leads us to the model with the most notorious difference between classes Figure Validation loss during the training process tained for the different scenarios FairCVtest Predicting the candidate score The recruitment tool was trained with the of the synthetic proﬁles CVs described in Section and retaining as validation set CVs each set equally distributed among gender and ethnicity using Adam optimizer epochs batch size of and mean absolute error as loss metric In Figure we can observe the validation loss during the training process for each Scenario see Section which gives us an idea about the performance of each network in the main task scoring applicants resumes In the ﬁrst two scenarios the network is able to model the target tion more precisely because in both cases it all the tures that inﬂuenced in the score generation Note that by adding small Gaussian noise to include some degree of Figure Hiring score distributions by gender for each Scenario The results show how multimodal learning is capable to reproduce the biases present in the training data even if the gender attribute is not explicitly available gender information could be present in the feature dings generated by networks oriented to other tasks sentiment analysis action recognition Therefore spite not having explicit access to the gender attribute the classiﬁer is able to reproduce the gender bias even though the attribute gender was not explicitly available during the training the gender was inferred from the latent features present in the face image In this case the KL divergence is around lower value than the of Scenario but anyway ten times higher than Unbiased Scenarios Moreover gender information is not the only sensitive information that algorithms like face recognition models can extract from unstructured data In Figure we present the distributions of the scores by ethnicity predicted by network trained with scores T in an ogous way to Scenario in the gender experiment The network is also capable to extract the ethnicity tion from the same facial feature embeddings leading to an network when trained with skewed data In this case we compute the KL divergence by making combinations vs vs and vs and reporting the average of the three divergences Figure Hiring score distributions by ethnicity group trained according to the setup of the Scenario note the KL divergence rising to which makes sense because we re explicitly providing it with gender formation In Scenario the network is able to detect the gender information from the face embeddings as tioned before and ﬁnd the correlation between them and the bias injected to the target function Note that these beddings were generated by network originally trained to perform face recognition not gender recognition Similarly authors develop method to generate synthetic datasets that approximate given original one but more fair with respect to certain protected attributes Other works focus on the learning process as the key point to prevent biased els The authors of propose an adaptation of DANN originally proposed to perform domain adaptation to generate agnostic feature representations unbiased related to some protected concept In the authors propose method to mitigate bias in occupation classiﬁcation without having access to protected attributes by reducing the relation between the classiﬁer s output for each individual and the word embeddings of their names joint learning and unlearning method is proposed in to simultaneously learn the main classiﬁcation task while unlearning biases by applying confusion loss based on computing the cross tropy between the output of the best bias classiﬁer and an uniform distribution The authors of propose new regularization loss based on mutual information between feature embeddings and bias training the networks using adversarial and gradient reversal techniques nally in an extension of triplet loss is applied to remove sensitive information in feature embeddings out losing performance in the main task In this work we have used the method proposed in to generate agnostic representations with regard to gender and ethnicity information This method was proposed to prove privacy in face biometrics by incorporating an sarial regularizer capable of removing the sensitive mation from the learned representations see for more details The learning strategy is deﬁned in this case as L T j min w X xj where is generated with sensitiveness detector and measures the amount of sensitive information in the learned model represented by We have trained the face sentation used in the Scenario according to this method named as Agnostic scenario in next experiments In Figure we present the distributions of the hiring scores predicted using the new agnostic embeddings for the face photographs instead of the previous dings Scenario compare with Figure As we can see after the sensitive information removal the network can t extract gender information from the embeddings As sult the two distributions are balanced despite using the In Figure labels and facial information we can see the results of the same experiment using the labels compare with Figure Just like the gender case the three distributions are also balanced ter removing the sensitive information from the face feature embeddings obtaining an ethnicity agnostic representation In both cases the KL divergence shows values similar to those obtained for unbiased Scenarios Figure Hiring score distributions by gender Up and nicity Down after removing sensitive information from the face feature embeddings FairCVtest Training fair models As we have seen using data with biased labels is not big concern if we can assure that there s information correlated with such bias in the algorithm s input but we can t always assure that Unstructured data are rich source of sensitive information for complex deep learning models which can exploit the correlations in the dataset and end up generating undesired discrimination Removing all sensitive information from the input in general AI setup is almost infeasible strates how removing explicit gender indicators from sonal biographies is not enough to remove the gender bias from an occupation classiﬁer as other words may serve as proxy On the other hand collecting large datasets that represent broad social diversity in balanced manner can be extremely costly Therefore researchers in AI and machine learning have devised various ways to prevent algorithmic discrimination when working with unbalanced datasets cluding sensitive data Some works in this line of fair AI propose methods that act on the decision rules rithm s output to combat discrimination the Table Distribution of the top candidates for each scenario in FairCVtest by gender and ethnicity group maximum difference across groups Dem Demographic attributes gender and ethnicity Scenario Bias Agnostic yes yes yes yes Input Features Gender Merits Dem Face Male yes yes yes yes yes yes yes yes yes Female Ethnicity Group Group Group Previous results suggest the potential of sensitive formation removal techniques to guarantee fair tations In order to evaluate further these agnostic sentations we conducted another experiment simulating the outcomes of recruitment tool We assume that the ﬁnal decision in recruitment process will be managed by mans and the recruitment tool will be used to realize ﬁrst screening among large list of candidates including the resumes used as validation set in our previous periments For each scenario we simulate the candidates screening by choosing the top scores among them scores with highest values We present the distribution of these selections by gender and ethnicity in Table as well as the maximum difference across groups As we can observe in Scenarios and where the classiﬁer shows demographic bias we have almost difference in the percentage of candidates selected from each demographic group On the other hand in Scenarios and the impact of the bias is notorious being larger in the ﬁrst one with difference of in the gender case and in the nicity case The results show differences of for the gender attribute in the Scenario and for the ity attribute However when the sensitive features removal technique is applied the demographic difference drops from to in the gender case and from to in the ethnicity one effectively correcting the bias in the dataset These results demonstrate the potential hazards of these recruitment tools in terms of fairness and also serve to show possible ways to solve them CVdb large set of synthetic proﬁles with mation typically found in job applicants resumes These proﬁles were scored introducing gender and ethnicity ases which resulted in gender and ethnicity discrimination in the learned models targeted to generate candidate scores for hiring purposes Discrimination was observed not only when those gender and ethnicity attributes were explicitly given to the system but also when face image was given instead In this scenario the system was able to expose sitive information from these images gender and ethnicity and model its relation to the biases in the problem at hand This behavior is not limited to the case studied where bias lies in the target function Feature selection or unbalanced data can also become sources of biases This last case is common when datasets are collected from historical sources that fail to represent the diversity of our society Finally we discussed recent methods to prevent sired effects of these biases and then experimented with one of these methods SensitiveNets to improve fairness in this recruitment framework Instead of removing the sensitive information at the input level which may not be possible or practical SensitiveNets removes sensitive formation during the learning process The most common approach to analyze algorithmic crimination is through bias However cent works are now starting to investigate biased effects in AI with methods Future work will update FairCVtest with such biases in dition to the considered bias Conclusions Acknowledgments We present FairCVtest new experimental framework publicly on automated recruitment to study how multimodal machine learning is affected by ases present in the training data Using FairCVtest we have studied the capacity of common deep learning algorithms to expose and exploit sensitive information from commonly used structured and unstructured data The contributed experimental framework includes This work been supported by projects BIBECA ETN PRIMA and by Accenture is supported by research fellowship from Spanish MINECO References Acien Morales Bartolome and Fierrez Measuring the Gender and Ethnicity Bias in Deep Models for Face Recognition In Proc of IbPRIA Madrid Spain Ali Sapiezynski Bogen Korolova Mislove and Rieke Discrimination through optimization How Facebook s ad delivery can lead to skewed outcomes In Proc of ACM Conf on CHI Alvi Zisserman and Nellaker Turning blind eye Explicit removal of biases and variation from deep In Proceedings of the European ral network embeddings Conference on Computer Vision Bakker Riveron Valdes et Fair enough Improving fairness in decision making using In AAAI Workshop on Artiﬁcial dence thresholds gence Safety pages New York NY USA Baltruˇsaitis Ahuja and Morency Multimodal chine learning survey and taxonomy IEEE Transactions on Pattern Analysis and Machine Intelligence Barocas and Selbst Big data s disparate impact California Law Review Berendt and Preibusch Exploring discrimination evaluation of data mining In IEEE ICDM Workshops pages Bogen and Rieke Help wanted Examination of hiring algorithms equity and bias Technical report Buolamwini and Gebru Gender shades Intersectional accuracy disparities in commercial gender classiﬁcation In Proc ACM Conf on FAccT NY USA Feb Chandler The AI chatbot will hire you now Wired Dastin Amazon scraps secret AI recruiting tool that showed bias against women Reuters Romanov et Bias in bios case study of semantic representation bias in setting In Proc of ACM FAccT page Drozdowski Rathgeb Dantcheva Damer and Busch Demographic bias in biometrics survey on an emerging challenge Evans and Mathews New York regulator probes United Health algorithm for racial bias The Wall Street nal Fierrez Morales and Camacho Multiple classiﬁers in biometrics part Fundamentals and review Information Fusion November Ganin Ustinova Ajakan Germain Larochelle Laviolette Marchand and Lempitsky adversarial training of neural networks Journal of Machine Learning Research Fierrez et Facial soft biometrics for recognition in the wild Recent works annotation and COTS evaluation IEEE Trans on Information Forensics and Security August Goodfellow et Generative adversarial nets In Advances in Neural Information Processing Systems pages Goodman and Flaxman EU regulations on algorithmic and Right to explanation AI Magazine Jun Zhang Ren and J Sun Deep residual learning for image recognition In IEEE Conf on CVPR pages Jun Jia and Cristianini Right for the In Advances in right reason Training agnostic networks Intelligent Data Analysis XVII pages Kehrenberg Chen and Quadrianto pretable fairness via target labels in gaussian process models Kim Kim Kim Kim and Kim Learning not to learn Training deep neural networks with biased data In Proc IEEE Conf on CVPR Knight The Apple Card didn t see gender and that s the problem Wired Morales Fierrez and tiveNets Learning agnostic representations with application to face recognition Nagpal Singh Singh Vatsa and Ratha Deep learning for face recognition Pride or prejudiced Raghavan Barocas Kleinberg and Levy igating bias in algorithmic employment screening ing claims and practices Ranjan Sankaranarayanan Bansal Bodla Chen Patel Castillo and Chellappa Deep learning for understanding faces Machines may be just as good or better than humans IEEE Signal Processing azine Romanov et What s in name ducing bias in bios without access to protected attributes In Proceedings of page Sattigeri Hoffman Chenthamarakshan and Varshney Fairness GAN Generating datasets with fairness properties using generative adversarial network IBM nal of Research and Development Schroff Kalenichenko and Philbin FaceNet uniﬁed embedding for face recognition and clustering In IEEE Conf on CVPR pages Jun Serna Morales Fierrez Cebrian Obradovich and Rahwan Algorithmic discrimination Formulation and exploration in deep face biometrics In Proc of AAAI Workshop on SafeAI Sweeney Discrimination in online ad delivery Queue Mar Zhang Bellamy and Varshney Joint optimization of AI fairness and utility approach In Conf on AIES NY USA Zhao Wang Yatskar Ordonez and Chang Men also like shopping Reducing gender bias ampliﬁcation In Proc of EMNLP pages using constraints,,[]
Journal of Medical Systems https EDUCATION TRAINING Tools for Coronavirus Outbreak Need of Active Learning and Models on Data Santosh Received March Accepted March Springer Media LLC part of Springer Nature Published online March Abstract The novel coronavirus outbreak which was identified in late requires special attention because of its future epidemics and possible global threats Beside clinical procedures and treatments since Artificial Intelligence AI promises new paradigm for healthcare several different AI tools that are built upon Machine Learning ML algorithms are employed for analyzing data and processes This means that tools help identify outbreaks as well as forecast their nature of spread across the globe However unlike other healthcare issues for to detect driven tools are expected to have active models that employs multitudinal and modal data which is the primary purpose of the paper Keywords Artificial intelligence Machine learning Active learning models Multitudinal and multimodal data Introduction The novel coronavirus is global threat since it was identified in late About the Centers for Disease Control and Prevention report clearly tioned the following spread of appears to occur mainly by respiratory transmission How easily the virus is transmitted between persons is currently unclear Signs and symptoms of include fever cough and shortness of breath Based on the bation period of illness for Middle East Respiratory Syndrome MERS and Severe Acute Respiratory Syndrome SARS coronaviruses as well as tional data from reports of This article is part of the Topical Collection on Education Training Santosh Department of Computer Science University of South Dakota Clark St Vermillion SD USA CDC estimates that symptoms of occur within days after exposure According to the World Health Organization WHO report as of today March China confirmed cases and of them were died Outside China were confirmed and of them were died from tries where Italy is found to be the most influenced country after china confirmed cases deaths Similarly there are confirmed cases deaths in Republic of Korea and is exception see Fig Based on the confirmed cases fatality rate as of today is still less than other respiratory diseases study of tients finds death rate Figure provides confirmed cases of across the world Considering its global coverage the WHO already been declared public health emergency and its possible global threats including quences The devastating case in Wuhan China and future epidemics require special attention At this point it is important to note that coronavirus was not surprise case since several years ago in novel coronavirus was identified in patients with SARS and it is thought to be caused by an unknown infectious agent Since then apart from clinical procedures and ments Artificial Intelligence AI promises new paradigm Page of J Med Syst Fig Known locations of coronavirus cases by county in the US Circles are sized by the number of people there who have tested positive which may differ from where they contracted the illness More than cases have been identified in New York source https March Fig Countries territories or areas with reported confirmed cases of source https sfvrsn March J Med Syst Page of for healthcare Several different AI tools that are built upon Machine Learning ML algorithms are employed to ing data and processes tools can be used to identify novel coronavirus outbreaks as well as forecast their nature of spread across the globe However the fundamental theory behind tools is that they require sufficient training data of all possible cases Often traditional machine learning requires clean set of tated data so that classifiers can possibly be well trained which falls under scope of supervised learning Over the past decades or more tremendous progress been made in resolving many issues of several different projects However we failed to reach the point to model an accurate classifier how big the size of training samples should be Do we still wait for collecting fairly large amount of data Deep Learning DL as an example quires large amount of data to be trained The primary idea behind the use of DL is not only to avoid feature engineering but also to extract tiny features in radiology data nodule for example Collecting large amount of data is not trivial and one to wait for long Most of the reported tools are ited to models for coronavirus case AI perts state the fact that limited data may skew results away from the severity of coronavirus outbreak The Wall Street Journal reported that coronavirus reveals limits of AI health tools some makers are holding off updating their tools highlighting the shortage of data on the new coronavirus and the limitations of health services billed as AI when faced with novel illnesses Parmy Olson February In nutshell social medias papers and health reports we note that conventional driven tools for cases with less data may not vide optimal performance To detect tools are expected to have active models that employs multitudinal and multimodal data In the following within the framework of the concept of active learning will be discussed Section models and its usefulness for are discussed in Section The necessity and the use of multitudinal and multimodal data are explained in Section The paper concludes in Section Active learning As compared to passive learning traditional machine learning classifiers active learning is used to learning problem where the learner some role in determining on what data it will be trained When it is an emergency it requires special attention so that data analysis and can be made consistently without waiting several days months and years for data collection Again exploiting data is the must since one can not wait for years to train machine and learn from them nor manual is possible This means that instead of having conventional set of train validation and test set we need tools that can learn over time without having complete knowledge about the data which we call Active Learning In other words mechanism helps Incremental Learning IL over time in the presence of experts if required The ILs aim is to tively help learn model to adapt to new data without forgetting its existing limited knowledge Figure provides schematic diagram of an mechanism where different data types are used While learning the changes in data over time can be assessed with the help of Anomaly Detection AD niques In tool AD helps rare items events or observations that bring suspicions by differing nificantly from the majority of the data or with respect to set of normal data for that particular event Fig For data schema of Active Learning model is provided For better understanding in dotted red circle is used with Deep Learning DL for all possible data types In expert s feedback is used in parallel with the decisions from each data type Since DL are data dependent separate DLs are used for different data type The final decision is made based on multitudinal and multimodal data Data Type Data Type Data Type n Output Feedback Output Feedback DLn Output n Feedback Decision ﬁnal DL Deep Learning Active Learning Page of J Med Syst models Beside the use of in machine learning test models are the must in such scenarios since we do not have enough data from the particular regions This means that there is need to automatically detect nVirus in Italy from the model trained in Wuhan China In other words for such respiratory disease it is essential to have models so that automated detection can be possible In parallel the collected data can be used for training models over time which are based on the decisions Conventionally in the literature such concept does not exist Multitudinal and multimodal data More often tools are limited to one data type Decisions that are solely based on one data type regardless of the data size may be skewed away from the severity of coronavirus influence In such case use of mutitudinal and multimodal data can help support process with higher confidence Since coronaviruses are enveloped viruses with RNA genome and nucleocapsid of helical symmetry the most popularly used data for tools mostly employ RNA sequences Besides Electronic Health Record EHRs Computerized Tomography CT scans Chest CRRs see Fig and other data are considered and tested Alibaba launched new system to detect coronavirus tion via CT scans with an accuracy of up to As mentioned before AD in image consists in finding portion of the images set of pixels with anomalous and unusual patterns With small changes in image pixels signatures can be deviated from standard threshold s Since AD is not just limited to image data it can be applied for all Fig Chest CT An axial CT image shows opacities with rounded morphology arrows in the right middle and lower lobes types of data that are ranging from vector pattern matrix image for instance to data In the recently reported work Chest CT see Fig as an example which is used to diagnose can be complemented to the polymerase chain reaction tests Therefore instead of using different machine learning models for one data type and even small size data and looking for ensemble techniques to combine results for AI researchers it is wise to use mutitudinal and multimodal data to check whether different data types can help yield consistent decisions about the navirus outbreak over time Conclusions Considering the possible future epidemics of the in this paper the importance of the tools and their appropriate train and test models have been introduced and discussed The primary purpose of the paper is that AI tists do not always wait for the complete datasets to train validate and test the models Instead tools are quired to be implemented from the beginning of data tion in parallel with the experts in the field where active learning needs to be employed To achieve higher confidence during process rather than relying on one data type several data types are expected to be employed For this under the scope of active learning the use of multitudinal and multimodal data have been discussed Besides considering the spread rate of across the globe tools are expected to work as population models Compliance with ethical standards Fig Chest Bilateral focal consolidation lobar consolidation and patchy consolidation are clearly observed check lower lung Conflict of interest Author declared conflict of interest J Med Syst Page of Ethical approval This article does not contain any studies with human participants performed by any of the authors Wit van Doremalen Falzarano and Munster Sarsandmers Recentinsights into emerging coronaviruses Nature Reviews Microbiology https References Wu Zhao Yu Chen Wang Song Hu Tao Tian Pei Yuan Zhang Dai Liu Wang Zheng Xu Holmes and Zhang new coronavirus associated with human respiratory disease in china Nature https Centers for Disease Control and Prevention Morbidity and Mortality Weekly Report MMWR Public Health Response to the Coronavirus Disease Outbreak United States February Chen Zhou Dong Qu Gong Qiu Wang Liu Wei Xia Yu Zhang and Zhang Epidemiological and clinical characteristics of cases of novel coronavirus pneumonia in wuhan china descriptive study The Lancet http WHO Report Coronavirus disease Situation Report accessed March https Wu and McGoogan Characteristics of and important lessons from the coronavirus disease outbreak in China Summary of report of cases from the Chinese center for disease control and prevention JAMA https Medscape Medical News The WHO declares public health gency for novel coronavirus January https Long and Ehrenfeld The role of augmented intelligence ai in detecting and preventing the spread of novel coronavirus Journal of Medical Systems https Paules Marston and Fauci Coronavirus tions more than just the common cold JAMA https Drosten Gunther et Identification of novel corona virus in patients with severe acute respiratory syndrome N Engl J Med https Peiris Lai Poon Guan Yam Lim Nicholls Yee Yan Cheung Cheng Chan Tsang Yung Ng and Yuen Coronavirus as possible cause of severe acute respiratory syndrome The Lancet https Song Xu Bao Zhang Yu Qu Zhu Zhao and Qin From sars to mers thrusting coronaviruses into the spotlight Viruses https Sammut and Webb eds Encyclopedia of machine ing and data mining Springer https Chen Wu Zhang Zhang Gong Zhao Hu Wang Hu Zheng Zhang Wu Dong Xu Zhu Chen Yu and Yu Deep model for detecting novel coronavirus pneumonia on computed tomography prospective study medRxiv https Guo Li Wang Wang Fang Tan Wu Xiao and Zhu Host and infectivity prediction of wuhan novel coronavirus using deep learning rithm bioRxiv https Dewey and Schlattmann Deep learning and medical nosis The Lancet https The Wall Street Journal Coronavirus reveals limits of AI health tools accessed February https Bouguelia Nowaczyk Santosh and Verikas Agreeing to disagree Active learning with noisy labels without crowdsourcing Int J Machine Learning Cybernetics https Technology Org AI algorithm detects coronavirus infections in patients from CT scans with accuracy accessed March https Ai Yang Hou Zhan Chen Lv Tao Sun and Xia Correlation of chest ct and testing in virus disease in china report of cases Radiology https Bernheim Mei Huang Yang Fayad Zhang Diao Lin Zhu Li Li Shan Jacobi and Chung Chest ct findings in coronavirus Relationship to duration of infection Radiology https Fong Li Dey Crespo and Finding an accurate early forecasting model from small dataset case of novel coronavirus outbreak International Journal of Interactive Multimedia and Artificial Intelligence https Publisher s note Springer Nature remains neutral with regard to tional claims in published maps and institutional affiliations,,[]
Multimodal biomedical AI Julián Acosta Guido Pranav Rajpurkar and Eric Topol The increasing availability of biomedical data from large biobanks electronic health records medical imaging wearable and ambient biosensors and the lower cost of genome and microbiome sequencing have set the stage for the development of timodal artificial intelligence solutions that capture the complexity of human health and disease In this Review we outline the key applications enabled along with the technical and analytical challenges We explore opportunities in personalized medicine digital clinical trials remote monitoring and care pandemic surveillance digital twin technology and virtual health assistants Further we survey the data modeling and privacy challenges that must be overcome to realize the full potential of multimodal artificial intelligence in health While artificial intelligence AI tools have transformed several domains for example language translation speech recognition and natural image recognition medicine lagged behind This is partly due to complexity and high other words large number of unique tures or signals contained in the to technical lenges in developing and validating solutions that generalize to diverse populations However there is now widespread use of able sensors and improved capabilities for data capture aggregation and analysis along with decreasing costs of genome sequencing and related omics technologies Collectively this sets the foundation and need for novel tools that can meaningfully process this wealth of data from multiple sources and provide value across biomedical discovery diagnosis prognosis treatment and prevention Most of the current applications of AI in medicine have addressed narrowly defined tasks using one data modality such as computed tomography CT scan or retinal photograph In trast clinicians process data from multiple sources and modalities when diagnosing making prognostic evaluations and deciding on treatment plans Furthermore current AI assessments are typically snapshots based on moment of time when the ment is performed and therefore not seeing health as continuous state In theory however AI models should be able to use all data sources typically available to clinicians and even those able to most of them for example most clinicians do not have deep understanding of genomic medicine The development of multimodal AI models that incorporate data across including biosensors genetic epigenetic proteomic microbiome metabolomic imaging text clinical social determinants and ronmental poised to partially bridge this gap and enable broad applications that include individualized medicine integrated pandemic surveillance digital clinical trials and virtual health coaches Fig In this Review we explore the ties for such multimodal datasets in healthcare we then discuss the key challenges and promising strategies for overcoming these Basic concepts in AI and machine learning will not be discussed here but are reviewed in detail Opportunities for leveraging multimodal data Personalized omics for precision health With the remarkable progress in sequencing over the past two decades there been revolution in the amount of biological data that can be obtained using novel technical developments These are collectively referred to as the omes and includes the genome proteome scriptome immunome epigenome metabolome and These can be analyzed in bulk or at the level which is relevant because many medical conditions such as cancer are quite heterogeneous at the tissue level and much of biology shows cell and tissue specificity Each of the omics shown value in different clinical and research settings individually Genetic and molecular markers of malignant tumors have been integrated into clinical with the US Food and Drug Administration FDA providing approval for several companion diagnostic devices and nucleic As an example Foundation Medicine and Oncotype IQ Genomic Health offer comprehensive genomic profiling tailored to the main classes of genomic alterations across broad panel of genes with the final goal of identifying potential therapeutic Beyond these molecular markers liquid biopsy easily accessible biological fluids such as blood and becoming widely used tool for analysis in precision oncology with some tests based on circulating tumor cells and circulating tumor DNA already approved by the Beyond oncology there been remarkable increase in the last years in the availability and sharing of genetic data which enabled association and characterization of the genetic architecture of complex human conditions and This improved our ing of biological pathways and produced tools such as polygenic risk which capture the overall genetic propensity to complex traits for each individual and may be useful for risk tion and individualized treatment as well as in clinical research to enrich the recruitment of participants most likely to benefit from The integration of these very distinct types of data remains lenging Yet overcoming this problem is paramount as the cessful integration of omics data in addition to other types such as electronic health record EHR and imaging data is expected to increase our understanding of human health even further and allow for precise and individualized preventive diagnostic and therapeutic Several approaches have been proposed for data integration in precision health Graph neural networks are one example these are deep learning model of Neurology Yale School of Medicine New Haven CT USA of Biomedical Informatics Harvard Medical School Boston MA USA Research Translational Institute Scripps Research Jolla CA USA authors jointly supervised this work Pranav Rajpurkar Eric Topol etopol NATuRE MEdIcINE VOL SEPTEMBER Review ARticlehttps Data modalities Opportunities Omics Precision health Metabolites immune status biomarkers Microbiome Wearable biosensors Ambient sensors Environment Digital clinical trials home Pandemic surveillance MATCH Digital twins Virtual health coach Fig data modalities and opportunities for multimodal biomedical AI Created with architectures that process computational data structure comprising nodes representing concepts or ties and edges representing connections or relationships between nodes allowing scientists to account for the known related structure of multiple types of omics data which can improve performance of Another approach is dimensionality reduction including novel methods such as PHATE and Multiscale PHATE which can learn abstract representations of biological and clinical data at different levels of granularity and have been shown to predict clinical outcomes for example in people with rus disease In the context of cancer overcoming challenges related to data access sharing and accurate labeling could potentially lead to impactful tools that leverage the combination of personalized omics data with histopathology imaging and clinical data to inform clinical trajectories and improve patient The tion of histopathological morphology data with transcriptomics data resulting in spatially resolved constitutes novel and promising methodological advancement that will enable research into gene expression within spatial context Of note researchers have utilized deep learning to leverage thology images to predict spatial gene expression from these images alone pointing to morphological features in these images not tured by human experts that could potentially enhance the utility and lower the costs of this Genetic data are increasingly cost effective requiring only ascertainment but they also have limited tive ability on their Integrating genomics data with other omics data may capture more dynamic and information on how each particular combination of genetic background and environmental exposures interact to produce the quantifiable tinuum of health status As an example Kellogg et conducted an study performing sequencing WGS and periodic measurements of other omics layers transcriptome teome metabolome antibodies and clinical biomarkers polygenic risk scoring showed an increased risk of type II diabetes mellitus and comprehensive profiling of other omics enabled early detection and dissection of signaling network changes during the transition from health to disease As the scientific field advances the profile of WGS will become increasingly favorable facilitating the tion of clinical and biomarker data with already available genetic data to arrive at rapid diagnosis of conditions that were previously difficult to Ultimately the capability to develop multimodal AI that includes many layers of omics data will get us to the desired goal of deep phenotyping of an individual in other words true understanding of each person s biological uniqueness and how that affects health Digital clinical trials Randomized clinical trials are the gold dard study design to investigate causation and provide evidence to support the use of novel diagnostic prognostic and therapeutic interventions in clinical medicine Unfortunately planning and executing clinical trial is not only time ing usually taking many years to recruit enough participants and follow them in time but also financially very In tion geographic sociocultural and economic disparities in access to enrollment have led to remarkable underrepresentation of several groups in these studies This limits the generalizability of results and leads to scenario whereby widespread sentation in biomedical research further perpetuates existing Digitizing clinical trials could provide an unprecedented opportunity to overcome these limitations by reducing barriers to participant enrollment and retainment promoting engagement and optimizing trial measurements and interventions At the same time the use of digital technologies can enhance the granularity of NATuRE MEdIcINE VOL SEPTEMBER Review ARticleNaTuRe MedIcINe the information obtained from participants thereby increasing the value of these Data from wearable technology including heart rate sleep physical activity electrocardiography oxygen saturation and cose monitoring and naires can be useful for monitoring clinical trial patients identifying adverse events or ascertaining trial Additionally recent study highlighted the potential of data from wearable sors to predict laboratory Consequently the number of studies using digital products been growing rapidly in the last few years with compound annual growth rate of around Most of these studies utilize data from single wearable device One pioneering trial used patch sensor for detecting atrial fibrillation the sensor was mailed to participants who were enrolled remotely without the use of any clinical sites and set the foundation for digitized clinical Many remote trials using wearables were conducted during the pandemic to detect Effectively combining data from different wearable sensors with clinical data remains challenge and an opportunity Digital cal trials could leverage multiple sources of participants data to enable automatic phenotyping and which could be useful for adaptive clinical trial designs that use ongoing results to modify the trial in real In the future we expect that the increased availability of these data and novel multimodal learning techniques will improve our capabilities in digital clinical trials Of note recent work in analysis by Google strated the promise of model architectures to bine both static and inputs to achieve interpretable forecasting As hypothetical example these models could understand whether to focus on static features such as genetic background known features such as time of the day or observed features such as current glycemic levels to make predictions on future risk of hypoglycemia or Graph neural networks have been recently proposed to overcome the problem of missing or irregularly sampled data from multiple health sensors by leveraging information from the interconnection between Patient recruitment and retention in clinical trials are essential but remain challenge In this setting there is an increasing interest in the utilization of synthetic control methods that is using nal data to create controls Although synthetic control trials are still relatively the FDA already approved medications based on historical and developed framework for the lization of AI models utilizing data from ferent modalities can potentially help identify or generate the most optimal synthetic Remote monitoring the Recent progress with biosensors continuous monitoring and analytics raises the sibility of simulating the hospital setting in person s home This offers the promise of marked reduction of cost less requirement for healthcare workforce avoidance of nosocomial infections and medical errors that occur in medical facilities along with the comfort convenience and emotional support of being with family In this context wearable sensors have crucial role in remote patient monitoring The availability of relatively affordable vasive devices smartwatches or bands that can accurately measure several physiological metrics is increasing Combining these data with those derived from standards such as the Fast Healthcare Interoperability Resources global industry standard for exchanging healthcare query relevant mation about patient s underlying disease risk could create more personalized remote monitoring experience for patients and givers Ambient wireless sensors offer an additional opportunity to collect valuable data Ambient sensors are devices located within the environment for example room wall or mirror ranging from video cameras and microphones to depth cameras and radio signals These ambient sensors can potentially improve remote care systems at home and in healthcare The integration of data from these multiple modalities and sensors represents promising opportunity to improve remote patient monitoring and some studies have already demonstrated the potential of multimodal data in these scenarios For example the combination of ambient sensors such as depth cameras and microphones with wearables data for example accelerometers which measure physical activity the potential to improve the reliability of fall detection systems while keeping low false alarm and to improve gait analysis Early detection of impairments in physical functional status via activities of daily living such as bathing dressing and eating is remarkably important to provide timely clinical care and the utilization of multimodal data from wearable devices and ambient sensors can potentially help with accurate detection and classification of difficulties in these Beyond management of chronic or degenerative disorders timodal remote patient monitoring could also be useful in the ting of acute disease recent program conducted by the Mayo Clinic showcased the feasibility and safety of remote monitoring in people with ref Remote patient monitoring for yet domized trials of multimodal remote monitoring versus hospital admission to show impairment of safety We need to be able to predict impending deterioration and have system to vene and this not been achieved yet Pandemic surveillance and outbreak detection The current pandemic highlighted the need for effective tious disease surveillance at national and state with some countries successfully integrating multimodal data from migration maps mobile phone utilization and health delivery data to forecast the spread of the outbreak and identify potential One study also demonstrated the utilization of resting heart rate and sleep minutes tracked using wearable devices to improve surveillance of illness in the This initial cess evolved into the Digital Engagement and Tracking for Early Control and Treatment DETECT Health study launched by the Scripps Research Translational Institute as an research program aiming to analyze diverse set of data from wearables to allow for rapid detection of the emergence of influenza coronavirus and other viral illnesses study from this program showed that jointly considering participant symptoms and sensor metrics improved performance relative to either modality alone reaching an area under the receiver operating curve value of confidence interval for ing versus Several other use cases for multimodal AI models in pandemic preparedness and response have been tested with promising results but further validation and replication of these results are Digital twins We currently rely on clinical trials as the best dence to identify successful interventions Interventions that help of people may be considered successful but these are applied to the other without proven or likely benefit complementary approach known as digital twins can fill the knowledge gaps by leveraging large amounts of data to model and predict with high precision how certain therapeutic intervention would benefit or harm particular patient Digital twin technology is concept borrowed from engineering that uses computational models of complex systems for example cities airplanes or patients to develop and test different strategies NATuRE MEdIcINE VOL SEPTEMBER Review ARticleNaTuRe MedIcINe or approaches more quickly and economically than in In healthcare digital twins are promising tool for drug target Integrating data from multiple sources to develop digital twin models using AI tools already been proposed in precision oncology and cardiovascular An modular framework also been proposed for the development of medical digital twin From commercial point of view developed and tested digital twin models that leverage diverse sets of clinical data to enhance clinical trials for Alzheimer s disease and multiple Considering the complexity of human organisms the ment of accurate and useful digital twin technology in medicine will depend on the ability to collect large and diverse multimodal data ranging from omics data and physiological sensors to clinical and sociodemographic data This will likely require large tions across health systems research groups and industry such as the Swedish Digital Twins The American Society of Clinical Oncology through its subsidiary called CancerLinQ developed platform that enables researchers to utilize wealth of data from patients with cancer to help guide optimal treatment and improve The development of AI models capable of effectively learning from all these data modalities together to make predictions is paramount Virtual health assistant More than of US consumers have acquired smart speaker in the last few years However virtual health coaches that can advise people on their health not been developed widely to date and those currently in the market often target particular tion or use case In addition recent review of versational agents apps found that most of these rely on approaches and predefined One of the most popular although not multimodal current applications of these narrowly focused virtual health tants is in diabetes care Virta health Accolade and Onduo by Verily Alphabet have all developed applications that aim to improve betes control with some demonstrating improvement in bin levels in individuals who followed the Many of these companies have expanded or are in the process of ing to other use cases such as hypertension control and weight loss Other examples of virtual health coaches have tackled mon conditions such as migraine asthma and chronic obstructive pulmonary disease among Unfortunately most of these applications have been tested only on small observational studies and much more research including randomized clinical trials are needed to evaluate their benefits Looking into the future the successful integration of multiple data sources in AI models will facilitate the development of broadly focused personalized virtual health These virtual health assistants can leverage individualized profiles based on genome sequencing other omics layers continuous monitoring of blood biomarkers and metabolites biosensors and other relevant ical promote behavior change answer tions triage symptoms or communicate with healthcare providers when appropriate Importantly these medical coaches will need to demonstrate beneficial effects on clinical outcomes via randomized trials to achieve widespread acceptance in the medical field As most of these applications are focused on improving health choices they will need to provide evidence of influencing health behavior which represents the ultimate pathway for the successful translation of most We still have long way to go to achieve the full potential of AI and multimodal data integration into virtual health assistants including the technical challenges challenges and privacy challenges discussed below Given the rapid advances in conversational coupled with the development of increasingly sophisticated multimodal learning approaches we expect future digital health applications to embrace the potential of AI to deliver accurate and personalized health coaching Multimodal data collection The first requirement for the successful development of modal applications is the collection curation and harmonization of and large annotated datasets as amount of technical sophistication can derive information not present in the In the last years many national and national studies have collected multimodal data with the ultimate goal of accelerating precision health Table In the UK the UK Biobank initiated enrollment in reaching final participant count of over and plans to follow participants for at least years after This large biobank collected multiple layers of data from participants including sociodemographic and lifestyle information physical measurements biological samples electrocardiograms and EHR Further almost all participants underwent array genotyping and more recently proteome and set of individuals also underwent brain magnetic resonance ing MRI cardiac MRI abdominal MRI carotid ultrasound and absorptiometry including repeat imaging across at least two time Similar initiatives have been conducted in other countries such as the China Kadoorie and Biobank In the USA the Department of Veteran Affairs launched the Million Veteran in aiming to enroll million veterans to tribute to scientific discovery Two important efforts funded by the National Institutes of Health NIH include the for Precision Medicine TOPMed program and the All of Us Research Program TOPMed collects WGS with the aim to grate this genetic information with other omics The All of Us Research constitutes another novel and ambitious initiative by the NIH that enrolled about diverse ticipants of the million people planned across the USA and is focused on enrolling individuals from broadly defined resented groups in biomedical research which is especially needed in medical Besides these large national initiatives independent institutional and efforts are also building deep multimodal data resources in smaller numbers of people The Project Baseline Health Study funded by Verily and managed in collaboration with Stanford University Duke University and the California Health and Longevity Institute aims to enroll at least individuals ing with an initial participants from whom broad range of multimodal data are collected with the aim of evolving into bined research As another example the American Gut Project collects microbiome data from participants across several These participants also complete surveys about general health status disease history style data and food frequency The Medical Information Mart for Intensive Care MIMIC organized by the Massachusetts Institute of Technology represents another example of mensional data collection and harmonization Currently in its fourth version MIMIC is an database that contains data from thousands of patients who were admitted to the critical care units of the Beth Israel Deaconess Medical Center including demographic information EHR data for example nosis codes medications ordered and administered laboratory data and physiological data such as blood pressure or intracranial sure values imaging data for example chest radiographs and in some versions natural language text such as radiology reports and medical notes This granularity of data is particularly useful for the data science and machine learning community and MIMIC NATuRE MEdIcINE VOL SEPTEMBER Review ARticleNaTuRe MedIcINe Table Examples of studies with multimodal data available Study UK Biobank country Year started data modalities UK China Kadoorie Biobank China Biobank Japan Japan Million Veteran Program USA TOPMed USA All of Us Research Program USA Project Baseline Health Study USA American Gut Project USA Questionnaires Laboratory genotyping WES WGS Imaging Metabolites Questionnaires Physical measurements Biosamples genotyping Questionnaires Clinical Laboratroy genotyping Laboratory Genome wide Clinical WGS Questionnaires SDH Laboratory Genome wide Wearables Questionnaires Laboratory Wearables Clinical Diet Microbiome Images Access Open access Sample size Restricted access Restricted access Restricted access million Open access Open access million target Restricted access target Open access Open access MIMIC MIPACT North American Prodrome Longitudinal Study USA USA USA Wearables physiological Restricted access laboratory Clinical Genetic Restricted access SDH social determinants of health WES sequencing become one of the benchmark datasets for AI models aiming to predict the development of clinical events such as kidney failure or outcomes such as survival or The availability of multimodal data in these datasets may help achieve better diagnostic performance across range of ent tasks As an example recent work demonstrated that the combination of imaging and EHR data outperforms each of these modalities alone to identify pulmonary and to entiate between common causes of acute respiratory failure such as heart failure pneumonia or chronic obstructive pulmonary The Michigan Predictive Activity Clinical Trajectories in Health MIPACT study constitutes another example with pants contributing data from wearables physiological data blood pressure clinical information EHR and surveys and laboratory The North American Prodrome Longitudinal Study is yet another example This multisite program recruited individuals and collected demographic clinical and blood biomarker data with the goal of understanding the prodromal stages of Other studies focusing on psychiatric disorders such as the Personalised Prognostic Tools for Early Psychosis Management also collected several types of data and have already empowered the development of multimodal machine learning Technical challenges Implementation and modeling challenges Health data are ently multimodal Our health status encompasses many domains social biological and environmental that influence in complex ways Additionally each of these domains is hierarchically organized with data being abstracted from the big picture macro level for example disease presence or absence to the micro level for example biomarkers proteomics and genomics Furthermore current healthcare systems add to this multimodal approach by generating data in multiple ways radiology and ogy images are for example paired with natural language data from their respective reports while disease states are also documented in natural language and tabular data in the EHR NATuRE MEdIcINE VOL SEPTEMBER Review ARticleNaTuRe MedIcINe Multimodal machine learning also referred to as multimodal learning is subfield of machine learning that aims to develop and train models that can leverage multiple different types of data and learn to relate these multiple modalities or combine them with the goal of improving prediction promising approach is to learn accurate representations that are similar for different modalities for example picture of an apple should be sented similarly to the word apple In early OpenAI released an architecture termed Contrastive Language Image Pretraining CLIP which when trained on millions of pairs matched the performance of competitive fully supervised models without CLIP was inspired by similar approach developed in the medical imaging domain termed Contrastive Visual Representation Learning from Text ConVIRT With ConVIRT an image encoder and text encoder are trained to erate image and text representations by maximizing the similarity of correctly paired image and text examples and minimizing the similarity of incorrectly paired is called tive learning This approach for paired been used recently to learn from chest and their associated text reports outperforming other and fully vised Other architectures have also been developed to integrate multimodal data from images audio and text such as the Transformer which uses videos to obtain paired multimodal image text and audio and to train accurate multimodal representations able to generalize with good performance on many as recognizing actions in videos classifying audio events classifying images and selecting the most adequate video for an input Another desirable feature for multimodal learning frameworks is the ability to learn from different modalities without the need for different model architectures Ideally unified multimodal model would incorporate different types of data images cal sensor data and structured and unstructured text data among others codify concepts contained in these different types of data in flexible and sparse way that is unique task activates only small part of the network with the model learning which parts of the network should handle each unique task produce aligned representations for similar concepts across modalities for example the picture of dog and the word dog should produce similar internal representations and provide any arbitrary type of output as required by the In the last few years there been transition from tures with strong as convolutional neural networks for images or recurrent neural networks for text and physiological relatively novel architecture called the Transformer which demonstrated good performance across wide variety of input and output modalities and The key strategy behind transformers is to allow neural are artificial learning models that loosely mimic the behavior of the human dynamically pay attention to different parts of the input when processing and ultimately making decisions Originally proposed for natural language processing thus providing way to capture the context of each word by attending to other words of the input sentence this architecture been successfully extended to other While each input token that is the smallest unit for ing in natural language processing corresponds to specific word other modalities have generally used segments of images or video clips as Transformer architectures allow us to unify the framework for learning across modalities but may still need tokenization and encoding recent study by Meta AI Meta Platforms proposed unified framework for learning that is independent of the modality of interest but still requires preprocessing and Benchmarks for multimodal learning allow us to measure the progress of methods across modalities for instance the Benchmark for learning DABS is recently proposed benchmark that includes chest sensor data and natural image and text Recent advances proposed by DeepMind Alphabet including and Perceiver propose framework for learning across modalities with the same backbone architecture Importantly the input to the Perceiver architectures are byte arrays which are condensed through an attention bottleneck that is an architecture feature that restricts the flow of information ing models to condense the most relevant to avoid large memory costs Fig After processing these inputs the Perceiver can then feed the representations to final classification layer to obtain the probability of each output category while the Perceiver IO can decode these representations directly into arbitrary outputs such as pixels raw audio and classification labels through query vector that specifies the task of interest for example the model could output the predicted imaging appearance of an ing brain tumor in addition to the probability of successful ment response promising aspect of transformers is the ability to learn ingful representations with unlabeled data which is paramount in biomedical AI given the limited and expensive resources needed to obtain labels Many of the approaches mentioned above require aligned data from different modalities for example pairs study from DeepMind in fact suggested that curating datasets may be more important than generating large datasets and other aspects of algorithm development and However these data may not be readily available in the setting of biomedical AI One sible solution to this problem is to leverage available data from one modality to help learning with multimodal learning task termed As an example some studies suggest that transformers pretrained on unlabeled language data might be able to generalize well to broad range of other In cine model architecture called CycleGANs trained on unpaired contrast and CT scans been used to generate synthetic or contrast CT with this approach showing improvements for instance in While promising this approach not been tested widely in the biomedical setting and requires further exploration Another important modeling challenge relates to the exceedingly high number of dimensions contained in multimodal health data collectively termed the curse of dimensionality As the number of dimensions that is variables or features contained in dataset increases the number of people carrying some specific tions of these features decreases or for some combinations even disappears leading to dataset blind spots that is portions of the feature space the set of all possible combinations of features or variables that do not have any observation These dataset blind spots can hurt model performance in terms of prediction and should therefore be considered early in the model development and evaluation Several strategies can be used to mitigate this issue and have been described in detail In brief these include collecting data using maximum performance tasks for example rapid finger tapping for motor control as opposed to passively collected data during everyday movement ensuring large and diverse sample sizes that is with the conditions matching those expected at clinical deployment of the model using domain edge to guide feature engineering and selection with focus on ture repeatability appropriate model training and regularization rigorous model validation and comprehensive model monitoring including monitoring the difference between the distributions of training data and data found after deployment Looking to the future developing models able to incorporate previous knowledge for example known gene regulatory pathways and protein interactions NATuRE MEdIcINE VOL SEPTEMBER Review ARticleNaTuRe MedIcINe b Audio input Image input Surgical and actions Masked and shifted target Byte arrays Concatenated input mechanism Rest of the network architecture multiple layers Output Text The patient developed heart Concatenated input Multimodal multitask AI model Images and questions User What caused this rash AI This is probably an allergic reaction Fig Simplified illustration of the novel technical concepts in multimodal AI Simplified schematic of the architecture images text and other inputs are converted agnostically into byte arrays that are concatenated that is fused and passed through mechanisms that is mechanism to project or condense information into representation to feed information into the network b Simplified illustration of the conceptual framework behind the multimodal multitask architectures for example Gato within hypothetical medical example distinct input modalities ranging from images text and actions are tokenized and fed to the network as input sequences with masked shifted versions of these sequences fed as targets that is the network only sees information from previous time points to predict future actions only previous words to predict the next or only the image to predict text the network then learns to handle multiple modalities and tasks might be another promising approach to overcome the curse of dimensionality Along these lines recent studies demonstrated that models augmented by retrieving information from large databases outperform larger models trained on larger datasets effectively leveraging available information and also providing added benefits such as An increasingly used approach in multimodal learning is to combine the data from different modalities as opposed to simply inputting several modalities separately into model to increase prediction termed multimodal fusion Fusion of different data modalities can be performed at ent stages of the process The simplest approach involves enating input modalities or features before any processing early fusion While simple this approach is not suitable for many plex data modalities more sophisticated approach is to combine and representations of these different modalities during the training process joint fusion allowing for preprocessing while still capturing the interaction between data modalities Finally an alternative approach is to train separate models for each modality and combine the output probabilities late fusion simple and robust approach but at the cost of missing any information that could be abstracted from the interaction between modalities Early work on fusion focused on allowing models to leverage information from structured covariates for tasks such as forecasting osteoarthritis progression and predicting cal outcomes in patients with cerebral As another ple of fusion group from DeepMind used dataset comprising dimensions that were jected into continuous embedding space with only dimensions capturing wide array of information in time frame for each patient and built recurrent neural network to predict acute ney injury over lot of studies have used fusion of two modalities bimodal fusion to improve predictive performance Imaging and data have been fused to improve detection of pulmonary embolism outperforming Another bimodal study fused imaging features from chest with clinical covariates improving the diagnosis of tuberculosis in individuals with Optical coherence tomography and red reflectance optic disc imaging have been combined to better predict visual field maps compared to using either of those ties Multimodal fusion is general concept that can be tackled using any architectural choice Although not biomedical we can learn from some AI imaging work modern guided image tion models such as and often concatenate information from different modalities into the same encoder This approach demonstrated success in recent study conducted by DeepMind using Gato generalist agent showing that nating wide variety of tokens created from text images and button presses among others can be used to teach model to perform eral distinct tasks ranging from captioning images and playing Atari games to stacking blocks with robot arm Fig Importantly recent study titled Align Before Fuse suggested that aligning resentations across modalities before fusing them might result in better performance in downstream tasks such as for creating text captions for recent study from Google Research posed using attention bottlenecks for multimodal fusion thereby restricting the flow of information to force models NATuRE MEdIcINE VOL SEPTEMBER Review ARticleNaTuRe MedIcINe to share the most relevant information across modalities and hence improving computational Another paradigm of using two modalities together is to late from one to the other In many cases one data modality may be strongly associated with clinical outcomes but be less affordable accessible or require specialized equipment or invasive procedures Deep computer vision been shown to ture information typically requiring modality for human interpretation As an example one study developed lutional neural network that uses echocardiogram videos to predict laboratory values of interest such as cardiac biomarkers troponin I and brain natriuretic peptide and other commonly obtained markers and found that predictions from the model were accurate with some of them even having more prognostic performance for heart failure admissions than conventional laboratory Deep learning also been widely studied in cancer pathology to make predictions beyond typical pathologist interpretation tasks with H stains with several applications including prediction of genotype and gene expression response to treatment and survival using only pathology images as Many other important challenges relating to multimodal model architectures remain For some modalities for example imaging even models using only single time point require large computing capabilities and the prospect of implementing model that also processes omics or text data represents an important infrastructural challenge While multimodal learning improved at an accelerated rate for the past few years we expect that current methods are unlikely to be sufficient to overcome all the major challenges mentioned above Therefore further innovation will be required to fully enable effective multimodal AI models Data challenges The multidimensional data underpinning health leads to broad range of challenges in terms of collecting linking and annotating these data Medical datasets can be described along several including the sample size depth of phenotyping the length and intervals of the degree of interaction between participants the heterogeneity and diversity of the participants the level of standardization and harmonization of the data and the amount of linkage between data sources While science and nology have advanced remarkably to facilitate data collection and phenotyping there are inevitable among these features of biomedical datasets For example although large sample sizes in the range of hundreds of thousands to millions are desirable in most cases for the training of AI models especially multimodal AI models the costs of achieving deep phenotyping and good dinal scales rapidly with larger numbers of participants becoming financially unsustainable unless automated methods of data collection are put in place There are efforts to provide meaningful zation to biomedical datasets such as the Observational Medical Outcomes Partnership Common Data Model developed by the Observational Health Data Sciences and Informatics Harmonization enormously facilitates research efforts and enhances reproducibility and translation into clinical practice However harmonization may obscure some relevant ological processes underlying certain diseases As an example emic stroke subtypes tend not to be accurately captured by existing but utilizing raw data from EHRs or radiology reports could allow for the use of natural language processing for Similarly the Diagnostic and Statistical Manual of Mental Disorders categorizes diagnoses based on clinical manifestations which might not fully represent underlying pathophysiological Achieving diversity across ancestry income level education level healthcare access age disability status geographic locations gender and sexual orientation proven difficult in practice Genomics research is prominent example with the vast majority of studies focusing on individuals from European However diversity of biomedical datasets is paramount as it constitutes the first step to ensure generalizability to the broader Beyond these considerations required step for timodal AI is the appropriate linking of all data types available in the datasets which represents another challenge owing to the ing risk of identification of individuals and regulatory Another frequent problem with biomedical data is the usually high proportion of missing data While simply excluding patients with missing data before training is an option in some cases tion bias can arise when other factors influence missing and it is often more appropriate to address these gaps with tical tools such as multiple As result imputation is pervasive preprocessing step in many biomedical scientific fields ranging from genomics to clinical data Imputation remarkably improved the statistical power of tion studies to identify novel genetic risk loci and is facilitated by large reference datasets with deep genotypic coverage such as the the Haplotype reference and recently Beyond genomics imputation also demonstrated utility for other types of medical Different strategies have been suggested to make fewer assumptions These include imputation with imputed values flagged and information added on when they were last and more complex strategies such as capturing the presence of missing data and time intervals using learnable decay The risk of incurring several biases is important when conducting studies that collect health data and multiple approaches are sary to monitor and mitigate these The risk of these biases is amplified when combining data from multiple sources as the bias toward individuals more likely to consent to each data ity could be amplified when considering the intersection between these potentially biased populations This complex and unsolved problem is more important in the setting of multimodal health data compared to unimodal data and would warrant its own review Medical AI algorithms using demographic features such as race as inputs can learn to perpetuate historical human biases thereby resulting in harm when Importantly recent work demonstrated that AI models can identify such features solely from imaging data which highlights the need for ate efforts to detect racial bias and equalize racial outcomes during data quality control and model In particular tion bias is common type of bias in large biobank studies and been reported as problem for example in the UK This problem also been pervasive in the scientific literature regarding ref For example patients using allergy medications were more likely to be tested for which leads to an ficially lower rate of positive tests and an apparent protective effect among those due to selection Importantly selection bias can result in AI models trained on sample that fers considerably from the general thus hurting these models at inference Privacy challenges The successful development of multimodal AI in health requires breadth and depth of data which passes higher privacy challenges than AI models For example previous studies have demonstrated that by utilizing only little background information about participants an sary could those in large datasets for example the Netflix prize dataset uncovering sensitive information about the In the USA the Health Insurance Portability and Accountability Act HIPAA Privacy Rule is the fundamental legislation to protect privacy of health data However some types of health as NATuRE MEdIcINE VOL SEPTEMBER Review ARticleNaTuRe MedIcINe and health not covered by this regulation which poses risk of reidentification by ing information from multiple sources In contrast the more recent General Data Protection Regulation GDPR from the European Union much broader scope regarding the definition of health data and even goes beyond data protection to also require the release of information about automated using these Given the challenges multiple technical solutions have been proposed and explored to ensure security and privacy while ing multimodal AI models including differential privacy ated learning homomorphic encryption and swarm Differential privacy proposes systematic random perturbation of the data with the ultimate goal of obscuring mation while maintaining the global distribution of the As expected this approach constitutes between the level of privacy obtained and the expected performance of the models Federated learning on the other hand allows several individuals or health systems to collectively train model without ring raw data In this approach trusted central server distributes model to each of the each individual or organization then trains the model for certain number of iterations and shares the model updates back to the trusted central Finally the trusted central server aggregates the model updates from all and starts another round Federated multimodal learning been implemented in collaboration for predicting clinical outcomes in people with ref Homomorphic encryption is cryptographic technique that allows mathematical operations on encrypted input data therefore providing the possibility of ing model weights without leaking Finally swarm learning is relatively novel approach that similarly to federated learning is also based on several individuals or organizations training model on local data but does not require trusted central server because it replaces it with the use of blockchain smart Importantly these approaches are often complementary and they can and should be used together recent study demonstrated the potential of coupling federated learning with homomorphic tion to train model to predict diagnosis from chest CT scans with the aggregate model outperforming all of the locally trained While these methods are promising multimodal health data are usually spread across several distinct organizations ranging from healthcare institutions and academic centers to maceutical companies Therefore the development of new methods to incentivize data sharing across sectors while preserving patient privacy is crucial An additional layer of safety can be obtained by leveraging novel developments in edge Edge computing as opposed to cloud computing refers to the idea of bringing computation closer to the sources of data for example close to ambient sensors or able devices In combination with other methods such as federated learning edge computing provides more security by avoiding the transmission of sensitive data to centralized servers Furthermore edge computing provides other benefits such as reducing storage costs latency and bandwidth usage For example some tems now run optimized versions of deep learning models directly in their hardware instead of transferring images to cloud servers for identification of As result of the expanding healthcare AI market biomedical data are increasingly valuable leading to another challenge taining to data ownership To date this constitutes an open issue of debate Some voices advocate for private patient ownership of the data arguing that this approach would ensure the patients right to support health data transactions and maximize patients benefit from data markets while others suggest Box Priorities for future development of multimodal biomedical AI Discover and formulate key medical AI tasks for which timodal data will add value over single modalities Develop approaches that can pretrain models using large amounts of unlabeled data across modalities and only require on limited labeled data Benchmark the effect of model architectures and multimodal approaches when working with previously underexplored data such as omics data Collect paired for example multimodal data that could be used to train and test the generalizability of multimodal medical AI algorithms regulatory model would better protect secure and transparent data Independent of the framework ate incentives should be put in place to facilitate data sharing while ensuring security and conclusion Multimodal medical AI unlocks key applications in healthcare and many other opportunities exist beyond those described here The field of drug discovery is pertinent example with many tasks that could leverage multidimensional data including target tion and validation prediction of drug interactions and prediction of side While we addressed many important challenges to the use of multimodal AI others that were outside the scope of this review are just as important including the potential for false positives and how clinicians should interpret and explain the risks to patients With the ability to capture multidimensional biomedical data we confront the challenge of deep each individual s uniqueness Collaboration across industries and sectors is needed to collect and link large and diverse multimodal health data Box Yet as this juncture we are far better at ing and storing such data than we are at data analysis To ingfully process such data and actualize the many exciting use cases it will take concentrated joint effort of the medical community and AI researchers to build and validate new models and ultimately demonstrate their utility to improve health outcomes Received March Accepted August Published online September References Esteva et guide to deep learning in healthcare Nat Med Esteva et Deep medical computer vision NPJ Digit Med Rajpurkar Chen Banerjee Topol AI in health and medicine Nat Med Karczewski J Snyder Integrative omics for health and disease Nat Rev Genet Sidransky Emerging molecular markers of cancer Nat Rev Cancer Parsons et An integrated genomic analysis of human glioblastoma multiforme Science Food and Drug Administration List of cleared or approved companion diagnostic devices in vitro and imaging tools https Food and Drug Administration Nucleic tests https Foundation Medicine Why comprehensive genomic profiling https profiling NATuRE MEdIcINE VOL SEPTEMBER Review ARticleNaTuRe MedIcINe Oncotype IQ Oncotype MAP tissue test https Heitzer Haque Roberts Speicher Current and future perspectives of liquid biopsies in oncology Nat Rev Genet Uffelmann et association studies Nat Rev Methods Primers Watanabe et global overview of pleiotropy and genetic architecture in complex traits Nat Genet Choi Mak Reilly Tutorial guide to performing polygenic risk score analyses Nat Protoc Damask et Patients with high polygenic risk scores for coronary artery disease may receive greater clinical benefit from alirocumab treatment in the ODYSSEY OUTCOMES trial Circulation Marston et Predicting benefit from evolocumab therapy in patients with atherosclerotic disease using genetic risk score results from the FOURIER trial Circulation Duan et Evaluation and comparison of data integration methods for cancer subtyping PLoS Comput Biol Zhang Zeman Tsiligkaridis Zitnik network for irregularly sampled multivariate time series In International Conference on Learning Representation ICLR Thorlund Dron Park Mills J Synthetic and external controls in clinical primer for researchers Clin Epidemiol Food and Drug Administration FDA approves first treatment for form of Batten disease https Food specific Food and Drug Administration evidence https AbbVie Synthetic control arm the end of placebos https Generating synthetic control subjects using machine learning for clinical trials in Alzheimer s disease DIA https Noah et Impact of remote patient monitoring on clinical outcomes an updated of randomized controlled trials NPJ Digit Med Kang Ko Mersha B roadmap for data Strain et physical activity and future integration using deep learning Brief Bioinform Wang et MOGONET integrates data using graph convolutional networks allowing patient classification and biomarker identification Nat Commun health risk Nat Med Iqbal Mahgoub Du Leavitt Asghar Advances in healthcare wearable devices NPJ Flex Electron Mandel Kreda Mandl Kohane Ramoni B Zhang Liang Liu Tang Graph neural networks and their current applications in bioinformatics Front Genet SMART on FHIR interoperable apps platform for electronic health records J Am Med Inform Assoc Haque Milstein Illuminating the dark spaces of Moon et Visualizing structure and transitions in healthcare with ambient intelligence Nature biological data Nat Biotechnol Kuchroo et Multiscale PHATE identifies multimodal signatures of Nat Biotechnol https Boehm Khosravi Vanguri Gao J Shah Harnessing multimodal data integration to advance precision oncology Nat Rev Cancer Marx Method of the year spatially resolved transcriptomics Nat Methods et Integrating spatial gene expression and breast tumour morphology via deep learning Nat Biomed Eng Bergenstråhle et spatial transcriptomics by deep data fusion Nat Biotechnol https Janssens Validity of polygenic risk scores are we measuring what we think we are Hum Mol Genet Kellogg Dunn J Snyder Personal omics for precision health Circ Res Owen et Rapid diagnosis of thiamine metabolism dysfunction syndrome Engl Med Moore Zhang Anderson Alexander Estimated costs of pivotal trials for novel therapeutic agents approved by the US food and drug administration JAMA Intern Med Sertkaya Wong Jessup Beleche Key cost drivers of pharmaceutical clinical trials in the United States Clin Trials Loree et Disparity of race reporting and representation in clinical trials leading to cancer drug approvals from to JAMA Oncol Steinhubl Nilsen Iturriaga Califf Digital clinical trials creating vision for the future NPJ Digit Med Inan et Digitizing clinical trials NPJ Digit Med Dunn et Wearable sensors enable personalized predictions of clinical laboratory measurements Nat Med Kwolek B Kepski Human fall detection on embedded platform using depth maps and wireless accelerometer Comput Methods Prog Biomed Wang et Multimodal gait analysis based on wearable inertial and microphone sensors In IEEE SmartWorld Ubiquitous Intelligence Computing Advanced Trusted Computed Scalable Computing Communications Cloud Big Data Computing Internet of People and Smart City Innovation Luo et Computer descriptive analytics of seniors daily activities for health monitoring In Proc Machine Learning Research Vol PMLR Coffey et Implementation of multisite interdisciplinary remote patient monitoring program for ambulatory management of patients with NPJ Digit Med Whitelaw Mamas Topol Van Spall Applications of digital technology in pandemic planning and response Lancet Digit Health Wu Leung Leung Nowcasting and forecasting the potential domestic and international spread of the outbreak originating in Wuhan China modelling study Lancet Jason Wang Ng Brook Response to in Taiwan big data analytics new technology and proactive testing JAMA Radin Wineinger Topol J Steinhubl Harnessing wearable device data to improve surveillance of illness in the USA study Lancet Digit Health Quer et Wearable sensor data and symptoms for detection Nat Med Syrowatka et Leveraging artificial intelligence for pandemic preparedness and response scoping review to identify key use cases NPJ Digit Med Marra Chen Coravos Stern Quantifying the use of Varghese B Thampi multimodal deep fusion graph connected digital products in clinical research NPJ Digit Med Steinhubl et Effect of wearable continuous ECG framework to detect social distancing violations and FCGs in pandemic surveillance Eng Appl Artif Intell monitoring patch on detection of undiagnosed atrial fibrillation the mSToPS randomized clinical trial JAMA San The digital twin revolution Nat Comput Sci Björnsson et Digital twins to personalize medicine Genome Med Pandit J Radin Quer Topol Smartphone apps in the pandemic Nat Biotechnol Kamel Boulos Zhang Digital twins from personalised medicine Pallmann et Adaptive designs in clinical trials why use them and to precision public health Pers Med how to run and report them BMC Med et Digital twins for predictive oncology will be Klarin Natarajan Clinical utility of polygenic risk scores for coronary artery disease Nat Rev Cardiol https Lim Arık Loeff Pfister Temporal fusion transformers for interpretable time series forecasting Int J Forecast paradigm shift for precision cancer care Nat Med Coorey Figtree Fletcher Redfern J The health digital twin advancing precision cardiovascular medicine Nat Rev Cardiol Masison et modular computational framework for medical digital twins Proc Natl Acad Sci USA NATuRE MEdIcINE VOL SEPTEMBER Review ARticleNaTuRe MedIcINe Fisher Smith Walsh Machine learning for Jabbour Fouhey Kazerooni Wiens J Sjoding comprehensive forecasting of Alzheimer s disease progression Sci Walsh et Generating digital twins with multiple sclerosis using probabilistic neural networks Preprint at https Swedish Digital Twin Consortium https accessed February Potter et Development of CancerLinQ health information learning platform from multiple electronic health record systems to support improved quality of care JCO Clin Cancer Inform Parmar Ryu Pandya Sedoc J Agarwal conversational agents in care review of apps NPJ Digit Med Combining chest and electronic health record data using machine learning to diagnose acute respiratory failure J Am Med Inform Assoc Golbus Pescatore Nallamothu Shah Kheterpal Wearable device signals and home blood pressure data across age sex race ethnicity and clinical phenotypes in the Michigan Predictive Activity Clinical Trajectories in Health MIPACT study prospective based observational study Lancet Digit Health Addington et North American Prodrome Longitudinal Study NAPLS overview and recruitment Schizophr Res Perkins et Towards psychosis risk blood diagnostic for persons experiencing symptoms preliminary results from the NAPLS project Schizophr Bull Dixon et virtual type diabetes clinic using continuous Koutsouleris et Multimodal machine learning workflows for glucose monitoring and endocrinology visits Diabetes Sci Technol Claxton et Identifying acute exacerbations of chronic obstructive pulmonary disease using symptoms and cough feature analysis NPJ Digit Med Topol J medicine the convergence of human and artificial intelligence Nat Med Patel Volpp Asch Nudge units to improve the delivery of health care Engl Med Roller et Recipes for building an Chatbot In Proc Conference of the European Chapter of the Association for Computational Linguistics Association for Computational Linguistics Chen Asch Machine learning and prediction in medicine beyond the peak of inflated expectations Engl Med Bycroft et The UK Biobank resource with deep phenotyping and genomic data Nature Woodfield Grant UK Biobank Stroke Outcomes Group UK Biobank and Outcomes Working Group Sudlow Accuracy of electronic health record data for identifying stroke cases in epidemiological studies systematic review from the UK biobank stroke outcomes group PLoS ONE Szustakowski et Advancing human genetics research and drug discovery through exome sequencing of the UK Biobank Nat Genet Halldorsson et The sequences of genomes in the UK Biobank Nature et The UK Biobank imaging enhancement of participants rationale data collection management and future directions Nat Commun Chen et China Kadoorie Biobank of million people survey methods baseline characteristics and Int Epidemiol Nagai et Overview of the BioBank Japan Project study design and profile Epidemiol prediction of psychosis in patients with clinical syndromes and depression JAMA Psychiatry Baltrusaitis Ahuja Morency Multimodal machine learning survey and taxonomy IEEE Trans Pattern Anal Mach Intell Radford et Learning transferable visual models from natural language supervision In Proc International Conference on Machine Learning eds Meila Zhang vol PMLR July Zhang Jiang Miura Manning Langlotz Contrastive learning of medical visual representations from paired images and text Preprint at https Zhou et Generalized radiograph representation learning via between images and radiology reports Nat Mach Intell Akbari et VATT transformers for multimodal learning from raw video audio and text In Advances in Neural Information Processing Systems eds Ranzato et vol Curran Associates Bao et VLMo unified with Preprint at https Dean J Introducing Pathways AI architecture https November Vaswani et Attention is all you need In Advances in Neural Information Processing Systems eds Guyon et vol Curran Associates Dosovitskiy et An image is worth words transformers for image recognition at scale In International Conference on Learning Representations ICLR Li et Oscar aligned for tasks Preprint at https Baevski et general framework for learning in speech vision and language Preprint at https Gaziano et Million Veteran Program to study Tamkin et DABS Benchmark for genetic influences on health and disease Clin Epidemiol Learning In Information Processing Systems Datasets and Benchmarks Track Taliun et Sequencing of diverse genomes from the NHLBI Jaegle et Perceiver general perception with iterative attention In TOPMed Program Nature All of Us Research Program Investigators et The All of Us Research Program Engl Med Mapes et Diversity and inclusion for the All of Us research program scoping review PLoS ONE Proc International Conference on Machine Learning eds Meila Zhang vol PMLR July Jaegle et Perceiver IO general architecture for structured inputs outputs In International Conference on Learning Representations ICLR Kaushal Altman Langlotz Geographic distribution of US cohorts Hendricks Mellor Schneider Alayrac Nematzadeh used to train deep learning algorithms JAMA Arges et The Project Baseline Health Study step towards broader mission to map human health NPJ Digit Med McDonald et American Gut an open platform for citizen science microbiome research mSystems Johnson et freely accessible critical care database Sci Data Johnson et publicly available database of chest radiographs with reports Sci Data Deasy Liò Ercole Dynamic survival prediction in intensive care units from heterogeneous time series without the need for variable selection or curation Sci Decoupling the role of data attention and losses in multimodal transformers Trans Assoc Comput Linguist Lu Grover Abbeel Mordatch Pretrained transformers as universal computation engines Preprint at https Sandfort Yan Pickhardt J Summers Data augmentation using generative adversarial networks CycleGAN to improve generalizability in CT segmentation tasks Sci Bai et Advancing diagnosis with collaboration in artificial intelligence Nat Mach Intell Berisha et Digital medicine and the curse of dimensionality NPJ Digit Med Barbieri et Benchmarking deep learning architectures for predicting readmission to the ICU and describing Sci Huang Pareek Zamanian Banerjee I Lungren Guu Lee Tung Pasupat Chang Retrieval augmented language model In Proc International Conference on Machine Learning eds Iii Singh vol PMLR July Multimodal fusion with deep neural networks for leveraging CT imaging and electronic health record in pulmonary embolism detection Sci Borgeaud et Improving language models by retrieving from trillions of tokens In Proc International Conference on Machine Learning eds Chaudhuri et vol PMLR July NATuRE MEdIcINE VOL SEPTEMBER Review ARticleNaTuRe MedIcINe Huang Pareek Seyyedi Banerjee I Lungren Fusion Vokinger Feuerriegel Kesselheim Mitigating bias in of medical imaging and electronic health records using deep learning systematic review and implementation guidelines NPJ Digit Med Muhammad et comprehensive survey on multimodal medical signals fusion for smart healthcare systems Inf Fusion Fiterau et ShortFuse Biomedical time series representations in the presence of structured information In Proc Machine Learning for Healthcare Conference eds et vol PMLR August Tomašev et clinically applicable approach to continuous prediction of future acute kidney injury Nature Rajpurkar et CheXaid deep learning assistance for physician diagnosis of tuberculosis using chest in patients with HIV NPJ Digit Med Kihara et multimodal deep learning for predicting visual fields from the optic disc and optical coherence tomography imaging Ophthalmology https Ramesh et generation In Proc International Conference on Machine Learning eds Meila Zhang vol PMLR July machine learning for medicine Commun Med Obermeyer Powers Vogeli Mullainathan Dissecting racial bias in an algorithm used to manage the health of populations Science Gichoya et AI recognition of patient race in medical imaging modelling study Lancet Digit Health Swanson The UK Biobank and selection bias Lancet Griffith et Collider bias undermines our understanding of disease risk and severity Nat Commun Thompson et The influence of selection bias on identifying an association between allergy medication use and infection EClinicalMedicine Fry et Comparison of sociodemographic and characteristics of UK biobank participants with those of the general population Am Epidemiol Keyes Westreich UK Biobank big data and the consequences of Lancet Narayanan Shmatikov Robust of large sparse datasets In IEEE Symposium on Security and Privacy Gerke Minssen Cohen Ethical and legal challenges of artificial Nichol et GLIDE towards photorealistic image generation and healthcare Artif Intelli Health editing with diffusion models In Proc International Conference on Machine Learning eds Chaudhuri et vol PMLR July Kaissis Makowski Rückert Braren Secure and federated machine learning in medical imaging Nat Mach Intell Reed et generalist agent Preprint at https Rieke et The future of digital health with federated learning NPJ Digit Med Li et Align before fuse vision and language representation learning with momentum distillation Preprint at https Ziller et Medical imaging deep learning with differential privacy Sci Dayan et Federated learning for predicting clinical outcomes in Nagrani et Attention bottlenecks for multimodal fusion In Advances patients with Nat Med in Neural Information Processing Systems eds Ranzato et vol Curran Associates Hughes et Deep learning evaluation of biomarkers from Wood Najarian Kahrobaei Homomorphic encryption for machine learning in medicine and bioinformatics ACM Comput Surv echocardiogram videos EBioMedicine et Swarm learning for decentralized and Echle et Deep learning in cancer pathology new generation of confidential clinical machine learning Nature clinical biomarkers Br J Cancer Zhou et Edge intelligence paving the last mile of artificial intelligence Shilo Rossman Segal Axes of revolution challenges and promises of big data in healthcare Nat Med Hripcsak et Observational Health Data Sciences and Informatics OHDSI opportunities for observational researchers Stud Health Technol Inform with edge computing Proc IEEE Intel How edge computing is driving advancements in healthcare analytics https March Ballantyne How should we think about clinical data ownership Med Rannikmäe et Accuracy of identifying incident stroke cases from linked health care data in UK Biobank Neurology Garg Oh Naidech Kording Prabhakaran Automating ischemic stroke subtype classification using machine learning and natural language processing Stroke Cerebrovasc Dis Casey et and RDoC progress in psychiatry research Nat Rev Neurosci Ethics Liddell Simon Lucassen Patient data ownership who owns your health J Law Biosci Bierer Crosas Pierce Data authorship as an incentive to data sharing Engl Med Scheibner et Revolutionizing medical data sharing using advanced technologies technical legal and ethical synthesis Med Internet Res Sirugo Williams Tishkoff The missing diversity in Vamathevan et Applications of machine learning in drug discovery human genetic studies Cell and development Nat Rev Drug Discov Zou J Schiebinger Ensuring that biomedical AI benefits diverse populations EBioMedicine Rocher Hendrickx Montjoye Estimating the success of in incomplete datasets using generative models Nat Commun Haneuse Arterburn Daniels Assessing missing data assumptions in studies complex and underappreciated task JAMA Netw Open van Smeden Penning Vries B Nab Groenwold Approaches to addressing missing values measurement error and confounding in epidemiologic studies Clin Epidemiol Genomes Project Consortium et global reference for human genetic variation Nature Consortium et The project identifies rare variants in health and disease Nature Acknowledgements We thank Tamkin for invaluable feedback NIH grant to supported this work competing interests Since completing this Review became an employee of Rad AI All the other authors declare competing interests Additional information Correspondence should be addressed to Pranav Rajpurkar or Eric Topol Peer review information Nature Medicine thanks Joseph Ledsam Leo Anthony Celi and Jenna Wiens for their contribution to the peer review of this work Primary Handling Editor Karen Leary in collaboration with the Nature Medicine team McCarthy et reference panel of haplotypes for genotype Reprints and permissions information is available at imputation Nat Genet Li et Imputation of missing values for electronic health record laboratory data NPJ Digit Med Tang et Democratizing EHR analyses with FIDDLE flexible preprocessing pipeline for structured clinical data J Am Med Inform Assoc Che et Recurrent neural networks for multivariate time series with Publisher s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations Springer Nature or its licensor holds exclusive rights to this article under publishing agreement with the author s or other rightsholder s author of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law missing values Sci Springer Nature America NATuRE MEdIcINE VOL SEPTEMBER Review ARticleNaTuRe MedIcINe,,[]
OPEN ARTICLE Integrated multimodal artiﬁcial intelligence framework for healthcare applications Luis Soenksen Yu Cynthia Leonard Kimberly Villalobos Liangyuan Na Holly Wiberg Michael Ignacio and Dimitris Artiﬁcial intelligence AI systems hold great promise to improve healthcare over the next decades Speciﬁcally AI systems leveraging multiple data sources and input modalities are poised to become viable method to deliver more accurate results and deployable pipelines across wide range of applications In this work we propose and evaluate uniﬁed Holistic AI in Medicine HAIM framework to facilitate the generation and testing of AI systems that leverage multimodal inputs Our approach uses generalizable data and machine learning modeling stages that can be readily adapted for research and deployment in healthcare environments We evaluate our HAIM framework by training and characterizing independent models based on multimodal clinical database N samples containing unique hospitalizations and patients spanning all possible input combinations of data modalities tabular text and images unique data sources and predictive tasks We show that this framework can consistently and robustly produce models that outperform similar approaches across various healthcare demonstrations by including distinct chest pathology diagnoses along with and h mortality predictions We also quantify the contribution of each modality and data source using Shapley values which demonstrates the heterogeneity in data modality importance and the necessity of multimodal inputs across different tasks The generalizable properties and ﬂexibility of our Holistic AI in Medicine HAIM framework could offer promising pathway for future multimodal predictive systems in clinical and operational healthcare settings npj Digital Medicine https INTRODUCTION Artiﬁcial intelligence AI and machine learning ML systems are poised to become fundamental tools in clinical practice and healthcare Such anticipated utility particularly in systems aimed to improve clinical efﬁciency and patient outcomes will require knowledge from multiple data sources and various input Multimodal architectures for systems are attractive because they can emulate the input conditions that clinicians and healthcare administrators currently use to perform predictions and respond to their complex typical clinical practice uses diverse set of information formats contained within the patient electronic health record EHR such as tabular data age demographics procedures history billing codes image data photographs scans magnetic resonance imaging pathology slides data intermittent pulse oximetry blood chemistry respiratory analysis electrocardiograms tests wearable sensors structured sequence data genomics proteomics lomics and unstructured sequence data notes forms written reports voice recordings video among other Recently models leveraging multiple data modalities have been demonstrated for the domains of public and healthcare operational analytics mortality and discharge predictions Furthermore it been shown that modality in most of these domains can increase the performance of systems accuracy compared to modality approaches for the same However developing uniﬁed and scalable pipelines that can consistently be applied to train multimodal systems that leverage and outperform their counterparts remained the development of our Holistic Artiﬁcial This motivates Intelligence in Medicine HAIM framework modular ML pipeline that can be adapted to receive standard EHR information from multiple input data modalities tabular data images and text Our HAIM framework addressed the need for more generalizable methodology to create this class of systems It can leverage extraction models as part of uniﬁed processing and feature aggregation stage that allows for simple and scalable stream modeling of variety of clinically relevant predictive tasks Based on this pipeline we build and test thousands of classiﬁcation models with sample EHR inputs to systematically investigate the value of adding individual data modalities to these systems To our knowledge this not yet been analyzed to greater detail in prior clinical multimodal tions We provide this work as an codebase for clinicians and researchers in the hope it will allow them to train and test systems more easily with the local datasets trained feature extractors and clinical questions of their choosing to fully leverage multimodality at their institutions Latif Jameel Clinic for Machine Learning in Health MIT Cambridge MA USA Institute for Biologically Inspired Engineering Harvard University Boston MA USA Research Center Massachusetts Institute of Technology MIT Cambridge MA USA School of Management MIT Cambridge MA USA authors contributed equally Luis Soenksen Yu Ma Cynthia Zeng Leonard Boussioux Kimberly Villalobos Carballo Liangyuan Na email dbertsim Published in partnership with Seoul National University Bundang Hospital Soenksen et RESULTS Demonstration of HAIM framework on multimodal clinical dataset We demonstrate the feasibility and versatility of the HAIM framework on compiled multimodal dataset involving which includes total of samples hospitalization stays and unique patients We summarize the general characteristics of number of samples and features in Table Qualitatively our HAIM work appears to improve on previous work in this by npj Digital Medicine Published in partnership with Seoul National University Bundang Hospital Soenksen et Fig Holistic Artiﬁcial Intelligence in Medicine HAIM framework Under this framework databases and tables sourced from speciﬁc healthcare institutions such as combined from and for this work are processed to generate individual patient ﬁles These ﬁles contain past and present multimodal patient information from the moment of admission For processing under the HAIM framework every data modality is fed to independent embedding generating streams In this work tabular data is minimally processed using simple transformations or normalizations to produce encodings or categorical numerical values ETabular n t where n unique and t sampling time Selected are processed by generating statistical metrics on each of the signals to produce embeddings representative of their trends ETimeSeries n t from the moment of admission until the sampling time Natural language inputs such as notes are processed using transformer neural network to generate text embeddings of ﬁxed size EText n t Furthermore image inputs such as are processed using convolutional neural network to also extract embeddings out of the model output probability vectors and dense features EImages n t While not done in this work thanks to the modularity of the embedding extraction process in the HAIM framework other models or systems could be added to generate embeddings from other types of data sources if needed EOther n t All generated embeddings are concatenated to generate fusion embedding which can be used to train test and deploy models for predictive analytics in healthcare operations For this work we tested and utilized only XGBoost as canonical type of architecture for building the downstream predictive models based on fusion embeddings CNN Convolutional Neural Network CT Computerized Tomography ECG Electrocardiogram ECO Echocardiogram MRI Magnetic Resonance Imaging NN Neural Network Oxygen ReLU Rectiﬁed Linear Unit RNN Recurrent Neural Network US Ultrasound Table General characteristics of the database Characteristic Samples Demographic Variables Chart Event Variables Laboratory Event Variables Procedure Event Variables Variables Text Note Variables Consolidation ΔAUROC ΔAUROC Pneumonia ΔAUROC Atelectasis ΔAUROC Lung Opacity ΔAUROC Pneumothorax ΔAUROC Edema ΔAUROC and Cardiomegaly ΔAUROC Furthermore the average percent improvement of all multimodal HAIM predictive systems is across all evaluated tasks Fig All results displayed in Figs and b are grouped and ordered by number of modalities range encompassing tabular text and images number of data sources range including each individual data source in and sample size N for ease of analysis is combination of and MIMIC Chest ﬁltered to only include patients that have at least one chest performed with the goal of validating multimodal predictive analytics in healthcare operations The number of samples and quantities of variables are described Demographic features correspond only to tabular data modality while chart laboratory and procedure events correspond to variables correspond to types of medical images while text note variables correspond to the test in radiology electrocardiogram and echocardiogram natural language reports including scalable data and enabling standardized feature extraction stages that allow for rapid prototyping testing and deployment of predictive models based on prediction targets Our HAIM framework displays consistent improvement on average AUROC Fig color gradient across all models as the number of modalities and data sources increases Furthermore the trend of reducing AUROC standard deviation SD values also appears to follow from increasing the number of modalities and data sources Fig greyscale gradient We also report Receiver Operating istic ROC curves for the best found predictive models Fig as compared with typical multimodal predictive models based on the HAIM framework Fig All individual model AUROCs for chest diagnosis prediction tasks for and mortality prediction are shown along with their respective SDs in Supplementary Fig These results suggest that our HAIM framework can consistently improve predictive analytics for various applications in healthcare as compared with analytics Quantitatively Fig b shows that our HAIM framework produces models with source and multimodality input combinations that improve from average performance of canonical and by extension systems for chest pathology prediction ΔAUROC ΔAUROC and h mortality ΔAUROC Speciﬁcally for chest pathology prediction the minimum per task improvements include Fracture ΔAUROC Lung Lesion ΔAUROC Enlarged Cardio mediastinum Analysis of source and multimodality contributions on model performances To understand how each data source and modality contributes to the ﬁnal performance we calculate Shapley of each of the sources and modalities as it contributes to the ﬁnal AUROC performance Since our demonstrated tive tasks are treated as binary classiﬁcation problems we assumed that the AUROC of model with data source is and the AUROC of the model of particular modality is the average AUROC of the models of all sources that belong to such modality Aggregated Shapley values for all data modalities per predictive task are reported in Fig while Shapley values for all data sources per predictive task are shown in Supplementary Fig Different tasks exhibit distinct distributions of gated Shapley values across data modalities and sources In particular we observe that vision data contributed most to the model performance for the chest pathology diagnosis tasks but for predicting and h mortality the patient s historical records appeared to be the most relevant Shapley values also provide way to monitor errors and information loss propagation during the feature extraction and model training phases of our HAIM framework Data modalities or negative Shapley values indicate associated with small either an absence of extracted information or error propagation leading to detrimental local effects on downstream model performance Fig and Supplementary Fig This situation can be potentially addressed by removing such input data modalities or by selecting different feature tion models speciﬁc to that data modality Nevertheless we see in our speciﬁc sample that across all tasks demonstrations every contributes positively to monotonic trend with diminishing returns on the predictive capacity of the models Fig and c likely due to multimodal data redundancy These observations attest to the potential value and limitations of using multimodal inputs and trained feature extraction modules in frameworks like HAIM which could be used to generate predictive models for diverse clinical tasks more than previous strategies Published in partnership with Seoul National University Bundang Hospital npj Digital Medicine Soenksen et Fig Performance of the multimodal HAIM framework on various demonstrations for healthcare operations Average and standard deviation values of the area under the receiver operating characteristic AUROC for all demonstrations including pathology diagnosis lung lesions fractures atelectasis lung opacities pneumothorax enlarged cardio mediastinum cardiomegaly pneumonia consolidation and edema as well as and h mortality prediction The number of modalities refers to the coverage among tabular text and image data The number of sources refers to the coverage among available input data sources for pathology diagnosis while for and h mortality prediction Thus the position Modality Sources corresponds to the average AUROC of all models across all input combinations covering any modalities using any input sources Increasing gradients on average AUROC appear to follow from increasing the number of modalities and number of sources across all evaluated tasks Decreasing gradients on AUROC standard deviations follow from less variability in performance as higher number of modalities and data sources is used b Receiver operating characteristic ROC curves for typical HAIM model across all use cases exhibiting input multimodal c ROC curves for model with inputs across the same use cases Consistent averaged improvements across all tasks are observed in multimodality as compared to systems AUROC Area under the curve AUROC Area under the receiver operating characteristic curve CM Cardiomediastinum Dx Diagnosis HAIM Holistic Artiﬁcial Intelligence in Medicine Ops Operations SD Standard deviation schematic of the complete HAIM pipeline for training and evaluation of models throughout this work is described in Fig The general process of database preparation as well as embedding extraction and fusion that serves as input for this pipeline can be found in Fig DISCUSSION Inferring latent features from rich and heterogeneous multimodal EHR information could provide clinicians administrators and researchers with unprecedented opportunities to develop better pathology detection systems actionable healthcare analytics and recommendation engines for precision medicine Our results directly illustrated that different data modes are more useful for different tasks and thus multimodal approach to construct comprehensive pipeline for in healthcare In addition to leveraging multimodal inputs our HAIM framework attempts to solve several bottleneck challenges in this kind of pipeline for healthcare in more uniﬁed and robust way than previous implementations including the possibility of working with tabular and data of unknown sparsity from multiple standardized and unstandardized heterogeneous data formats The use of fusion embeddings obtained directly from individual patient ﬁles suggests that HAIM framework can potentially facilitate the deﬁnition testing and deployment of models that may be useful for managing complex clinical situations and practice in healthcare systems More speciﬁcally if implemented across many predictive tasks while using the same patient embeddings this approach could potentially help erate the advent of scalable predictive systems to improve patient outcomes and quality of care From these observations our work distinguishes itself from previously published systems in three main ways First our work systematically investigates the value of progressively adding data modalities and sources to clinical multimodal systems in much greater detail and larger combinatorial input space than any prior investigation of such class systems Previous works in this ﬁeld assume advantageous properties to multimodality without clear validation of the such expected performance beneﬁts as data dynamics of modalities are added Through our investigation by conducting model experiments with different input modalities and data source combinations we provide strong empirical evidence that supports the potential for reaching such positive monotonic trends in performance from multimodal systems as data modalities are added However our investigation also unveils previously unreported local and diminishing return effects on the predictive capacity of these models under certain conditions of data source availability error and dancy which are relevant and can become interpretable through our use of aggregated Shapley values during analysis B Second our data and modeling pipeline expands on the notion of high modularity from previously published work that tend to employ multimodal architectures trained directly on fused data inputs which are usually closed less compatible npj Digital Medicine Published in partnership with Seoul National University Bundang Hospital Soenksen et Fig Multimodal HAIM framework is ﬂexible and robust method to improve predictive capacity for healthcare machine learning systems as compared to approaches Average percent change of area under the receiver operating characteristic curve Avg ΔAUROC for all tested multimodality HAIM models as compared to their counterparts While different models exhibit varying degrees of improvement all tested models show positive Avg ΔAUROC percentages The number of modalities refers to the coverage among tabular text and image data The number of sources refers to the coverage among available input data sources for pathology diagnosis for and h mortality prediction Thus the position Modality Sources corresponds to the average AUROC of all models across all input combinations covering any modalities using any input sources b Expanded Avg ΔAUROC percentages for all tested multimodality HAIM models and ordered by the number of used modalities tabular text or images as well as the number of used data sources c Waterfall plots of aggregated Shapley values for independent data modalities per predictive task While Shapley values for all data modalities appear to be positively contributing to the predictive capacity of all models different tasks exhibit distinct distributions of aggregated Shapley values d schematic of the HAIM pipeline developed to support the presented work After data collection or sourcing for this work process of feature selection and embedding extraction is applied to feed fusion embeddings into process of iterative architecture engineering model and hyperparameter selection After particular models are selected and trained they can be benchmarked to test and report results This process concludes by the selection of model for deployment in use case scenario with other datasets and modeling changes across users Instead our approach leverages externally validated models as feature extractors to create uniﬁed vector representations of patient ﬁles that allow for much simpler downstream modeling of target variables Furthermore framework enables and encourages users to update selected feature extractors more easily with new SOTA or more advantageous methods as the community develops them without requiring to other feature extractors C Finally our work demonstrates one of the highest numbers of sources and data modalities used so far in multimodal clinical systems for EHRs including tabular data text and images along with the use of this interpretability techniques such as Shapley values Using gated Shapley values we can quantitatively establish the importance and heterogeneity of different data sources and modalities across large number of experiments in different healthcare tasks Thus we demonstrate the potential of learning from multiple data sources and modalities underscoring the need to collect more holistic patient data that facilitates the application of multimodal ML in the healthcare domain Our system is also provided as an codebase to allow clinicians and researchers to train and test their own multimodal systems more easily with local datasets feature extractors and their own clinical questions While our systematic evaluation of the Published in partnership with Seoul National University Bundang Hospital npj Digital Medicine Soenksen et effects of multiple data modality additions to our work was based on the dataset this input was only used to exemplify our pipeline and to provide strong empirical evidence on the dynamics of performance from the use of different data modalities in canonical HER scenario The downstream trained models generated for this investigation could potentially be used in the future by people interested in predicting the demonstrated clinical tasks within intensive care units ICUs using multimodal data However we primarily encourage users to use our codebase to process their own EHR datasets and train predictive tasks of interest to them with the help of our pipeline We envision broad utility for the HAIM framework and its subprocesses focusing on driving activities for clinical and operations We hope that our HAIM framework can help reduce the time required to develop relevant systems while efﬁciently utilizing human ﬁnancial and digital resources in more timely and uniﬁed approach than the current methods used in healthcare organizations METHODS Dataset For this work we utilize the Medical Information Cart for Intensive Care MIMIC an openly accessible database that contains records of individual patients admitted to the ICU or emergency department ED of Beth Israel Deaconess Medical Center BIDMC in Boston MA USA between and inclusive s most recent version improves on to provide public access to the EHR data of over hospitalized patients based on the BIDMC s MetaVision clinical information system We selected due to its detailed documentation generalizable formatting corroborated use in and prior evaluations in terms of fairness and To augment BIDMC s ML interpretability we used the MIMIC Chest CXR database containing radiology images with imaging events that can reports representing medical be matched to corresponding patients included in Both databases have been independently by deleting all personal health information following the US Health Insurance Portability and Accountability Act of Safe Harbor requirements After getting credentialled access from PhysioNet we combined and into uniﬁed multimodal dataset based on matched patient admission and identiﬁers from and databases We used throughout this study to test all the presented ML use cases analyzing various combinations of structured patient information data medical images and unstructured text notes as presented in the following sections data representation We generated the individual ﬁles containing information for single hospital admissions by querying the aggregated multimodal dataset Every EHR ﬁle contains the details of current and previous patient admissions transfers demographics laboratory measurements provider orders microbiology cultures medication intravenous and ﬂuid tions prescriptions procedure events inputs sensor outputs measurement events radiological images radiological reports electrocardiogram reports echocardiogram reports notes hospital billing information diagnosis and codes as well as other and charted information The samples therefore include all available patient data collected within speciﬁc admission and stay with all prior information occurring before the discharge or death time stamp We stored all the individual patient ﬁles in MM as pickle object structures for ease of processing in subsequent sampling and modeling tasks The code to generate the aggregated dataset from credentialled access to and datasets is available at our PhysioNet repository https as well as our GitHub repository https samples of processed pickle patient ﬁles of can be found in our PhysioNet project page https schematic of this data representation as multimodal input for our HAIM framework is shown in Fig In addition Patient data processing and multimodal feature extraction We processed each patient ﬁle individually to generate vector embeddings for each of the possible input types including all patient information from the time of admission until the selected inference event time of imaging procedure for pathology diagnosis or for h mortality predictions The generated embeddings from input modalities include tabular data such as demographics Ede demographics structured events Ece chart events Ele laboratory events Epe procedure events unstructured free text Eradn radiological notes Eecgn electrocardiogram notes Eecon cardiogram notes vision Evp visual probabilities Evd visual features and vision Evmp aggregated visual probabilities Evmd aggregated visual layer features From these patient signals used as for embedding extraction classiﬁed by type of event can be found in Supplementary Table We then implemented ﬁxed embedding extraction procedures based on standard data modalities tabular data to reduce its dependence on data architectures and allow for consistent embedding format that may be applied to arbitrary ML pipelines Note that throughout this work we refer to data modality as distinct term to data source where the former is used to deﬁne broad classes of data usually digitalized in different format types while the latter simply refers to different input variables belonging to data modality as deﬁned in tary Table text and images We extracted the embeddings based solely on tabulated demographics data Ede by querying normalized numerical values from the patient record We obtained embeddings using data from the structured patient chart laboratory and procedure event lists Ec Ele Epe respectively We selected set of key clinical signals for each type of event list and constructed the corresponding time sequences from the time of patient admission to the allowable for each individual feature see Supplementary Table The embeddings encode the signal length maximum minimum mean median SD variance number of peaks and average slope and change over time of these metrics The signals for Ece include heart rate HR systolic blood pressure NBPs diastolic blood pressure NBPd respiratory rate oxygen saturation by pulse oximetry Glasgow coma scales GCS for verbal eye and motor response GCSV GCSE GCSM respectively Moreover Ele include glucose potassium sodium chloride creatinine urea nitrogen bicarbonate anion gap hemoglobin hematocrit magnesium platelet count phosphate white blood cells total calcium mean corpuscular hemoglobin MCH red blood cells mean corpuscular hemoglobin concentration mean corpuscular volume red blood cell distribution width platelet count neutrophils vancomycin Lastly Epe procedures include foley catheter erally inserted central catheter intubation peritoneal dialysis bronchoscopy electroencephalogram EEG dialysis with PICC npj Digital Medicine Published in partnership with Seoul National University Bundang Hospital continuous renal replacement therapy dialysis with catheter removed chest tubes and hemodialysis We obtained embeddings for the unstructured free text Eradn Eecgn and Eecon by concatenating all available text from each of these types of notes as continuous strings and then by processing them using Clinical bidirectional encoder model on large corpus of biomedical and medical text This model generates single vector or embedding per unstructured text type We split notes longer than the maximum input token size for Clinical BERT tokens into the smallest number of processable text chunks to generate various embeddings tially all of which are averaged to produce single dimensional output embedding for the entire text Finally we processed vision data included in this work using convolutional neural network CNN previously on the CheXpert We selected this model because the availability of at least one chest per patient ﬁle within the database as its core visual component is part of TorchXRayVision uniﬁed library and repository of datasets and SOTA models for chest pathology classiﬁcation using While other computer vision models on large sets of medical imaging data may be utilized to extract embeddings within the HAIM framework for the purpose of experimentally validating our pipeline we used as canonical method to extract visual embeddings We obtained the embeddings per patient ﬁle by rescaling each image into size using standard interpolation method with resampling using pixel area relations and then feeding it into the selected network to extract output class probabilities and b ﬁnal features The output classes per image are the diagnosis probability vector generated directly by which produces the embedding Evp The dense network features per image are the vector generated by extracting the outputs the model which produces the of embedding Evd embeddings are also obtained by averaging the output class probabilities and feature embeddings of all available images per patient ﬁle studies with multiple planes and past studies This produces an aggregated diagnosis probability embedding Evmp and embedding Evmd per patient that considers all available and not only the most recent one the last dense layer of There are various advantages of using SOTA models speciﬁc to each data modality tabular text and images such as Clinical and as feature extractors in our HAIM framework First every can be and easily SOTA model exchanged with updated ones as long as their respective dense features or embeddings are accessible This departs from other multimodal strategies to directly fuse heterogeneous input data which makes these systems less modular and usually incompatible with the use of performing models produced by other organizations and second advantage of using SOTA feature extractors within our framework is that users can easily generate uniﬁed input vectors to focus primarily on downstream modeling and rapid training of their predictive systems of interest which can accelerate deployment that attempt In our sample demonstration of the HAIM framework using the database the dimensionality of each of these embeddings is Ede Ece Ele Epe Eradn Eecgn Eecon Evp Evd Evmp and Evmd Detail on the presence and handling of missing input data is provided as part of Supplementary Table Once all Soenksen et embeddings are generated we ﬂatten normalize and concatenate them into single multimodal fusion embedding per patient ﬁle which constitutes the input for all downstream modeling tasks in our HAIM framework see Supplementary Fig for algorithmic detail of such process This deep patient representation in vector form can be made of ﬁxed size within or across healthcare institutions rapid iteration in the development of generic ML systems for relevant predictive analytics in various applications this work which can allow for for Modeling After we extracted all multimodal fusion embeddings for all EHR patient ﬁles in the database we generated classiﬁcation models across various clinical and operational tasks including chest pathology diagnosis b and c h mortality predictions For each of these modeling tasks we split the available embeddings randomly into training and testing sets times with different splits stratifying by patients during our experiments to avoid data leakage of level information from training to testing compute SDs and to ensure adequate comparison of recorded predictive values For the chest pathology diagnosis tasks we applied an additional stratiﬁcation by pathology to balance the target ratios We then conducted experiments to compare the effect of all different combinations of input data modalities and sources using the extracted multimodal fusion embeddings as presented in further sections An algorithmic formulation of our HAIM framework in the context of the data processing feature extraction and stream predictive task modeling stages is provided as part of Supplementary Fig Detail on the sensitivity of missing input data to downstream predictions is also provided as part of Supplementary Fig fractures Tasks of interest Chest pathology diagnosis prediction Early detection of certain pathologies in CT scans and other diagnostic imaging modalities enables clinicians to focus on early intervention rather than delayed treatment for advanced stages of relevant pathologies Within this task of interest we chose to target the prediction of lung lesions common pathologies lung enlarged cardio mediastinum consolidation pneumonia opacities atelectasis pneumothorax edema and cardiomegaly that can be typically assessed by radiologists through chest to demonstrate that HAIM outperforms approaches The values for each chest pathology included in are derived from where radiology notes were processed to determine if each of these pathologies was explicitly conﬁrmed as present value explicitly conﬁrmed as absent value inconclusive in the study value or not explored value We only selected samples with or values removing the rest from the training and testing data Thus for this speciﬁc task we utilized the multimodal fusion embeddings as input and the chest pathology values as the output target to predict From these embeddings we only excluded the tured radiology notes component Erad from the allowable input to avoid potential overﬁtting or misrepresentations of real predictive value We trained and tested independent binary classiﬁcation models for each target chest pathology and input source combination as described in the general model training setup section Final sample sizes for each pathology diagnosis task are Fracture N Lung Lesion N Enlarged cardio mediastinum N Consolidation N Pneumonia N Lung opacity N Atelectasis N Pneumothorax N Edema N and galy N Published in partnership with Seoul National University Bundang Hospital npj Digital Medicine Soenksen et prediction Projected patient plays vital role for both patients and hospital systems in making informed medical and economic decisions An accurate forecast of patient stay enhances patient satisfaction hospital resource allocations and doctors ability to make more effective treatment Particularly predicting next h discharges is critical for physicians to identify and prioritize patients ready for to accelerate discharge and for case management discharge preparations which ultimately reduces patient burden and direct operating costs in healthcare To strate the HAIM framework for healthcare operations tasks we predicted whether or not patient will be discharged without expiration during the next h as binary classiﬁcation problem discharged alive h or otherwise In case of patient death we set the class label to Each sample in this predictive task corresponds to single EHR time point where an image was obtained N teams h mortality prediction Due to its time and environments clinicians in ICU units often need to make rapid evaluations of patient conditions to inform treatment However current standards of estimating patient severity such as the Acute Physiologic Assessment and Chronic Health Evaluation score fail to incorporate medical characteristics beyond acute Accurate mortality prediction can give clinicians advanced warnings of possible deteriorations and share the burdens of making To further demonstrate the versatility of the HAIM framework we also built models to predict the probability that patient will expire during the next h as binary classiﬁcation problem expired h or otherwise In the case of patient whose hospital exit status is not expiration we set the class label to It should be noted that patient can acquire different target class labels at different time points during their stay due to changes in status and proximity to the discharge or time of death Similar to the modeling each sample in this predictive task corresponds to single EHR time point where an image was obtained N General model training setup including logistic We initially explored seven ML architectures regression classiﬁcation and regression trees random forest perceptron gradient boosted trees XGBoost gradient boosting machines LightGBM as well as attentive tabular networks TabNet to heuristically decide on the best model choice for experiments Since XGBoost supports fast tions for experiments and consistently outperformed other architectures during preliminary observations we selected this canonical methodology for all further tests Our based modeling experiments were conducted using every possible combination of input embeddings extracted as described in previous sections from the allowable data sources Ede Ece Epe Ele Eecgn Eecon Eradn Evp Evd Evmp and Evmd and modalities tabular text and images In this process we concatenated each data stream permutation to produce fusion embeddings and train XGBoost models using modality and nation of inputs This corresponds to the generation of models per predictive task for the cases of and h mortality As previously mentioned in the case of chest pathology diagnosis the embeddings corresponding to all radiology notes Eradn are not included as part of the input fusion embeddings to allow for fair comparison with the output target which was originally determined from examining notes in This reduced the total number of possible models per chest pathology diagnosis task to Since there are ten chest pathologies deﬁned as binary classiﬁcation problems for our experiments we trained total of models for chest pathology diagnosis prediction As mentioned previously all XGBoost models were trained ﬁve times with ﬁve different data splits to repeat the experiments and compute average metrics and SDs All deﬁned models NModels were trained and tested to evaluate the advantage of multimodal predictive systems based on the HAIM framework as compared to single modality ones for the aforementioned clinical and operational tasks We capture average trends of model performance by reporting the average area under the receiver operating characteristic AUROC curve on over ﬁve consecutive iterations of the testing set randomized data splitting and model training The hyperparameter combinations of individual XGBoost models were selected within each training loop using ﬁvefold grid search on the training set This XGBoost tuning process selected the maximum depth of the trees the number of estimators or and the learning rate according to the parameter value combination leading to the highest observed AUROC within the training loop This model strategy at the level of each data source combination ensures that the respective test sets are never used training model selection model comparison or for model reporting across any of the uniquely trained models Thus throughout this study the test set remains unseen at the level of each model for all models which minimizes the potential for data leakage or model selection overﬁtting The aggregated test set performance metrics ﬁvefold test averages and SDs of all these models grouped by the number of data sources and modalities can be found in Fig We conducted all embedding generation and computational experiments using parallelization strategy under MIT s Supercloud server https with RAM and NVIDIA Tesla instance Volta graphics processing unit per schematic representation of the HAIM framework from data sourcing to model benchmarking can be found in Fig Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this article DATA AVAILABILITY The multimodal dataset in Pickle ﬁle format used for this work can be fully generated by combining the openly accessible https https and using credentialed access and the resources found in our PhysioNet online repository https All the extracted feature embeddings per subject from are also available for download in our PhysioNet online repository CODE AVAILABILITY All the code used to prepare the dataset generate models and evaluate the conclusions of this work can be found in our GitHub repository https as well as the Supplementary Materials of this work Received February Accepted August REFERENCES Topol Deep medicine how artiﬁcial again Hachette UK intelligence can make healthcare human Huang Pareek Seyyedi Banerjee I Lungren Fusion of medical imaging and electronic health records using deep learning systematic review and implementation guidelines NPJ Dig Med npj Digital Medicine Published in partnership with Seoul National University Bundang Hospital Soenksen et Gietzelt Löpprich Karmen Ganzinger Models and data sources used in systems medicine Methods Inf Med Boonn Langlotz Radiologist use of and perceived need for patient data access J Dig imaging Goldberger et PhysioBank PhysioToolkit and PhysioNet components of new research resource for complex physiologic signals Circulation Johnson et freely accessible critical care database Sci Data Wang Krishnan Big data and clinicians review on the state of the science JMIR Med Inform Sun et Data processing and text mining technologies on electronic medical records Healthcare Eng Agrawal et Selection of predictors from candidate multimodal features using machine learning improves coronary artery disease prediction Patterns Royalty Machine Learning Mortality Prediction in Critical Care Database Doctoral dissertation Undergraduate Research Scholars Program Available electronically from https Meng Trinh Xu Liu Interpretability and Fairness luation of Deep Learning Models on Dataset arXiv preprint Bagheri et Multimodal learning for cardiovascular risk prediction using EHR Johnson et large publicly available database of labeled data arXiv preprint chest radiographs arXiv preprint Li Hu Liu Prediction of cardiovascular diseases by integrating modal features with machine learning methods Biomed Signal Process Control Soenksen Ma Code for generating the HAIM multimodal dataset of clinical data and version PhysioNet https Liu et deep learning system for differential diagnosis of skin diseases Nat Alsentzer et Publicly available clinical BERT embeddings arXiv preprint Med Stidham Artiﬁcial Intelligence for Understanding Imaging Text and Data in Gastroenterology Gastroenterol Hepatol Paquette Hood Price Sadovsky Deep Phenotyping During Pregnancy for Delivery of Predictive and Preventive Medicine Med Purwar Tripathi Ranjan Saxena Detection of microcytic hypochromia using cbc and blood ﬁlm features extracted from convolution neural network by different classiﬁers Multimed Tools Appl Hügle Kalweit Hügle Boedecker J In Explainable AI in Healthcare and Medicine Springer Tomašev et clinically applicable approach to continuous prediction of future acute kidney injury Nature Ieracitano Mammone Hussain Morabito novel machine learning based approach for automatic classiﬁcation of EEG recordings in dementia Neural Netw Prashanth Roy Mandal Ghosh detection of early Parkinson s disease through multimodal features and machine learning Int Med Inform Hyun Ahn Koh Lee J approach using radiomics to predict the histological subtypes of lung cancer Clin Nucl Med Yala Lehman Schuster Portnoi Barzilay deep learning model for improved breast cancer risk prediction ology Reda et Deep learning role in early diagnosis of prostate cancer Technol Cancer Res Treat An et Comparison of classiﬁcation models for glaucoma management Healthcare Eng Patel et Machine learning approaches for integrating clinical and ging features in depression classiﬁcation and response prediction Int Geriatr Psychiatry Huang Pareek Zamanian Banerjee I Lungren Multimodal fusion with deep neural networks for leveraging CT imaging and electronic health record in pulmonary embolism detection Sci Tiulpin et Multimodal machine knee osteoarthritis gression prediction from plain radiographs and clinical data Sci Wu et Radiological tumour classiﬁcation across imaging modality and tology Nat Mach Intell Mei et Artiﬁcial rapid diagnosis of patients with Nat Med Bardak B Tan Improving clinical outcome predictions using convolution over medical entities with multimodal learning Artif Intell Med Jin et Improving hospital mortality prediction with medical named entities and multimodal learning arXiv preprint Rajkomar et Scalable and accurate deep learning with electronic health records NPJ Digital Med Li et Inferring multimodal latent topics from electronic health records Nat Commun Štrumbelj Kononenko I Explaining prediction models and individual dictions with feature contributions Knowl Inf Syst Johnson et version PhysioNet https Irvin et Chexpert large chest radiograph dataset with uncertainty labels and expert comparison Proc AAAI Conf Artif Intell Cohen et TorchXRayVision library of chest datasets and models arXiv preprint Bertsimas Pauphilet Stevens J Tandon Predicting inpatient ﬂow at major hospital using interpretable analytics Manufact Service Operations Manag Zhu Luo Zhang Shi Shen approaches for casting the number of hospital daily discharged inpatients IEEE Biomed Health Inform Awad McNicholas J Briggs J Early hospital mortality prediction of intensive care unit patients using an ensemble learning approach Int Med Inform Awad McNicholas J Patient length of stay and mortality prediction survey Health Serv Manag Res ACKNOWLEDGEMENTS We thank the PhysioNet team from the MIT Laboratory for Computational Physiology for providing our researchers with credentialled access to the and datasets and for their support in guiding multimodal data interrogation and consolidation We especially thank Leo Celi and Sicheng Hao for their support on data review as well as the Harvard TH Chan School of Public Health Harvard Medical School the Institute for Medical Engineering and Science at MIT and the Beth Israel Deaconess Medical Centre for their continued support of this work We thank the MIT Supercloud for their support and help in setting up workspace as well as offering technical advice throughout the project Finally we thank Eli Pivo for providing feedback and support on computational experiments to our work This work was supported by the Abdul Latif Jameel Clinic for Machine Learning in Health and is supported by the National Science Foundation Graduate Research Fellowship under Grant Any opinion ﬁndings conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of the National Science Foundation AUTHOR CONTRIBUTIONS planned and performed experiments wrote code analyzed the data and wrote the paper performed experiments wrote code analyzed the data and edited the paper contributed to research design and edited the paper directed overall research and edited the paper In aggregate contributed equally to this work COMPETING INTERESTS The authors declare competing interests ETHICS APPROVAL Human subject research This work only makes use of and MIMIC Chest CXR to generate the multimodal dataset and does not contain any additional information involving human participants obtained by the authors Published in partnership with Seoul National University Bundang Hospital npj Digital Medicine Soenksen et ADDITIONAL INFORMATION Supplementary information The online version contains supplementary material available at https Correspondence and requests for materials should be addressed to Dimitris Bertsimas Reprints and permission information is available at http reprints Publisher s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations Open Access This article is licensed under Creative Commons Attribution International License which permits use sharing adaptation distribution and reproduction in any medium or format as long as you give appropriate credit to the original author s and the source provide link to the Creative Commons license and indicate if changes were made The images or other third party material in this article are included in the article s Creative Commons license unless indicated otherwise in credit line to the material If material is not included in the article s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use you will need to obtain permission directly from the copyright holder To view copy of license visit http this The Author s npj Digital Medicine Published in partnership with Seoul National University Bundang Hospital,,[]
